# Publications


## 2025

<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/hologram_compression.png" width="200" alt/>
</div>
**Assessing Learned Models for Phase-only Hologram Compression**

<img src="https://img.shields.io/badge/-SIGGRAPH-red">


[Zicong Peng](https://scholar.google.com/citations?user=lpi8DvIAAAAJ&hl=zh-CN),
[Yicheng Zhan](https://scholar.google.com/citations?user=lpi8DvIAAAAJ&hl=zh-CN),
[Josef Spjut](https://josef.spjut.me/),
and [Kaan Akşit](https://kaanaksit.com)

:material-web: [Project site](https://complightlab.com/publications/hologram_compression)
:material-newspaper-variant: [Manuscript](https://www.kaanaksit.com/assets/pdf/PengEtAl_SIGGRAPH2025_Assessing_learned_models_for_phase_only_hologram_compression.pdf)
:material-newspaper-variant: [Poster](https://www.kaanaksit.com/assets/pdf/PengEtAl_SIGGRAPH2025_Poster_assessing_learned_models_for_phase_only_hologram_compression.pdf)
:material-file-code: [Code](https://github.com/Dasheng6666/final_project_files/blob/main/INR.ipynb)
:material-video-account: [Project video](https://kaanaksit.com/assets/video/PengSig2025Assessing.mp4)
??? info ":material-tag-text: Bibtex"
	```
        @inproceedings{peng2025assessing,
          author = {Zicong Peng and Yichen Zhan and Josef Spjut and Kaan Ak{\c{s}}it},
          title = {Assessing Learned Models for Phase-only Hologram Compression},
          booktitle = {SIGGRAPH 2025 Posters (SA Posters '25)},
          year = {2025},
          location = {Vancouver, BC, Canada},
          publisher = {ACM},
          address = {New York, NY, USA},
          pages = {2},
          doi = {10.1145/3721250.3742993},
          url = {https://doi.org/10.1145/3721250.3742993},
          month = {August 10--14}
        }
	```
<br clear="left"/>


## 2024
<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/focal_surfaec_lightprop_web_image.png" width="200" alt/>
</div>
**Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions**

<img src="https://img.shields.io/badge/-SIGGRAPH ASIA-critical">

[Chuanjun Zheng](https://scholar.google.com.hk/citations?user=9Jk_LC8AAAAJ&hl=zh-CN),
[Yicheng Zhan](https://scholar.google.com/citations?hl=zh-CN&user=x2ptSYUAAAAJ),
[Liang Shi](https://people.csail.mit.edu/liangs/),
[Ozan Cakmakci](https://scholar.google.com/citations?user=xZLjeAMAAAAJ&hl=en),
and [Kaan Akşit](https://kaanaksit.com)

:material-web: [Project site](https://complightlab.com/publications/focal_surface_light_transport/)
:material-newspaper-variant: [Manuscript](https://kaanaksit.com/assets/pdf/ZhengEtAl_SigAsia2024_Focal_surface_holographic_light_transport_using_learned_spatially_adaptive_convolutions.pdf)
:material-newspaper-variant:[Supplementary](https://kaanaksit.com/assets/pdf/ZhengEtAl_SigAsia2024_Supplementary_Focal_surface_holographic_light_transport_using_learned_spatially_adaptive_convolutions.pdf)
:material-file-code: [Code](https://github.com/complight/focal_surface_holographic_light_transport)
??? info ":material-tag-text: Bibtex"
	```
        @inproceedings{zheng2024focalholography,
          title={Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions},
          author={Zheng, Chuanjun and Zhan, Yicheng and Shi, Liang and Cakmakci, Ozan and Ak{\c{s}}it, Kaan},
          booktitle = {SIGGRAPH Asia 2024 Technical Communications (SA Technical Communications '24)},
          keywords = {Computer-Generated Holography, Light Transport, Optimization},
          location = {Tokyo, Japan},
          series = {SA '24},
          month={December},
          year={2024},
          doi={https://doi.org/10.1145/3681758.3697989}
        }
	```
<br clear="left"/>

<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/spec_track_different_attributes.png" width="200" alt/>
</div>
**SpecTrack: Learned Multi-Rotation Tracking via Speckle Imaging**

<img src="https://img.shields.io/badge/-SIGGRAPH ASIA-critical">

:fontawesome-solid-award: **Honorable Mention Award**

[Ziyang Chen](https://ziyang.space/),
[Mustafa Doğa Doğan](https://www.dogadogan.com/),
[Josef Spjut](https://josef.spjut.me/),
and [Kaan Akşit](https://kaanaksit.com)

:material-web: [Project site](https://complightlab.com/publications/spec_track)
:material-newspaper-variant: [Manuscript](https://kaanaksit.com/assets/pdf/ChenEtAl_SigAsia2024_Abstract_spectrack_learned_multi_rotation_tracking_via_speckle_imaging.pdf)
:material-newspaper-variant: [Poster](https://kaanaksit.com/assets/pdf/ChenEtAl_SigAsia2024_Poster_spectrack_learned_multi_rotation_tracking_via_speckle_imaging.pdf)
:material-file-code: [Code](https://github.com/complight/SpecTrack)
:material-video-account: [Project video](https://kaanaksit.com/assets/video/ChenSigAsia2024SpecTrack.mp4)
??? info ":material-tag-text: Bibtex"
	```
        @inproceedings{chen2024spectrack,
          author = {Ziyang Chen and Mustafa Dogan and Josef Spjut and Kaan Ak{\c{s}}it},
          title = {SpecTrack: Learned Multi-Rotation Tracking via Speckle Imaging},
          booktitle = {SIGGRAPH Asia 2024 Posters (SA Posters '24)},
          year = {2024},
          location = {Tokyo, Japan},
          publisher = {ACM},
          address = {New York, NY, USA},
          pages = {2},
          doi = {10.1145/3681756.3697875},
          url = {https://doi.org/10.1145/3681756.3697875},
          month = {December 03--06}
        }
	```
<br clear="left"/>

<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/diffractive_visual_processor.png" width="200" alt/>
</div>
**All-optical image denoising using a diffractive visual processor**

<img src="https://img.shields.io/badge/-Light: Science & Applications-critical">

[Çağatay Işıl](https://cagatayisil.github.io/),
[Tianyi Gan](https://www.linkedin.com/in/tianyi-gan-177214285),
[Fazil Onuralp](https://www.linkedin.com/in/fazil-onuralp-ardic-72a501177),
[Koray Mentesoglu](https://www.linkedin.com/in/korayucla2024/),
[Jagrit Digani](https://www.linkedin.com/in/jagrit06),
[Huseyin Karaca](https://www.linkedin.com/in/huseyinkaraca),
[Hanlong Chen](https://www.linkedin.com/in/%E7%92%90%E5%93%B2-%E9%BB%84-59854b155),
[Jingxi Li](https://scholar.google.com/citations?user=_FMlkBoAAAAJ&hl=en&oi=ao),
[Deniz Mengu](https://scholar.google.com/citations?user=MpYPqXEAAAAJ&hl=en),
[Mona Jarrahi](http://www.seas.ucla.edu/~mjarrahi/mjarrahi.html),
[Kaan Akşit](https://kaanaksit.com),
and [Ozcan Aydogan](https://www.ee.ucla.edu/aydogan-ozcan)

:material-web-box: [Publisher site](https://doi.org/10.1038/s41377-024-01385-6)
:material-newspaper-variant: [Manuscript](https://kaanaksit.com/assets/pdf/IsilEtAl_LightScienceAndApplications2024_All_optical_image_denoising_using_a_diffractive_visual_processor.pdf)
??? info ":material-tag-text: Bibtex"
	```
        @article{Işıl2024,
          author = {I{\c{s}}{\i}l, {\c{C}}a{\u{g}}atay and Gan, Tianyi and Ardic, Fazil Onuralp and Mentesoglu, Koray and Digani, Jagrit and Karaca, Huseyin and Chen, Hanlong and Li, Jingxi and Mengu, Deniz and Jarrahi, Mona and Ak{\c{s}}it, Kaan and Ozcan, Aydogan},
          title = {All-optical image denoising using a diffractive visual processor},
          journal = {Light: Science {\&} Applications},
          year = {2024},
          month = feb,
          day = {04},
          volume = {13},
          number = {1},
          pages = {43},
          issn = {2047-7538},
          doi = {10.1038/s41377-024-01385-6},
          url = {https://doi.org/10.1038/s41377-024-01385-6}
       }
	```
<br clear="left"/>

<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/autocolor.png" width="200" alt/>
</div>
**Autocolor: Learned Light Power Control for Multi-Color Holograms**

<img src="https://img.shields.io/badge/-SPIE AR|VR|MR-yellow">

[Yicheng Zhan](https://github.com/AlberTgarY),
[Hakan Urey](https://mysite.ku.edu.tr/hurey/),
[Qi Sun](https://qisun.me/),
and [Kaan Akşit](https://kaanaksit.com)


:material-web: [Project site](https://complightlab.com/autocolor_)
:material-newspaper-variant: [Manuscript](https://arxiv.org/pdf/2305.01611.pdf)
:material-file-code: [Code](https://github.com/complight/autocolor)
??? info ":material-tag-text: Bibtex"
	```
        @article{zhan2023autocolor,
          title = {AutoColor: Learned Light Power Control for Multi-Color Holograms},
          author = {Zhan, Yicheng and Sun, Qi and Akşit, Kaan},
          journal  = "arxiv",
          year = {2023},
          month = may,
        }
	```
<br clear="left"/>

## 2023
<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/multicolor.png" width="200" alt/>
</div>
**Multi-color Holograms Improve Brightnes in Holographic Displays**

<img src="https://img.shields.io/badge/-SIGGRAPH ASIA-critical">

[Koray Kavaklı](https://scholar.google.com/citations?user=rn6XtO4AAAAJ&hl=en&oi=ao),
[Liang Shi](https://people.csail.mit.edu/liangs/),
[Hakan Urey](https://mysite.ku.edu.tr/hurey/),
[Wojciech Matusik](https://cdfg.csail.mit.edu/wojciech),
and [Kaan Akşit](https://kaanaksit.com) 


:material-web: [Project site](multi_color.md)
:material-newspaper-variant: [Manuscript](https://kaanaksit.com/assets/pdf/KavakliEtAl_SigAsia2023_Multi_color_holograms_improve_brightness_in_holographic_displays.pdf)
:material-file-code: [Code](https://github.com/complight/multi_color)
:material-video-account: [Project video](https://kaanaksit.com/assets/video/KavakliSigAsia2023Multicolor.mp4)
??? info ":material-tag-text: Bibtex"
	```
        @inproceedings{kavakli2023multicolor,
          title={Multi-color Holograms Improve Brightness in Holographic Displays},
          author={Kavaklı, Koray and Shi, Liang and Urey, Hakan and Matusik, Wojciech and Akşit, Kaan},
          booktitle = {SIGGRAPH Asia 2023 Conference Papers},
          articleno = {20},
          numpages = {11},
          keywords = {Brightness, Computer-generated holography, Holographic displays},
          location = {Sydney, NSW, Australia},
          series = {SA '23},
          month={December},
          year={2023},
          doi={https://doi.org/10.1145/3610548.3618135}
        }
	```
<br clear="left"/>


<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/learned_prescription.png" width="200" alt/>
</div>
**ChromaCorrect: Prescription Correction in Virtual Reality Headsets through Perceptual Guidance**

<img src="https://img.shields.io/badge/-Biomedical Optics Express-informational">

[Ahmet Güzel](https://aguzel.github.io/),
[Jeanne Beyazian](https://www.linkedin.com/in/jeanne-beyazian-736824183),
[Praneeth Chakravarthula](https://www.cs.unc.edu/~cpk/),
and [Kaan Akşit](https://kaanaksit.com)


:material-web: [Project site](http://complightlab.com/ChromaCorrect/)
:material-newspaper-variant: [Manuscript](https://arxiv.org/pdf/2212.04264.pdf)
:material-file-code: [Code](https://github.com/complight/ChromaCorrect)
:material-video-account: [Project video](https://www.youtube.com/watch?v=fjexa7ga-tQ&feature=youtu.be)
??? info ":material-tag-text: Bibtex"
	```
        @ARTICLE{guzel2022prescription,
          title    = "ChromaCorrect: Prescription Correction in Virtual Reality Headsets through Perceptual Guidance",
          author   = "Güzel, Ahmet and Beyazian, Jeanne and Chakravarthula, Praneeth and Akşit, Kaan",
          journal  = "Biomedical Optics Express",
          month    =  jan,
          year     =  2023,
        }
	```
<br clear="left"/>



<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/holobeam.png" width="200" alt/>
</div>
**HoloBeam: Paper-Thin Near-Eye Displays**

<img src="https://img.shields.io/badge/-IEEE VR-important">

[Kaan Akşit](https://kaanaksit.com)
and [Yuta Itoh](https://www.ar.c.titech.ac.jp/people/yuta-itoh)


:material-web: [Project site](holobeam.md)
:material-newspaper-variant: [Manuscript](https://arxiv.org/pdf/2212.05057.pdf)
:material-file-code: [Code](https://github.com/complight/multiholo)
??? info ":material-tag-text: Bibtex"
	```
        @ARTICLE{aksit2022holobeam,
          title    = "HoloBeam: Paper-Thin Near-Eye Displays",
          author   = "Akşit, Kaan and Itoh, Yuta",
          journal  = "IEEE VR 2023",
          month    =  Mar,
          year     =  2023,
        }
	```
<br clear="left"/>


<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/realistic_defocus_cgh.png" width="200" alt/>
</div>
**Realistic Defocus Blur for Multiplane Computer-Generated Holography**

<img src="https://img.shields.io/badge/-IEEE VR-important">

[Koray Kavaklı](https://scholar.google.com/citations?user=rn6XtO4AAAAJ&hl=en&oi=ao),
[Yuta Itoh](https://www.ar.c.titech.ac.jp/people/yuta-itoh),
[Hakan Urey](https://mysite.ku.edu.tr/hurey/)
and [Kaan Akşit](https://kaanaksit.com)

:material-web: [Project site](realistic_defocus_cgh.md)
:material-newspaper-variant: [Manuscript](https://arxiv.org/abs/2205.07030)
:material-video-account: [Project video](https://youtu.be/5tG8SaJGpUc)
:material-file-code: [Code](https://github.com/complight/realistic_defocus)
??? info ":material-tag-text: Bibtex"
	```
        @misc{kavakli2022realisticdefocus,
          doi = {10.48550/ARXIV.2205.07030},
          url = {https://arxiv.org/abs/2205.07030},
          author = {Kavaklı, Koray and Itoh, Yuta and Urey, Hakan and Akşit, Kaan},
          keywords = {Computer Vision and Pattern Recognition (cs.CV), Graphics (cs.GR), FOS: Computer and information sciences, FOS: Computer and information sciences, I.3.3},
          title = {Realistic Defocus Blur for Multiplane Computer-Generated Holography},
          publisher = {IEEE VR 2023},
          month = {Mar},
          year = {2023},
          copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
        }
	```
<br clear="left"/>


## 2022
<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/metameric_inpainting.png" width="200" alt/>
</div>
**Metameric Inpainting for Image Warping**

<img src="https://img.shields.io/badge/-IEEE-important">

[Rafael Kuffner Dos Anjos](https://rafaelkuffner.github.io/),
[David R. Walton](https://drwalton.github.io/),
[Kaan Akşit](https://kaanaksit.com),
[Sebastian Friston](https://wp.cs.ucl.ac.uk/sebastianfriston/),
[David Swapp](https://scholar.google.co.in/citations?user=sQhgJh4AAAAJ&hl=en),
[Anthony Steed](http://vecg.cs.ucl.ac.uk/)
and [Tobias Ritschel](https://www.homepages.ucl.ac.uk/~ucactri/)


:material-web-box: [Publisher site](https://doi.org/10.1109/tvcg.2022.3216712)
:material-newspaper-variant: [Manuscript](https://media.githubusercontent.com/media/kaanaksit/kaanaksit.github.io/master/assets/pdf/KuffnerEtAl_IEEETVCG2022_Metameric_inpainting_for_image_warping.pdf)
??? info ":material-tag-text: Bibtex"
	```
        @ARTICLE{Kuffner_Dos_Anjos2022-hm,
            title    = "Metameric inpainting for image warping",
            author   = "Kuffner Dos Anjos, Rafael and Walton, David R and Akşit, Kaan and
                        Friston, Sebastian and Swapp, David and Steed, Anthony and
                        Ritschel, Tobias",
            journal  = "IEEE Trans. Vis. Comput. Graph.",
            volume   = "PP",
            month    =  oct,
            year     =  2022,
        }
	```
<br clear="left"/>


<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/optimizing_vision_and_visuals.png" width="200" alt/>
</div>
**Optimizing vision and visuals: lectures on cameras, displays and perception**

<img src="https://img.shields.io/badge/-SIGGRAPH-red">

[Koray Kavaklı](https://scholar.google.com/citations?user=rn6XtO4AAAAJ&hl=en&oi=ao),
[David Robert Walton](https://drwalton.github.io/),
[Nick Antipa](http://nickantipa.com/),
[Rafał Mantiuk](https://www.cl.cam.ac.uk/~rkm38/),
[Douglas Lanman](https://alumni.media.mit.edu/~dlanman/)
and [Kaan Akşit](https://kaanaksit.com)

:material-web: [Project site](https://complightlab.com/teaching/siggraph2022_optimizing_vision_and_visuals/)
:material-web-box: [Publisher site](https://doi.org/10.1145/3532720.3535650)
:material-newspaper-variant: [Manuscript](https://github.com/complight/cameras-displays-perception-course/blob/main/latex/course.pdf)
:material-video-account: [Project video](https://youtu.be/z_AtSgct6_I)
:material-file-code: [Code](https://github.com/complight/cameras-displays-perception-course)
??? info ":material-tag-text: Bibtex"
	```
        @incollection{kavakli2022optimizing,
          title = {Optimizing vision and visuals: lectures on cameras, displays and perception},
          author = {Kavaklı, Koray and Walton, David Robert and Antipa, Nick and Mantiuk, Rafał and Lanman, Douglas and Ak{\c{s}}it, Kaan},
          booktitle = {ACM SIGGRAPH 2022 Courses},
          pages = {1--66},
          year = {2022},
          doi = {https://doi.org/10.1145/3532720.3535650},
          video = {https://youtu.be/z_AtSgct6_I},
        }
	```
<br clear="left"/>




<br clear="left"/>


<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/unrolled_primal_dual.png" width="200" alt/>
</div>
**Unrolled Primal-Dual Networks for Lensless Cameras**

<img src="https://img.shields.io/badge/-Optics Express-informational">

[Oliver Kingshott](https://oliver.kingshott.com/),
[Nick Antipa](http://nickantipa.com/),
[Emrah Bostan](https://emrahbostan.com/)
and [Kaan Akşit](https://kaanaksit.com)

:material-newspaper-variant: [Manuscript](https://arxiv.org/abs/2203.04353)
:material-web-box: [Publisher site](https://doi.org/10.1364/OE.475521)
:material-newspaper-variant: [Supplementary](https://media.githubusercontent.com/media/kaanaksit/kaanaksit.github.io/master/assets/pdf/KingshottEtAl_OpticsExpress2022_Unrolled_primal_dual_networks_for_lensless_cameras_Supplementarty.pdf)
:material-file-code: [Code](https://github.com/complight/unrolled_primal_dual_networks)
??? info ":material-tag-text: Bibtex"
	```
        @article{kingshott2022unrolled,
           selected={true},
           title={Unrolled Primal-Dual Networks for Lensless Cameras},
           author={Kingshott, Oliver and Antipa, Nick and Bostan, Emrah and Akşit, Kaan},
           journal={Optics Express},
           year={2022},
           doi={https://doi.org/10.48550/arXiv.2203.04353}
        }
	```


<br clear="left"/>


<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/metameric.png" width="200" alt/>
</div>
**Metameric Varifocal Holograms**

<img src="https://img.shields.io/badge/-IEEE VR-important">

[David R. Walton](https://drwalton.github.io/),
[Koray Kavaklı](https://scholar.google.com/citations?user=rn6XtO4AAAAJ&hl=en&oi=ao),
Rafael Kuffner Dos Anjos,
David Swapp,
[Tim Weyrich](https://reality.tf.fau.de/weyrich.html),
[Hakan Urey](https://mysite.ku.edu.tr/hurey/),
[Anthony Steed](http://vecg.cs.ucl.ac.uk/),
[Tobias Ritschel](https://www.homepages.ucl.ac.uk/~ucactri/)
and [Kaan Akşit](https://kaanaksit.com)

:material-web: [Project site](https://vr.cs.ucl.ac.uk/research/pipelines/metameric-varifocal-holography/)
:material-newspaper-variant: [Manuscript](https://arxiv.org/abs/2110.01981)
:material-video-account: [Project video](https://vimeo.com/623474853)
:material-file-code: [Code](https://github.com/complight/metameric_holography)
??? info ":material-tag-text: Bibtex"
	```
        @article{walton2021metameric,
                 title={Metameric Varifocal Holography},
                 author={Walton, David R and Kavakl{\i}, Koray and Anjos, Rafael Kuffner dos and Swapp, David and Weyrich, Tim and Urey, Hakan and Steed, Anthony and Ritschel, Tobias and Ak{\c{s}}it, Kaan},
                 publisher = {IEEE VR},
                 month = {March},
                 year={2022}
                }
	```



<br clear="left"/>


<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/learned_light.gif" width="200" alt/>
</div>
**Learned holographic light transport**

<img src="https://img.shields.io/badge/-Applied Optics-blueviolet">

:fontawesome-solid-award: **Invited**

[Koray Kavaklı](https://scholar.google.com/citations?user=rn6XtO4AAAAJ&hl=en&oi=ao),
[Hakan Urey](https://mysite.ku.edu.tr/hurey/)
and [Kaan Akşit](https://kaanaksit.com)

:material-web-box: [Publisher site](https://doi.org/10.1364/AO.439401)
:material-newspaper-variant: [Manuscript](https://kaanaksit.files.wordpress.com/2021/09/learned_holographic_light_transport.pdf)
:material-file-code: [Code](https://github.com/complight/realistic_holography)
:material-database: [Dataset](https://rdr.ucl.ac.uk/articles/dataset/Phase-only_holograms_and_captured_photographs/15087867)
??? info ":material-tag-text: Bibtex"
	```
        @article{Kavakli:22,
          author = {Koray Kavakl{i} and Hakan Urey and Kaan Ak\c{s}it},
          journal = {Appl. Opt.},
          keywords = {Holographic displays; Holographic recording; Holographic techniques; Image quality; Image reconstruction; Visible light communications},
          number = {5},
          pages = {B50--B55},
          publisher = {OSA},
          title = {Learned holographic light transport: invited},
          volume = {61},
          month = {Feb},
          year = {2022},
          url = {http://www.osapublishing.org/ao/abstract.cfm?URI=ao-61-5-B50},
          doi = {10.1364/AO.439401},
        }
	```


<br clear="left"/>
 

## 2021

<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/telelife.png" width="200" alt/>
</div>
**Telelife: the future of remote living**

<img src="https://img.shields.io/badge/-Frontiers in Virtual Reality-ff69b4">

[Jason Orlosky](https://www.jeoresearch.com/research),
[Misha Sra](https://sites.cs.ucsb.edu/~sra/),
[Kenan Bektaş](https://www.unisg.ch/en/personenverzeichnis/dc93dc3e-fad5-45ce-a9dd-fa7bf0b64cd9),
[Huaishu Peng](https://www.cs.umd.edu/people/huaishu),
[Jeeeun Kim](http://www.jeeeunkim.com/),
[Nataliya Kosmyna](https://www.braini.io/),
[Tobias Hollerer](https://sites.cs.ucsb.edu/~holl/),
[Anthony Steed](http://vecg.cs.ucl.ac.uk/),
[Kiyoshi Kiyokawa](https://carelab.info/en/)
and [Kaan Akşit](https://kaanaksit.com)

:material-web-box: [Publisher site](https://doi.org/10.3389/frvir.2021.763340)
:material-newspaper-variant: [Manuscript](https://kaanaksit.files.wordpress.com/2021/07/2107.02965.pdf)
??? info ":material-tag-text: Bibtex"
	```
	@ARTICLE{10.3389/frvir.2021.763340,
	AUTHOR={Orlosky, Jason and Sra, Misha and Bektaş, Kenan and Peng, Huaishu and Kim, Jeeeun and Kos’myna, Nataliya and Höllerer, Tobias and Steed, Anthony and Kiyokawa, Kiyoshi and Ak\c{s}it, Kaan},   
	TITLE={Telelife: The Future of Remote Living},      
	JOURNAL={Frontiers in Virtual Reality},      
	VOLUME={2},      
	PAGES={147},     
	YEAR={2021},      
	URL={https://www.frontiersin.org/article/10.3389/frvir.2021.763340},       
	DOI={10.3389/frvir.2021.763340},      
	ISSN={2673-4192},   
	}
	```


<br clear="left"/>
 

<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/sensicut.png" width="200" alt/>
</div>
**SensiCut: material-aware laser cutting using speckle sensing and deep learning**

<img src="https://img.shields.io/badge/-UIST-9cf">

[Mustafa Doga Dogan](https://www.dogadogan.com/),
Steven Vidal Acevedo Colon,
Varnika Sinha,
[Kaan Akşit](https://kaanaksit.com)
and [Stefanie Mueller](https://hcie.csail.mit.edu/stefanie-mueller.html)

:material-web-box: [Publisher site](https://dl.acm.org/doi/10.1145/3472749.3474733)
:material-web: [Project site](https://hcie.csail.mit.edu/research/sensicut/sensicut.html)
:material-newspaper-variant: [Manuscript](https://kaanaksit.files.wordpress.com/2021/08/2021-uist-sensicut-paper.pdf)
:material-video-account: [Project video](https://www.youtube.com/watch?v=BdvSAJaukI8)
:material-video-account: [Presentation recording](https://www.youtube.com/watch?v=fxD5GEMQ8kk)
??? info ":material-tag-text: Bibtex"
	```
	@inproceedings{dogan2021sensicut,
	  title={SensiCut: Material-Aware Laser Cutting Using Speckle Sensing and Deep Learning},
	  author={Dogan, Mustafa Doga and Acevedo Colon, Steven Vidal and Sinha, Varnika and Ak{\c{s}}it, Kaan and Mueller, Stefanie},
	  booktitle={The 34th Annual ACM Symposium on User Interface Software and Technology},
	  pages={24--38},
	  year={2021}
	}

	```

<br clear="left"/>
 
 


<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/beyond_blur.png" width="200" alt/>
</div>
**Beyond blur: ventral metamers for foveated rendering**

<img src="https://img.shields.io/badge/-SIGGRAPH-red">

[David R. Walton](https://drwalton.github.io/),
Rafael Kuffner Dos Anjos,
Sebastian Friston,
David Swapp,
[Kaan Akşit](https://kaanaksit.com),
[Anthony Steed](http://vecg.cs.ucl.ac.uk/)
and Tobias Ritschel

:material-web-box: [Publisher site](https://doi.org/10.1145/3450626.3459943)
:material-web: [Project site](https://www.homepages.ucl.ac.uk/~ucabdw0/beyondblur.html)
:material-newspaper-variant: [Manuscript](https://kaanaksit.files.wordpress.com/2021/08/beyond_blur_preprint.pdf)
??? info ":material-tag-text: Bibtex"
	```
	@article{walton2021beyond,
	    author = {David R. Walton and Rafael Kuffner Dos Anjos and Sebastian Friston and David Swapp and Kaan Akşit and Anthony Steed and Tobias Ritschel},
	    title    = {Beyond Blur: Ventral Metamers for Foveated Rendering},
	    journal = {ACM Trans. Graph. (Proc. SIGGRAPH 2021)},
	    year = {2021},
	    volume = {40},
	    number = {4},
	}
	```
<br clear="left"/>
 
 


<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/beaming_displays.png" width="200" alt/>
</div>
**Beaming displays**

<img src="https://img.shields.io/badge/-IEEE VR-important">

:fontawesome-solid-award: **Best paper nominee at IEEE VR 2021**

[Yuta Itoh](https://www.ar.c.titech.ac.jp/people/yuta-itoh),
Takumi Kaminokado
and [Kaan Akşit](https://kaanaksit.com)

:material-web-box: [Publisher site](https://doi.org/10.1109/TVCG.2021.3067764)
:material-newspaper-variant: [Manuscript](https://kaanaksit.files.wordpress.com/2021/03/vr2021_beaming_display_revision-1.pdf)
:material-video-account: [Project video](https://www.youtube.com/watch?v=TKl1l3b-LDs)
:material-video-account: [Presentation recording](https://www.youtube.com/watch?v=j0nY4_cauZY)
??? info ":material-tag-text: Bibtex"
	```
	@article{itoh2021beaming,
	    author = {Yuta Itoh, Takumi Kaminokado, and Kaan Ak{s}it},
	    keywords = {Near-eye displays},
	    publisher = {IEEE VR},
	    title = {Beaming Displays},
	    month = {April},
	    year = {2021}
	}
	```
<br clear="left"/>
 
 



## 2020
<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/single_pixel_gaze.png" width="200" alt/>
</div>
**Optical gaze tracking with spatially-sparse single-pixel detectors**

<img src="https://img.shields.io/badge/-ISMAR-yellow">

[Richard Li](https://lichard49.github.io),
[Eric Whitmire](https://www.ericwhitmire.com/),
[Michael Stengel](https://graphics.tu-bs.de/people/stengel),
Ben Boudaoud,
[Jan Kautz](https://www.jankautz.com/),
[David Luebke](https://luebke.us/),
[Shwetak Patel](https://homes.cs.washington.edu/~shwetak/)
and [Kaan Akşit](https://kaanaksit.com)

:material-web-box: [Publisher site](https://doi.org/10.1109/ISMAR50242.2020.00033)
:material-web: [Project site](https://lichard49.github.io/nextgaze.html)
:material-newspaper-variant: [Manuscript](https://arxiv.org/abs/2009.06875)
:material-video-account: [Presentation recording](https://www.youtube.com/watch?v=j0nY4_cauZY&feature=emb_logo)
??? info ":material-tag-text: Bibtex"
	```
	@article{li2020opticalgaze,
	    author = {Richard Li, Eric Whitmire, Michael Stengel, Ben Boudaoud, Jan Kautz, David Luebke, Shwetak Patel, and Kaan Ak{s}it},
	    keywords = {Gaze tracking, eye tracking, LEDs, photodiodes},
	    publisher = {ISMAR},
	    title = {Optical Gaze Tracking with Spatially-Sparse Single-Pixel Detectors},
	    month = {Nov},
	    year = {2020}
	}
	```
<br clear="left"/>
 
 


<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/patch_scan.png" width="200" alt/>
</div>
**Patch scanning displays: spatiotemporal enhancement for displays**

<img src="https://img.shields.io/badge/-Optics Express-informational">

[Kaan Akşit](https://kaanaksit.com)

:material-web-box: [Publisher site](https://doi.org/10.1364/OE.380858)
:material-newspaper-variant: [Manuscript](https://kaanaksit.files.wordpress.com/2020/01/oe-28-2-2107.pdf)
:material-video-account: [Project video](https://youtu.be/c3okd_gIlrg)
??? info ":material-tag-text: Bibtex"
	```
	@article{aksit2020patch,
	    author = {Kaan Ak\c{s}it},
	    journal = {Opt. Express},
	    keywords = {Digital micromirror devices; Image quality; Image reconstruction; Light sources; Optical components; Three dimensional imaging},
	    number = {2},
	    pages = {2107--2121},
	    publisher = {OSA},
	    title = {Patch scanning displays: spatiotemporal enhancement for displays},
	    volume = {28},
	    month = {Jan},
	    year = {2020},
	    url = {http://www.opticsexpress.org/abstract.cfm?URI=oe-28-2-2107}
	}
	```
<br clear="left"/>
 
 



## 2019
<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/next_displays.png" width="200" alt/>
</div>
**Near-eye display and tracking technologies for virtual and augmented reality**

<img src="https://img.shields.io/badge/-Eurographics-lightgrey">

[George Alex Koulieris](https://koulieris.com/)
[Kaan Akşit](https://kaanaksit.com),
[Michael Stengel](https://graphics.tu-bs.de/people/stengel),
[Rafał Mantiuk](https://www.cl.cam.ac.uk/~rkm38/),
[Katerina Mania](http://surreal.tuc.gr/)
and [Christian Richardt](https://richardt.name/)

:material-web-box: [Publisher site](https://doi.org/10.1111/cgf.13654)
:material-newspaper-variant: [Manuscript](https://kaanaksit.files.wordpress.com/2019/05/neareyedisplayandtracking-koulierisetal-cgf2019-star.pdf)
:material-video-account: [Project video](https://youtu.be/e-A2dgKV5-I)
??? info ":material-tag-text: Bibtex"
	```
	@article{NearEyeDisplayAndTrackingSTAR,
	author  = {George Alex Koulieris and Kaan Ak{\c{s}}it and Michael Stengel and Rafa{\l} K. Mantiuk and Katerina Mania and Christian Richardt},
	title   = {Near-Eye Display and Tracking Technologies for Virtual and Augmented Reality},
	journal = {Computer Graphics Forum},
	year    = {2019},
	volume  = {38},
	number  = {2},
	url     = {https://richardt.name/nedtt/},
	}
	```
<br clear="left"/>
 
 


<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/foveated_displays.png" width="200" alt/>
</div>
**Foveated AR: dynamically-foveated augmented reality display**

<img src="https://img.shields.io/badge/-SIGGRAPH-red">

:fontawesome-solid-award: **Emerging Technology best in show award at SIGGRAPH 2019**

[Jonghyun Kim](http://j-kim.kr/),
[Youngmo Jeong](http://www.youngmoj.com/),
[Michael Stengel](https://graphics.tu-bs.de/people/stengel),
[Kaan Akşit](https://kaanaksit.com),
[Rachel Albert](http://www.rachelabrown.com/),
Ben Boudaoud,
Trey Greer,
Joohwan Kim,
Ward Lopes,
Zander Majercik,
[Peter Shirley](https://www.petershirley.com/),
[Josef Spjut](http://josef.spjut.me/),
[Morgan Mcguire](https://casual-effects.com/morgan/index.html)
and [David Luebke](https://luebke.us/)

:material-web-box: [Publisher site](https://doi.org/10.1145/3306346.3322987)
:material-newspaper-variant: [Manuscript](https://kaanaksit.files.wordpress.com/2019/07/foveated_ar___v2-15.pdf)
:material-video-account: [Project video](https://www.youtube.com/watch?v=IknBUoRGUkM)
??? info ":material-tag-text: Bibtex"
	```
	@article{kim2019foveated,
	  title={Foveated AR: dynamically-foveated augmented reality display},
	  author={Kim, Jonghyun and Jeong, Youngmo and Stengel, Michael and Ak{\c{s}}it, Kaan and Albert, Rachel and Boudaoud, Ben and Greer, Trey and Kim, Joohwan and Lopes, Ward and Majercik, Zander and others},
	  journal={ACM Transactions on Graphics (TOG)},
	  volume={38},
	  number={4},
	  pages={1--15},
	  year={2019},
	  publisher={ACM New York, NY, USA}
	}
	```
<br clear="left"/>
 
 


## 2018
<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/focusar.png" width="200" alt/>
</div>
**FocusAR: auto-focus augmented reality eyeglasses for both real and virtual**

<img src="https://img.shields.io/badge/-ISMAR-yellow">

:fontawesome-solid-award: **Best paper award at ISMAR 2018**

:material-eye: **Presented at SIGGRAPH ASIA 2018**

[Praneeth Chakravarthula](http://www.cs.unc.edu/~cpk/),
[David Dunn](http://www.qenops.com/), 
[Kaan Akşit](https://kaanaksit.com)
and [Henry Fuchs](https://scholar.google.com/citations?user=guhwcP8AAAAJ&hl=en)

:material-web-box: [Publisher site](https://doi.org/10.1109/TVCG.2018.2868532)
:material-newspaper-variant: [Manuscript](https://kaanaksit.files.wordpress.com/2018/10/10-1109tvcg-2018-2868532.pdf)
:material-video-account: [Presentation recording](https://www.youtube.com/watch?v=6rC_XGXk3CY&feature=emb_logo)
:material-presentation: [Presentation source](https://docs.google.com/presentation/d/1EOhFAtlxEzt3j6aRtu5sEHOLaiWp1amaz7h62PlZx1k/edit?usp=sharing)
??? info ":material-tag-text: Bibtex"
	```
	@article{chakravarthula2018focusar,
	  title={focusar: auto-focus augmented reality eyeglasses for both real and virtual},
	  author={chakravarthula, praneeth and dunn, david and ak{\c{s}}it, kaan and fuchs, henry},
	  journal={ieee transactions on visualization and computer graphics},
	  year={2018},
	  publisher={ieee}
	}
	```
<br clear="left"/>
 
 


<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/manufacturing_displays.png" width="200" alt/>
</div>
**Manufacturing application-driven foveated near-eye displays**

<img src="https://img.shields.io/badge/-IEEE VR-important">

:fontawesome-solid-award: **Best paper nominee at IEEE VR 2018**

:fontawesome-solid-award: **Emerging Technology best in show award at SIGGRAPH 2018**

[Kaan Akşit](https://kaanaksit.com),
[Praneeth Chakravarthula](http://www.cs.unc.edu/~cpk/),
[Kishore Rathinavel](https://sites.google.com/site/kishorerathinavel/),
[Youngmo Jeong](http://www.youngmoj.com/),
[Rachel Albert](http://www.rachelabrown.com/),
[Henry Fuchs](https://scholar.google.com/citations?user=guhwcP8AAAAJ&hl=en)
and [David Luebke](https://luebke.us/)

:material-web-box: [Publisher site](https://doi.org/10.1109/TVCG.2019.2898781)
:material-newspaper-variant: [Manuscript](https://kaanaksit.files.wordpress.com/2018/08/08642529.pdf)
:material-file-video: [Project video](https://youtu.be/dBUeB8RLYCU)
:material-video-account: [Presentation recording](https://www.youtube.com/watch?v=jYWjqPeMSw8&feature=emb_logo)
:material-presentation: [Presentation source](https://docs.google.com/presentation/d/15mrXj3Rv_dHP7BYUtCid8Mi7FJRl912LOIryE1PDqk8/edit?usp=sharing)
??? info ":material-tag-text: Bibtex"
	```
	@article{akcsit2019manufacturing,
	  title={Manufacturing application-driven foveated near-eye displays},
	  author={Ak{\c{s}}it, Kaan and Chakravarthula, Praneeth and Rathinavel, Kishore and Jeong, Youngmo and Albert, Rachel and Fuchs, Henry and Luebke, David},
	  journal={IEEE transactions on visualization and computer graphics},
	  volume={25},
	  number={5},
	  pages={1928--1939},
	  year={2019},
	  publisher={IEEE}
	}
	```
<br clear="left"/>
 
 



## 2017
<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/see_through_hoe.png" width="200" alt/>
</div>
**Near-Eye varifocal augmented reality display using see-through screens**

<img src="https://img.shields.io/badge/-SIGGRAPH ASIA-critical">

[Kaan Akşit](https://kaanaksit.com),
Ward Lopes,
[Jonghyun Kim](http://j-kim.kr/),
[Peter Shirley](https://www.petershirley.com/)
and [David Luebke](https://luebke.us/)

:material-web-box: [Publisher site](https://doi.org/10.1145/3130800.3130892)
:material-newspaper-variant: [Manuscript](https://kaanaksit.files.wordpress.com/2017/08/aksitetal_siggraphasia2017_near-eye-varifocal-augmented-reality-display-using-see-through-screens1.pdf)
:material-file-video: [Video](https://www.youtube.com/watch?v=dN-8X0lUig4)
??? info ":material-tag-text: Bibtex"
	```
	@Article{Aksit2017Varifocal,
	Title      = {Near-Eye Varifocal Augmented Reality Display using See-Through Screens},
	Author     = {K. Ak{\c{s}}it and W. Lopes and J. Kim and P. Shirley and D. Luebke},
	journal    = {ACM Trans. Graph. (SIGGRAPH)},
	issue      = {36},
	number     = {6},
	year = {2017}}
	```
<br clear="left"/>
 
 



<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/varifocal_membrane.png" width="200" alt/>
</div>
**Wide field of view varifocal near-eye display using see-through deformable membrane mirrors**

<img src="https://img.shields.io/badge/-IEEE VR-important">

:fontawesome-solid-award: **Best paper award at IEEE VR 2017**

:fontawesome-solid-award: **SIGGRAPH 2017 Emerging Technologies DCEXPO Special Prize**

[David Dunn](http://www.qenops.com/), 
Cary Tippets, 
Kent Torell, 
[Petr Kellnhofer](https://kellnhofer.xyz/), 
[Kaan Akşit](https://kaanaksit.com), 
[Piotr Didyk](https://www.pdf.inf.usi.ch/people/piotr/), 
[Karol Myszkowski](https://people.mpi-inf.mpg.de/~karol/), 
[David Luebke](https://luebke.us/)
and [Henry Fuchs](https://scholar.google.com/citations?user=guhwcP8AAAAJ&hl=en)

:material-web-box: [Publisher site](https://doi.org/10.1109/TVCG.2017.2657058)
:material-web: [Project site](http://www.qenops.com/publications/)
:material-newspaper-variant: [Manuscript](https://kaanaksit.files.wordpress.com/2017/01/dunn_2017_tvcg_membranear.pdf)
:material-file-video: [Video](https://www.youtube.com/watch?v=aRZrtZfVKv0&feature=youtu.be)
??? info ":material-tag-text: Bibtex"
	```
	@article{dunn2017wide,
	title={Wide Field Of View Varifocal Near-Eye Display Using See-Through Deformable Membrane Mirrors},
	author={Dunn, David and Tippets, Cary and Torell, Kent and Kellnhofer, Petr and Ak{\c{s}}it, Kaan and Didyk, Piotr and Myszkowski, Karol and Luebke, David and Fuchs, Henry},
	journal={IEEE Transactions on Visualization and Computer Graphics},
	volume={23},
	number={4},
	pages={1322--1331},
	year={2017},
	publisher={IEEE}
	}}
	```
<br clear="left"/>
 
 


## 2016
<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/gaze_sensing_leds.png" width="200" alt/>
</div>
**Gaze-sensing LEDs for head mounted displays**

<img src="https://img.shields.io/badge/-Arxiv-yellowgreen">

[Kaan Akşit](https://kaanaksit.com), 
[Jan Kautz](https://www.jankautz.com/) 
and [David Luebke](https://luebke.us/)

:material-web-box: [Publisher site](https://arxiv.org/abs/2003.08499)
:material-newspaper-variant: [Manuscript](https://kaanaksit.files.wordpress.com/2020/02/script.pdf)
:material-file-video: [Video](https://www.youtube.com/watch?v=vV4CiNsfUYY)
??? info ":material-tag-text: Bibtex"
	```
	@article{akcsit2020gaze,
	  title={Gaze-sensing leds for head mounted displays},
	  author={Ak{\c{s}}it, Kaan and Kautz, Jan and Luebke, David},
	  journal={arXiv preprint arXiv:2003.08499},
	  year={2020}
	}
	```
<br clear="left"/>


## 2015
<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/pinhole.png" width="200" alt/>
</div>
**Slim near-eye display using pinhole aperture arrays**

<img src="https://img.shields.io/badge/-Applied Optics-blueviolet">

[Kaan Akşit](https://kaanaksit.com), 
[Jan Kautz](https://www.jankautz.com/) 
and [David Luebke](https://luebke.us/)

:material-web-box: [Publisher site](https://doi.org/10.1364/AO.54.003422)
:material-web: [Project site](https://kaanaksit.com/portfolio/slim-near-eye-display-using-pinhole-aperture-arrays/)
:material-newspaper-variant: [Manuscript](https://kaanaksit.files.wordpress.com/2014/10/slim-near-eye-display-using-pinhole-aperture-arrays.pdf)
:material-file-video: [Video](https://www.youtube.com/watch?v=UYGa6n_0aUs&)
??? info ":material-tag-text: Bibtex"
	```
	@article{Aksit:15, 
	author = {Kaan Ak\c{s}it and Jan Kautz and David Luebke}, 
	journal = {Appl. Opt.}, 
	keywords = {Apertures; Vision - binocular and stereopsis ; Computational imaging},
	number = {11}, 
	pages = {3422--3427}, 
	publisher = {OSA},
	title = {Slim near-eye display using pinhole aperture arrays}, 
	volume = {54}, 
	month = {Apr},
	year = {2015},
	url = {http://ao.osa.org/abstract.cfm?URI=ao-54-11-3422},
	doi = {10.1364/AO.54.003422},
	abstract = {We report a new technique for building a wide-angle, lightweight, thin-form-factor, cost-effective, easy-to-manufacture near-eye head-mounted display (HMD) for virtual reality applications. Our approach adopts an aperture mask containing an array of pinholes and a screen as a source of imagery. We demonstrate proof-of-concept HMD prototypes with a binocular field of view (FOV) of 70\&amp;\#xB0;\&amp;\#xD7;45\&amp;\#xB0;, or total diagonal FOV of 83\&amp;\#xB0;. This FOV should increase with increasing display panel size. The optical angular resolution supported in our prototype can go down to 1.4\&amp;\#x2013;2.1 arcmin by adopting a display with 20\&amp;\#x2013;30\&amp;\#xA0;\&amp;\#x3BC;m pixel pitch.},
	}
	```
<br clear="left"/>
 
 



## 2014
<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/head_worn_projector.png" width="200" alt/>
</div>
**Head-worn mixed reality projection display application**

<img src="https://img.shields.io/badge/-ACE-inactive">

[Kaan Akşit](https://kaanaksit.com),
Daniel Kade,
Oğuzhan Özcan 
and [Hakan Urey](https://mysite.ku.edu.tr/hurey/)

:material-web-box: [Publisher site](https://doi.org/10.1145/2663806.2663826)
:material-newspaper-variant: [Manuscript](https://kaanaksit.files.wordpress.com/2014/10/3782.pdf)
:material-file-video: [Video](https://www.youtube.com/watch?v=7F3Z-4UZWUc)
??? info ":material-tag-text: Bibtex"
	```
	@inproceedings{Aksit:2014:HMR:2663806.2663826,
	 author = {Ak\c{s}it, Kaan and Kade, Daniel and \"{O}zcan, O\u{g}uzhan and \"{U}rey, Hakan},
	 title = {Head-worn Mixed Reality Projection Display Application},
	 booktitle = {Proceedings of the 11th Conference on Advances in Computer Entertainment Technology},
	 series = {ACE '14},
	 year = {2014},
	 isbn = {978-1-4503-2945-3},
	 location = {Funchal, Portugal},
	 pages = {11:1--11:9},
	 articleno = {11},
	 numpages = {9},
	 url = {http://doi.acm.org/10.1145/2663806.2663826},
	 doi = {10.1145/2663806.2663826},
	 acmid = {2663826},
	 publisher = {ACM},
	 address = {New York, NY, USA},
	 keywords = {head-mounted projection display, immersive environments, laser projector, mixed reality, motion capture},
	} 
	```
<br clear="left"/>
 
 



<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/super_stereoscopy.png" width="200" alt/>
</div>
**Super stereoscopy technique for comfortable and realistic 3D displays**

<img src="https://img.shields.io/badge/-Optics Letters-blue">

[Kaan Akşit](https://kaanaksit.com),
Amir Niaki, 
Erdem Ulusoy 
and [Hakan Urey](https://mysite.ku.edu.tr/hurey/)

:material-web-box: [Publisher site](https://doi.org/10.1364/OL.39.006903)
:material-newspaper-variant: [Manuscript](https://kaanaksit.files.wordpress.com/2014/10/ol-39-24-6903.pdf)
??? info ":material-tag-text: Bibtex"
	```
	@article{Aksit:14, 
	author = {Kaan Ak\c{s}it and Amir Hossein Ghanbari Niaki and Erdem Ulusoy and Hakan Urey}, 
	journal = {Opt. Lett.}, 
	keywords = {Displays; Vision - binocular and stereopsis ; Visual optics, accommodation},
	number = {24}, 
	pages = {6903--6906}, 
	publisher = {OSA},
	title = {Super stereoscopy technique for comfortable and realistic 3D displays}, 
	volume = {39}, 
	month = {Dec},
	year = {2014},
	url = {http://ol.osa.org/abstract.cfm?URI=ol-39-24-6903},
	doi = {10.1364/OL.39.006903},
	abstract = {Two well-known problems of stereoscopic displays are the accommodation-convergence conflict and the lack of natural blur for defocused objects. We present a new technique that we name Super Stereoscopy (SS3D) to provide a convenient solution to these problems. Regular stereoscopic glasses are replaced by SS3D glasses which deliver at least two parallax images per eye through pinholes equipped with light selective filters. The pinholes generate blur-free retinal images so as to enable correct accommodation, while the delivery of multiple parallax images per eye creates an approximate blur effect for defocused objects. Experiments performed with cameras and human viewers indicate that the technique works as desired. In case two, pinholes equipped with color filters per eye are used; the technique can be used on a regular stereoscopic display by only uploading a new content, without requiring any change in display hardware, driver, or frame rate. Apart from some tolerable loss in display brightness and decrease in natural spatial resolution limit of the eye because of pinholes, the technique is quite promising for comfortable and realistic 3D vision, especially enabling the display of close objects that are not possible to display and comfortably view on regular 3DTV and cinema.},
	}
	```
<br clear="left"/>
 
 



<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/sound_to_light.png" width="200" alt/>
</div>
**From Sound to Sight: Using Audio Processing to enable Visible Light Communication**

<img src="https://img.shields.io/badge/-Globecom-brightgreen">

Stefan Schmid, 
D. Schwyn, 
[Kaan Akşit](https://kaanaksit.com), 
Giorgio Corbellini, 
Thomas Gross 
and [Stefan Mangold](https://www.lovefield.ch/)

:material-web-box: [Publisher site](https://doi.org/10.1109/GLOCOMW.2014.7063484)
:material-newspaper-variant: [Manuscript](https://kaanaksit.files.wordpress.com/2014/10/from-sound-to-sight-using-audio-processing-to-enable-visible-light-communication-paper.pdf)
??? info ":material-tag-text: Bibtex"
	```
	@INPROCEEDINGS{7063484,
	author={S. Schmid and D. Schwyn and K. Akşit and G. Corbellini and T. R. Gross and S. Mangold},
	booktitle={2014 IEEE Globecom Workshops (GC Wkshps)},
	title={From sound to sight: Using audio processing to enable visible light communication},
	year={2014},
	pages={518-523},
	keywords={audio signal processing;light emitting diodes;mobile handsets;optical communication;photodiodes;protocols;audio jack;audio processing;communication protocols;electrical signals;light signals;microphone input;mobile phones;on-board audio signal processing;passive components;peripheral device;photodiode;visible light communication;Decoding;Hardware;Lifting equipment;Light emitting diodes;Photodiodes;Protocols;Throughput},
	doi={10.1109/GLOCOMW.2014.7063484},
	ISSN={2166-0077},
	month={Dec},}
	```
<br clear="left"/>
 
 



<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/connect_toys.png" width="200" alt/>
</div>
**Connecting Networks of Toys and Smartphones with Visible Light Communication**

<img src="https://img.shields.io/badge/-Communications magazine-green">

Giorgio Corbellini, 
[Kaan Akşit](https://kaanaksit.com), 
[Stefan Mangold](https://www.lovefield.ch/)
Stefan Schmid 
and Thomas R. Gross

:material-web-box: [Publisher site](https://doi.org/10.1109/MCOM.2014.6852086)
:material-newspaper-variant: [Manuscript](https://kaanaksit.files.wordpress.com/2014/10/06852086.pdf)
:material-file-video: [Video](https://www.youtube.com/watch?v=10lv_FwlqMo)
??? info ":material-tag-text: Bibtex"
	```
	@ARTICLE{6852086,
	author={G. Corbellini and K. Aksit and S. Schmid and S. Mangold and T. R. Gross},
	journal={IEEE Communications Magazine},
	title={Connecting networks of toys and smartphones with visible light communication},
	year={2014},
	volume={52},
	number={7},
	pages={72-78},
	keywords={light emitting diodes;optical communication;optical receivers;smart phones;LED;VLC systems;brightness;consumer electronics;illumination;light emitting diodes;light receivers;microcontrollers;public environment;residential environment;smartphones;toys;visible light communication;wireless communication interface;Cameras;Commercialization;Frequency measurement;Illumination;Light emitting diodes;Microcontrollers;Receivers;Smart phones;Transceivers},
	doi={10.1109/MCOM.2014.6852086},
	ISSN={0163-6804},
	month={July},}
	```
<br clear="left"/>
 
 



## 2013
<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/dynamic_exit_pupil.png" width="200" alt/>
</div>
**Dynamic exit pupil trackers for autostereoscopic displays**

<img src="https://img.shields.io/badge/-Optics Express-informational">

[Kaan Akşit](https://kaanaksit.com), 
Hadi Baghsiahi, 
P. Surman, 
Selim Ӧlçer, 
E. Willman, 
David R. Selviah, 
Sally Day
and [Hakan Urey](https://mysite.ku.edu.tr/hurey/)

:material-web-box: [Publisher site](https://doi.org/10.1364/OE.21.014331)
:material-newspaper-variant: [Manuscript](https://kaanaksit.files.wordpress.com/2014/10/oe-21-12-14331.pdf)
:material-file-video: [Video](https://www.youtube.com/watch?v=Oh1xDgdbvYU)
??? info ":material-tag-text: Bibtex"
	```
	@article{Aksit:13, 
	author = {Kaan Ak\c{s}it and Hadi Baghsiahi and Phil Surman and Selim Ӧl\c{c}er and Eero Willman and David R. Selviah and Sally Day and Hakan Urey}, 
	journal = {Opt. Express}, 
	keywords = {Displays; Optical systems; Optoelectronics; Laser beam shaping; Vision - binocular and stereopsis},
	number = {12}, 
	pages = {14331--14341}, 
	publisher = {OSA},
	title = {Dynamic exit pupil trackers for autostereoscopic displays}, 
	volume = {21}, 
	month = {Jun},
	year = {2013},
	url = {http://www.opticsexpress.org/abstract.cfm?URI=oe-21-12-14331},
	doi = {10.1364/OE.21.014331},
	abstract = {This paper describes the first demonstrations of two dynamic exit pupil (DEP) tracker techniques for autostereoscopic displays. The first DEP tracker forms an exit pupil pair for a single viewer in a defined space with low intraocular crosstalk using a pair of moving shutter glasses located within the optical system. A display prototype using the first DEP tracker is constructed from a pair of laser projectors, pupil-forming optics, moving shutter glasses at an intermediate pupil plane, an image relay lens, and a Gabor superlens based viewing screen. The left and right eye images are presented time-sequentially to a single viewer and seen as a 3D image without wearing glasses and allows the viewer to move within a region of 40 cm {\texttimes} 20 cm in the lateral plane, and 30 cm along the axial axis. The second DEP optics can move the exit pupil location dynamically in a much larger 3D space by using a custom spatial light modulator (SLM) forming an array of shutters. Simultaneous control of multiple exit pupils in both lateral and axial axes is demonstrated for the first time and provides a viewing volume with an axial extent of 0.6{\textminus}3 m from the screen and within a lateral viewing angle of {\textpm} 20{\textdegree} for multiple viewers. This system has acceptable crosstalk (\&lt; 5\%) between the stereo image pairs. In this novel version of the display the optical system is used as an advanced dynamic backlight for a liquid crystal display (LCD). This has advantages in terms of overall display size as there is no requirement for an intermediate image, and in image quality. This system has acceptable crosstalk (\&lt; 5\%) between the stereo image pairs.},
	}
	```
<br clear="left"/>
 
 




<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/rotating_3d.png" width="200" alt/>
</div>
**Multi-view autostereoscopic projection display using rotating screen**

<img src="https://img.shields.io/badge/-Optics Express-informational">

:material-eye: **[Spotlight on Optics](https://www.osapublishing.org/spotlight/summary.cfm?uri=oe-21-23-29043)**

Osman Eldes, 
[Kaan Akşit](https://kaanaksit.com) 
and [Hakan Urey](https://mysite.ku.edu.tr/hurey/)

:material-web-box: [Publisher site](https://doi.org/10.1364/OE.21.029043)
:material-newspaper-variant: [Manuscript](https://kaanaksit.files.wordpress.com/2014/10/oe-21-23-29043.pdf)
:material-file-video: [Video](http://youtu.be/853-4knJ2Nc)
??? info ":material-tag-text: Bibtex"
	```
	@article{Eldes:13,
	author = {Osman Eldes and Kaan Ak\c{s}it and Hakan Urey},
	journal = {Opt. Express},
	keywords = {Displays; Diffusers; Vision - binocular and stereopsis ; Autostereoscopic displays; Brightness; Fresnel lenses; Image registration; Pico projectors; Systems design},
	number = {23},
	pages = {29043--29054},
	publisher = {OSA},
	title = {Multi-view autostereoscopic projection display using rotating screen},
	volume = {21},
	month = {Nov},
	year = {2013},
	url = {http://www.osapublishing.org/oe/abstract.cfm?URI=oe-21-23-29043},
	doi = {10.1364/OE.21.029043},
	abstract = {A new technique for multi-view autostereoscopic projection display is proposed, and demonstrated. The technique uses two mobile projectors, a rotating retro-reflective diffuser screen, and a head-tracking camera. As two dynamic viewing slits are created at the viewer's position, the slits can track the position of the eyes by rotating the screen. The display allows a viewer to move approximately 700 mm along the horizontal axis, and 500 mm along the vertical axis with an average crosstalk below 5 \%. Two screen prototypes with different diffusers have been tried, and they provide luminance levels of 60 Cd/m2, and 160 Cd/m2 within the viewing field.},
	}
	```
<br clear="left"/>
 
 



## 2012
<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/portable_3d.png" width="200" alt/>
</div>
**Portable 3D Laser Projector Using Mixed Polarization Technique**

<img src="https://img.shields.io/badge/-Journal of Display Technology-orange">

:fontawesome-solid-award: **Best 3D product award of International 3D Society (4th year)**

[Kaan Akşit](https://kaanaksit.com), 
Osman Eldeş, 
Selvan Viswanathan, 
Mark Freeman 
and [Hakan Urey](https://mysite.ku.edu.tr/hurey/)

:material-web-box: [Publisher site](https://doi.org/10.1109/JDT.2012.2205664)
:material-newspaper-variant: [Manuscript](https://kaanaksit.files.wordpress.com/2014/10/06297485.pdf)
:material-file-video: [Video](http://youtu.be/mZXOTRDEyg0)
??? info ":material-tag-text: Bibtex"
	```
	@ARTICLE{6297485,
	  author={Aksit, Kaan and Eldes, Osman and Viswanathan, Selvan and Freeman, Mark O. and Urey, Hakan},
	  journal={Journal of Display Technology}, 
	  title={Portable 3D Laser Projector Using Mixed Polarization Technique}, 
	  year={2012},
	  volume={8},
	  number={10},
	  pages={582-589},
	  doi={10.1109/JDT.2012.2205664}}
	```
<br clear="left"/>
 
 


## 2010
<div style="float: left; height:340px;" class="boxed">
<img align="left" src="media/heart_rate.png" width="200" alt/>
</div>
**Heart rate monitoring via remote photoplethysmography with motion artifacts reduction**

<img src="https://img.shields.io/badge/-Optics Express-informational">

Giovanni Cennini, 
Jeremie Arguel, 
[Kaan Akşit](https://kaanaksit.com)
and Arno van Leest

:material-web-box: [Publisher site](https://doi.org/10.1364/OE.18.004867)
:material-newspaper-variant: [Manuscript](https://kaanaksit.files.wordpress.com/2014/10/oe-18-5-4867.pdf)
:material-file-video: [Video](http://youtu.be/5X1clsjqQnw)
??? info ":material-tag-text: Bibtex"
	```
	@article{Cennini:10, 
	author = {Giovanni Cennini and Jeremie Arguel and Kaan Ak\c{s}it and Arno van Leest}, 
	journal = {Opt. Express}, 
	keywords = {Medical optics instrumentation; Optical devices; Optical sensing and sensors},
	number = {5}, 
	pages = {4867--4875}, 
	publisher = {OSA},
	title = {Heart rate monitoring via remote photoplethysmography with motion artifacts reduction}, 
	volume = {18}, 
	month = {Mar},
	year = {2010},
	url = {http://www.opticsexpress.org/abstract.cfm?URI=oe-18-5-4867},
	doi = {10.1364/OE.18.004867},
	abstract = {In this paper, we present a novel photoplethysmographic device that operates remotely, i.e. not in contact with the skin. The device allows for real time measurements of heart rate with motion artifact reduction from a distance of a few centimeters up to several meters. High mobility of users is achieved in assessment of vital body signs, such as heart rate.},
	}
	```
<br clear="left"/>
 
 


