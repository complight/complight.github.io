{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome","text":""},{"location":"#research-highlights","title":"Research Highlights","text":"Learned Display Radiance Fields with Lensless Cameras ACM SIGGRAPH Asia 2025 Technical Communications Foveation Improves Payload Capacity in Steganography ACM SIGGRAPH Asia 2025 Poster Learned Single-Pass Multitasking Perceptual Graphics for Immersive Displays ACM Multimedia 2025 Assessing Learned Models for Phase-only Hologram Compression ACM SIGGRAPH 2025 poster Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions ACM SIGGRAPH Asia 2024 Technical Communications SpecTrack: Learned Multi-Rotation Tracking via Speckle Imaging ACM SIGGRAPH 2024 poster Multi-color Holograms Improve Brightness in Holographic Displays ACM SIGGRAPH Asia 2023 Technical Papers ChromaCorrect: Prescription Correction in Virtual Reality Headsets through Perceptual Guidance Optica Biomedical Optics Express 2023 HoloBeam: Paper-Thin Near-Eye Displays IEEE VR 2023 Technical Papers Realistic Defocus Blur for Multiplane Computer-Generated Holography IEEE VR 2023 Technical Papers"},{"location":"#introduction","title":"Introduction","text":"Who are we? <p>Led by Kaan Ak\u015fit, the Computational light laboratory conducts research and development in light-related sciences, including computer-generated holography, computer graphics, computational imaging, computational displays, and visual perception. We share our scientific output in the form of published articles and papers. Our primary software toolkit to tackle our research problems is public and open-source. We host our toolkit as Odak in GitHub. We translate our scientific output to actual lecture modules, and we create useful documentation for our research and development. To read more about our recent activities, please visit our recent timeline. Our research focus in terms of applications is depicted in the below conceptual figure. </p><p> </p><p></p> Interested in registering to our seminars and discussion group? <p>Our laboratory organizes weekly seminars, and hosts a research hub in the form of a public Slack group.</p> <p>Subscribe to our mailing list and slack group</p> Where are we located? <p>The computational light laboratory is part of Computer Science Department at University College London. Computational light laboratory is located at <code>room G06, 169 Euston Road, London NW1 2AE, United Kingdom of Great Britain and Northern Ireland</code>.  </p><p> </p><p></p> Interested in joining our research group? <p>If you are interested in joining our group as an intern, an undergraduate student, a master student, a Ph.D. student, a postdoctoral researher or a visiting researcher, please do not hesitate to reach out to Kaan Ak\u015fit.</p>"},{"location":"documentation/","title":"List","text":""},{"location":"documentation/#documentation","title":"Documentation","text":"<p>This page provides links to a list of documents. These documents vary in their topics, from workplace guidances to technical digests.</p>"},{"location":"documentation/#for-candidates","title":"For candidates","text":"Documents Description <code>How to become a doctoral student in our laboratory</code> This documentation describes the steps involved in becoming a doctoral student at our laboratory."},{"location":"documentation/#public-announcements","title":"Public Announcements","text":"Documents Description Computational Light Laboratory at ACM SIGGRAPH Asia 2024 Notes on our activities and presence in ACM SIGGRAPH Asia 2024 conference Computational Light Laboratory bridges student potential into scientific success with global academic and industrial partners Notes on our activities and presence in 2025 across ACM SIGGRAPH 2025, ACM Multimedia 2025 and ACM SIGGRAPH Asia 2025 conferenes"},{"location":"documentation/#for-newcomers","title":"For newcomers","text":"Documents Description <code>Establishing yourself as a member</code> This documentation is designed to help newcomers to establish themselves as a member of the Computational light laboratory. <code>Explore the completed projects</code> This link will get you to the list of completed projects. It can be a good resource to inspire your next and help you get aligned with the rest of the team. <code>Our codebases</code> This link will get you to the list of code bases that we have compiled and published online. These codes will serve you a lot to get started with your developments. <code>Our toolkit</code> This link will get you to the repository where we host our toolkit, Odak. Remember that we rely on Odak in our research and it grows with the contributions of users and collaborators. <code>Logo of our team</code> This documentation will describe to you the meaning of our team's logo, and it is also a place to download the source file of our logo."},{"location":"documentation/#resources","title":"Resources","text":"Documents Description <code>Learn Light, Computation and Computational Light</code> This documentation will link you to our course notes. <code>Learn more about raytracing and geometric optics</code> This documentation will link you to a specific section in our course notes. <code>Learn more about computer-generated holography</code> This documentation will link you to a specific section in our course notes. <code>Explore our printable designs</code> Often times, we design and print our equipment for our laboratory experiments. This link will navigate you to a catalog of items that we designed for our experiments. <code>Tips and tricks for using our 3D printer</code> Often times, we design and print our equipment for our laboratory experiments. This link will navigate you to a catalog of items that we designed for our experiments."},{"location":"documentation/#technical-notes","title":"Technical Notes","text":"Documents <code>Creating a virtual environment for Python development</code> <code>How to add a user with controlled access to resources</code> <code>How to change name server on Ubuntu permanently</code> <code>How to copy or restore tmux settings</code>"},{"location":"documentation/3d_printing/","title":"Tips and tricks for using our 3D printer","text":""},{"location":"documentation/3d_printing/#3d-printing-at-complightlab","title":"3D Printing at CompLightLab","text":"<p>The Computational Light Laboratory has a Snapmaker 2.0 3D Printer. This page will provide some brief instructions on using the printer, and some notes on using specific types of filament.</p> <p> </p>"},{"location":"documentation/3d_printing/#software","title":"Software","text":"<p>Snapmaker provide their own slicing software, Snapmaker Luban to slice 3D models and produce the .gcode files compatible with the printer. Generally using this involves:</p> <ul> <li>Switching to 3D printing mode (click the cube icon on the left of the window).</li> <li>Adding your model using the blue <code>Add</code> button at the top left. Further models can be added by pressing <code>Add</code> again.</li> <li>Rotate, scale and arrange your model(s) for optimal printing.</li> <li>Selecting your filament material (check on the roll at the top of the printer) at the top right.</li> <li>Change settings as necessary (supports, adhesion etc.). Click <code>Customize</code> under <code>Printing Settings</code>, and then the black <code>+</code> symbol in order to change the settings (see notes below).</li> <li>Click <code>Generate G-Code</code> to slice the model.</li> <li>Check the preview of your sliced model looks OK.</li> <li>Click <code>Export G-Code to File</code> and export it onto a USB stick with appropriate filename.</li> <li>Insert the USB stick to the port on the right side of the printer.</li> <li>Select your model in the printer interface, and begin the print.</li> </ul> <p> </p> <p>Note that supports are disabled by default, but depending on your model, switching them on may be necessary. Also by default a skirt will be printed, but we recommend disabling this setting as they can be challenging to remove from the build plate (under <code>Adhesion</code>, set the type to <code>None</code>).</p> <p> </p>"},{"location":"documentation/3d_printing/#filaments","title":"Filaments","text":"<p>Profiles are provided for PLA and ABS filaments. We also have PETG filament in the lab - this can be useful as it has greater strength and temperature resistance than PLA. However it requires different temperature settings for printing. </p> <p>I recommend adding a PETG profile by clicking the black <code>+</code> symbol under Material, and setting the temperatures as appropriate. The following settings performed well with the Snapmaker official PETG filament:</p> <p> </p> <p>As a quick safety note, these settings heat up the build plate substantially more than when printing PLA. Please take extra care when removing prints, and allow the build plate to cool to a safe temperature before removing it. The temperature is displayed on the printer's screen as it cools.</p>"},{"location":"documentation/become_phd_student/","title":"Become a doctoral student","text":""},{"location":"documentation/become_phd_student/#how-do-you-become-a-doctoral-student-at-our-laboratory","title":"How do you become a doctoral student at our laboratory?","text":"<p>I wrote this documentation to shed light on the subject of how can you become a doctoral student at our laboratory. Please continue reading the steps listed below to get a better idea of the case.</p>"},{"location":"documentation/become_phd_student/#0-do-you-know-us","title":"(0) Do you know us?","text":"<p>This one is a crucial question.  In any case, whenever you apply to something, you have to understand what you are applying for. We describe ourselves in many ways within this website. As a starter, have an excursion on this website. Try to get your perspective on our research activities, where we publish, or what kind of output we generate for the public. Once you have an image in your mind about our laboratory, ask yourself if you are interested in similar lines of research.</p> <p>Info</p> <p>If the answer is yes, continue reading the next step. </p> <p>Warning</p> <p>If the answer is no, you can be in many other cool places, and please pay attention to finding your research home. Applying for a doctoral study is about you! Giving the right amount of care is highly important.</p>"},{"location":"documentation/become_phd_student/#1-are-you-unique","title":"(1) Are you unique?","text":"<p>Being exceptional may mean many things. Most importantly, being exceptional here refers to being rare. You may be thinking in a unique form, or you care about specific topics more than others. We value that kind of uniqueness. To highlight such uniqueness in your character, you need evidence from your previous experiences that can inform us to understand you better. This evidence can be all or some of these listed items:</p> <ul> <li>You are an exceptional person in your current programme (academic or industrial), and the highest-ranked person you work with is ready to support your case by providing a reference letter.</li> <li>You maintain public software or hardware related projects that are part of your previous research, and these repositories are exciting other people. </li> <li>You have helped organize events in your research field or participated in exciting venues where you demonstrated your work in public to other people, and they got excited.</li> <li>You publish at highly reputable top-tier conferences or journals that are relevant to our laboratory's work. Here are some examples of these conferences and journals: <code>ACM SIGGRAPH, ACM SIGCHI, ACM UIST, IEEE VR, IEEE ISMAR, Optica Optica, Optica Optics Express, Nature Photonics, Nature Light or alike</code>.</li> <li>(Optional -- not a must) You have an award or funding to support your doctoral studies in the future.</li> </ul> <p>Note that this list is not the advice of our university's doctoral admission process; you should check the requirements of our doctoral program before applying.</p> <p>Info</p> <p>If the answer is yes, continue reading the next step. </p> <p>Warning</p> <p>If the answer is no, you can be in many other cool places, and please pay attention to finding your research home. Applying for a doctoral study is about you! Giving the right amount of care is highly important.</p>"},{"location":"documentation/become_phd_student/#2-do-you-meet-the-requirements-of-our-doctoral-program","title":"(2) Do you meet the requirements of our doctoral program?","text":"<p>This bit is pretty significant.  Please check that you are eligible to apply to our doctoral program. Remember, you can always ask people who are in charge of our doctoral applications system, and you will find their email for enquiries on the application website.</p> <p>Info</p> <p>If the answer is yes, continue reading the next step. </p> <p>Warning</p> <p>If the answer is no, you can be in many other cool places, and please pay attention to finding your research home. Applying for a doctoral study is about you! Giving the right amount of care is highly important.</p>"},{"location":"documentation/become_phd_student/#3-are-you-ready-to-reach-to-us","title":"(3) Are you ready to reach to us?","text":"<p>Given you are at this step, it is now time for you to decide if you want to reach us. Please carefully revise your resume such that a third person can easily find your achievements and links to the evidence of your previous successes. I often talk to close collaborators, friends and family, even at this stage when I prepare submissions (e.g., grants, awards, papers, etc.). Find that trustable person to review your resume before sending it our way. Once you have your resume, prepare an email as short as possible. Make sure not to write a pages-long email.  Remember you want us to get our attention on you. Make sure to spend time finding how to describe yourself with the least amount of words. Make sure to spend time finding what you should be highlighting in your email about you (e.g., my recent superb paper, funding that I have for doctoral training, your unique disruptive line of research). Make sure to state if you have funding at hand.</p> <p>Info</p> <p>If the answer is yes, add your resume to your email and send it to kaanaksit@kaanaksit.com. </p> <p>Warning</p> <p>If the answer is no, you can be in many other cool places, and please pay attention to finding your research home. Applying for a doctoral study is about you! Giving the right amount of care is highly important.</p>"},{"location":"documentation/become_phd_student/#4-do-you-have-your-application-in-the-system","title":"(4) Do you have your application in the system?","text":"<p>Use our doctoral program's website for your application. After your submission, if all goes well, you will be interviewed by academics at our university. You can find a copy of our interview form. Note that interview questions are not limited to this specific form.</p> <p>Info</p> <p>If the answer is yes, continue reading the next step. </p> <p>Warning</p> <p>If the answer is no, you can be in many other cool places, and please pay attention to finding your research home. Applying for a doctoral study is about you! Giving the right amount of care is highly important.</p>"},{"location":"documentation/become_phd_student/#5-do-you-have-an-offer-from-our-university","title":"(5) Do you have an offer from our university?","text":"<p>Our admission and offer process can take a long time. Make sure to keep in touch with us.  Once you get your offer from the university, let us know immediately.</p> <p>Info</p> <p>If the answer is yes, continue reading the next step. </p> <p>Warning</p> <p>If the answer is no, you can be in many other cool places, and please pay attention to finding your research home. Applying for a doctoral study is about you! Giving the right amount of care is highly important.</p>"},{"location":"documentation/become_phd_student/#6-do-you-or-do-we-have-the-proper-funding-for-your-studies","title":"(6) Do you or do we have the proper funding for your studies?","text":"<p>You may have no funds at hand, or our laboratory is not able to fund you at this time. However, please make sure to contact us.  Depending on your profile, we may direct you to sources that can help you obtain your funding. Or even better, you or we may have the proper funding for your studies.</p> <p>Info</p> <p>If the answer is yes, you will be admitted to our laboratory and university. We look forward to collaborating with you and conducting exciting research with you in the future. </p> <p>Warning</p> <p>If the answer is no, you can be in many other cool places, and please pay attention to finding your research home. Applying for a doctoral study is about you! Giving the right amount of care is highly important.</p>"},{"location":"documentation/getting_started/","title":"Welcome aboard!","text":""},{"location":"documentation/getting_started/#welcome-aboard","title":"Welcome aboard!","text":"<p>Quote</p> <p>Welcome aboard!</p> <p>The computational light laboratory conducts research and development in light related sciences, including computer-generated holography, computer graphics, computational imaging, computational displays and visual perception.</p> <p>Our core mission is to show our societies that there can be better services, experiences, and goods that serve the benefits of humanity by using light. We are here to invent the next in light-based techniques and unlock the mystery of light.</p> <p>We build our tools to perform work and invent new methods to improve state of the art.  Most importantly, we document our steps so that the others can follow. Finally, we release our work to the public on our GitHub organization.  We have multiple social media outlets to promote our work.  These include our Twitter account, our LinkedIn account, our YouTube account and our webpage.  We don't shy away from going public and participate in public demonstrations with our prototypes.</p> <p>I wholeheartedly welcome every member at every stage to the Computational light laboratory. We can improve the state of the world, and I need your help in doing that!</p> <p>Kaan Ak\u015fit</p>"},{"location":"documentation/getting_started/#getting-aboard","title":"Getting aboard!","text":"<p>In the rest of this documentation, you will find a checklist that will help you establish yourself as a member of the Computational light laboratory. There is also an additional subsection that provides a list of suggestions to help you get you to establish collaborative work ethics. Note that this and the other documents that you will find on this website are always subject to change. In fact, as a member, please do not hesitate to suggest improvements and be the change by actually having a pull request in the source repository.</p>"},{"location":"documentation/getting_started/#checklist","title":"Checklist","text":"<ul> <li>Are you full registered for the graduate programme? Is all the administrator work done? Relevant contact: cs.phdadmissions@ucl.ac.uk.</li> <li>Do you know when you will receive your first paycheck? Relevant contact: cs.phdadmissions@ucl.ac.uk.</li> <li>Do you have a UCL identity card? Relevant contact: securitysystems@ucl.ac.uk.</li> <li>Do you know which building is our office building? Reach out to Kaan or any other member and ask, kaanaksit@kaanaksit.com.</li> <li>Can you get into the building where our office is using your UCL identity card? Relevant contact: facilities@cs.ucl.ac.uk.</li> <li>Do you have a desk and a chair reserved for you in the office? Relevant contact: facilities@cs.ucl.ac.uk.</li> <li>Do you know where Kaan's office is? Reach out to Kaan or any other member and ask, kaanaksit@kaanaksit.com.</li> <li>Do you know where our laboratory space is? Reach out to Kaan or any other member and ask, kaanaksit@kaanaksit.com.</li> <li>Do you have a computer to conduct your research? Reach out to Kaan and ask, kaanaksit@kaanaksit.com. For queries such as where to get a display mount, cable for this and that, try reaching out to facilities@cs.ucl.ac.uk.</li> <li>Do you have access to remote computational resources that you may be needing in your work? Reach out to Kaan and ask, kaanaksit@kaanaksit.com.</li> <li>Do you have access to hardware resources that you may be needing in your work? Reach out to Kaan and ask, kaanaksit@kaanaksit.com.</li> <li>Make sure to meet other members. Send emails! They are listed on this website. Ask about their experiences and thoughts. Explore what they are conducting in their research.</li> <li>Make sure to discuss with Kaan Ak\u015fit to see how you can contribute to Odak in the near future.</li> <li>Do you know what you will be focusing on? Do you know what projects are carried out in the team? Are you up-to-date with what the team has achieved recently?</li> <li>Are you listed as a member in the GitHub organization? In that organization which team do you belong to? Reach out to Kaan and ask, kaanaksit@kaanaksit.com.</li> <li>Do you have a weekly 1:1 meeting arranged with Kaan? Reach out to Kaan and ask, kaanaksit@kaanksit.com.</li> <li>Are you receiving calendar invitations for weekly group meetings? Reach out to Kaan or any other team member and ask, kaanaksit@kaanaksit.com.</li> <li>Do you have a research plan? What are your goals? How will your research impact the society in the near and far future? Tinker deeply in a structured manner. Agree with Kaan and your other supervisors. Reach out to them and initiate conversations about your roadmap towards your degree.</li> <li>Do you know where you can find the term dates and closures? Visit this website and this website for more.</li> <li>Do you know where you can book the meeting room 402 at 169 Euston Road? Visit CMIS GO system for booking purposes.</li> </ul>"},{"location":"documentation/getting_started/#suggestions","title":"Suggestions","text":"<p>These are some suggestions to help you get establishing yourself as a collaborative member of the group.</p> <ul> <li> <p>Install software that helps you send emails. With that software, make sure you can schedule emails. Please do not send emails to people you don't know well outside 8 am to 6 pm (unless they are in a different time zone).</p> </li> <li> <p>Life brings many challenges, and not all days are sunny. Even if communication degrades over time, keep the kindness. Control yourself. Never say anything that you will regret! (Life is not war)</p> </li> <li> <p>We are all collaborators. The best things happen when people collaborate. Being a Swiss knife is good, but there isn't a leader in history that leads no one. There was no human on this planet can exist by themself.</p> </li> <li> <p>Avoid unnecessary communication, leave others room to organize themselves.</p> </li> <li> <p>If you are angry, stand up and walk. Take a break, be somewhere else for some time. When it is time, and you are calm, come back.</p> </li> <li> <p>Smile, stand up, walk, be kind and love yourself, and respect yourself.</p> </li> <li> <p>Know yourself!</p> </li> <li> <p>Spend time to understand things if you want to be an expert in the topic. Do not worry about how much it takes, but worry if you don't understand.</p> </li> <li> <p>You will be exposed to noise most of the time in your communications. Improve your filters to extract useful information.</p> </li> <li> <p>Do it now if you can. Tomorrow will arrive with new tasks.</p> </li> <li> <p>The work is not complete until it is complete. Don't be handwavy. Ensure that you provide a working solution (not an \"it can work easily in the next step\" solution).</p> </li> <li> <p>Research impact means the beneficial application of expertise, knowledge, analysis or discovery. It can also be described as an effect on change or benefit to the economy, society, culture, public policy or services, health, the environment or quality of life beyond academia.</p> </li> <li> <p>Build it. They will come.</p> </li> </ul>"},{"location":"documentation/logo/","title":"Our laboratory's logo","text":""},{"location":"documentation/siggraph_asia_2024/","title":"Computational Light Laboratory contributes to ACM SIGGRAPH Asia 2024 with innovative computational display and lensless camera research","text":""},{"location":"documentation/siggraph_asia_2024/#computational-light-laboratory-and-its-collaborators-contribute-to-acm-siggraph-asia-2024-with-innovative-computational-display-and-lensless-camera-research","title":"Computational Light Laboratory and its collaborators contribute to ACM SIGGRAPH Asia 2024 with innovative computational display and lensless camera research","text":"<p>Written by Kaan Ak\u015fit, 15 November 2024</p> <p></p> <p>ACM SIGGRAPH Asia 2024 continues to serve as one of the leading venues in computer graphics and interactive techniques related research. This year, the conference is taking place between 3rd and 6th of December in Tokyo, Japan. We, members of Computational Light Laboratory (CLL) and its global research collaborators, are contributing to ACM SIGGRAPH Asia 2024 with our cutting edge research. Our work continues to deliver innovations in emerging fields including computational displays and lensless cameras. This document describes a brief introduction to our work presented at the conference.</p>"},{"location":"documentation/siggraph_asia_2024/#propagating-light-to-arbitrary-shaped-surfaces","title":"Propagating light to arbitrary shaped surfaces","text":"<p>Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions</p> <p>Presented by: Chuanjun Zheng and Yicheng Zhan</p> <p>Location: G510, G Block, Level 5, Tokyo International Forum, Tokyo, Japan</p> <p>Time: 11:27 am and 11:40 am JST, Wednesday 4 December 2024</p> <p>Session: Mixed Reality &amp; Holography</p> <p>Classical computer graphics work with rays to render 3D scenes, largely avoiding wave characteristics of light. In the recent years, treating light as waves in rendering pipelines has gained a strong attention with increasing interest in holographic displays and improving physical realism in computer graphics.</p> <p> </p> Our wave-based rendering method unlocks light simulation from a plane to a focal surface. <p>Chuanjun Zheng has spearheaded a new wave-based rendering method for rendering light beams emerging from a plane landing on arbitrary shaped surfaces. Previous literature on wave-based rendering typically rely on planes to propagate light in 3D, which would require multiple computational steps as it would slice a 3D scene into many planes.</p> <p> </p> Performance of our method in plane-to-plane simulation. <p>Our work enables a new way to overcome this computational complexity arising from plane to plane treatment, and unlocks a new rendering method that could propagate light beams from a plane to a focal surface. This new model could help reduce computational complexity in simulating light. Specifically, it could help verify and calculate holograms for holographic displays with much ease and lesser computation.</p> <p> </p> Our wave-based rendering method unlocks light simulation from a plane to a focal surface. <p>Chuanjun Zheng has conducted this major effort as our remote intern, showing an examplery scientific rigor and motivation. The work is also conducted in collaboration with various partners from industry and academia, including Yicheng Zhan (UCL), Liang Shi (MIT), Ozan Cakmakci (Google) and Kaan Ak\u015fit (UCL). For more technical details including manuscript, supplementary materials, examples and codebase please visit the project website.</p>"},{"location":"documentation/siggraph_asia_2024/#precisely-detecting-orientation-of-objects-remotely-in-micron-scale","title":"Precisely detecting orientation of objects remotely in micron scale","text":"<p>SpecTrack: Learned Multi-Rotation Tracking via Speckle Imaging</p> <p>Presented by: Ziyang Chen</p> <p>Location: Lobby Gallery (1) &amp; (2), G Block, Level B1, Tokyo International Forum, Tokyo, Japan</p> <p>Time: 09:00 am and 06:00 pm JST, Tuesday, 3 December 2024</p> <p>Session: Poster</p> <p>Tracking objects and people using cameras is a commonplace in computer vision and interactive techniques related research. The accuracy in terms of identifying spatial location of objects are limited in vision-based methods. The vision-based methods provide an accuracy in the ball park of several centimeters.</p> <p> </p> Our speckle imaging technique precisely tracks orientation and location of objects with coded apertures in micron-scale. <p>Ziyang Chen built a hardware setup that contains a lensless camera, a laser and a controllable stage to investigate on speckle imaging as an alternative to vision based systems in tracking of objects. Previous literature demonstrated that speckle imaging could provide micron-scale tracking ability. However, the literature was missing a technique to precisely detect orientation of objects in a 3D scene using speckle imaging. Our work addressed this gap in the literature by proposing a learned method that could precisely detect the orientation of objects in a 3D scene using speckle imaging. This innovative approach relies on our light-weight learned method and clever usage of coded apertures on objects.</p> <p> </p> A demonstration of our learned method on actual bench-top prototype. <p>Ziyang Chen, coming from computer science background, has swiftly adapted to the challenges in hardware research by building a bench-top optical setup in our laboratory space, and he managed to couple that with artificial intelligence through his learned model. Ziyang Chen conducted this research both as a part of his master thesis and his first year Ph.D. studies at UCL in the passing year and a half. Ziyang Chen collaborated with various industrial and academic researchers in this project, including Do\u011fa Do\u011fan (Adobe), Josef Spjut (NVIDIA), and Kaan Ak\u015fit (UCL). For more technical details including manuscript, supplementary materials, examples and codebase please visit the project website.</p>"},{"location":"documentation/siggraph_asia_2024/#public-outcomes","title":"Public outcomes","text":"<p>We release codebases for each project at their project websites (Focal Surface Holography and SpecTrack) for other researchers and developers to be able to replicate and compare their development with ours in the future. These codebases use modern Python programming language libraries, and our toolkit, Odak. We have integrated our models to Odak as:</p> <ul> <li><code>odak.learn.wave.focal_surface_light_propagation</code>,</li> <li><code>odak.learn.lensless.spec_track</code>.</li> </ul> <p>These model classes also have their unit tests avaible in:</p> <ul> <li><code>test/test_learn_wave_focal_surface_light_propagation.py</code>,</li> <li><code>test/test_learn_lensless_models_spec_track.py</code>.</li> </ul> <p>These models are also shipped with the new version of <code>odak==0.2.6</code> and readily available to install using <code>pip install odak</code>. To learn more about how to install odak, visit our README.md.</p>"},{"location":"documentation/siggraph_asia_2024/#photo-gallery","title":"Photo gallery","text":"<p>Here, we release photographs from our visit to the conference, highlighting parts of our SIGGRAPH Asia experience.</p> <p> </p>"},{"location":"documentation/siggraph_asia_2024/#outreach","title":"Outreach","text":"<p>We host a Slack group with more than 250 members. This Slack group focuses on the topics of rendering, perception, displays and cameras. The group is open to public and you can become a member by following this link.</p>"},{"location":"documentation/siggraph_asia_2024/#contact-us","title":"Contact Us","text":"<p>Warning</p> <p>Please reach us through email to provide your feedback and comments.</p>"},{"location":"documentation/year_at_a_glance_2025/","title":"Computational Light Laboratory bridges student potential into scientific success with global academic and industrial partners","text":""},{"location":"documentation/year_at_a_glance_2025/#computational-light-laboratory-bridges-student-potential-into-scientific-success-with-global-academic-and-industrial-partners","title":"Computational Light Laboratory bridges student potential into scientific success with global academic and industrial partners","text":"<p>Written by Kaan Ak\u015fit, 15 December 2025</p>"},{"location":"documentation/year_at_a_glance_2025/#a-year-at-a-glance-2025","title":"A year at a glance: 2025","text":"<p>Computational Light Laboratory continues its journey with a strong belief of \"groundbreaking research is powered by curiosity, collaboration, and  pursuit of excellence\" under Kaan Ak\u015fit's leadership. In 2025, Kaan is elected as an Optica's Fellow, IEEE's Senior member and SPIE's Senior member. Within 2025, Kaan also started his term as the Optica's Intelligent Interfaces and Display Technology (IT) group chair and started serving as ACM TOG Associate Editor. This year, 2025, our students have embodied our values, transforming ambitious ideas into high-impact publications and forging international partnerships that push the boundaries of perceptual graphics, computational imaging, and immersive technologies:</p> <ul> <li>Ziyang Chen\u2019s pioneering research on Learned Display Radiance Fields with Lensless Cameras accepted at ACM SIGGRAPH Asia 2025 technical communications track introduces a new, easy-to-use display calibration tool, while his proactive engagement with global experts underscores his leadership in advancing computational imaging and interactive technologies. Ziyang will participate research activities as a visiting student at the Institute of Science Tokyo, </li> <li>A dynamic collaboration between Henry Kam and Lifeng Qiu Lin, whose work on foveated steganography earned a spot at the prestigious ACM SIGGRAPH Asia 2025 poster track, embarking on Lifeng's next academic journey at Tsinghua University,</li> <li>Do\u011fa Y\u0131lmaz\u2019s journey from intern to Ph.D. student at University College London, marked by a novel deep learning model for immersive displays accepted to ACM Multimedia 2025,</li> <li>Zicong Peng\u2019s mastery of deep learning-based compression for holographic displays led to an impressive work accepted at the ACM SIGGRAPH 2025 track, showcasing his exceptional drive and technical ingenuity in shaping the future of immersive technologies, leading to his next as a Ph.D. student at Ko\u00e7 University,</li> <li>Chuanjun Zheng\u2019s remote internship bridged continents and expertise while collaborating with an external team to mentor their Ph.D. student, pioneer a learned implicit neural representation for optical lenses, and present their accepted work at ACM SIGGRAPH 2025 poster track, all before embarking on his next academic adventure at the University of Hawai\u02bbi at M\u0101noa.</li> </ul> <p>Each of our stories reflect the power of proactive mentorship and interdisciplinary teamwork with global partners in the academia and industry. Our stories are not limited to the ones that have been reported in this document and there are many in the works. These achievements are not just milestones for our students, but also a testament to the vibrant, supportive environment that fuels their success while establishing Computational Light Laboratory as true hub for innovation, research and development for people at various levels and experiences.</p> <p>Beyond our shared stories, Computational Light Laboratory and the international collaborators had presence and presentations at IEEE VR 2025 and SPIE Photonics West 2025. These presence and presentations arrived in collaboration with world leading scientists including but not limited to Yuta Itoh, Tomoya Nakamura, Rafa\u0142 Mantiuk, Dongyeon Kim, Muhammet Gen\u00e7, \u00c7a\u011fatay I\u015f\u0131l, Mona Jarrahi and Aydogan Ozcan. For a complete list of our publications, consider visiting Kaan's publications page. In the meantime, Computational Light Laboratory's outreach activities have continued with its seminars and social media groups also in 2025.</p>"},{"location":"documentation/year_at_a_glance_2025/#learned-display-radiance-fields-with-lensless-cameras","title":"Learned Display Radiance Fields with Lensless Cameras","text":"<p>Learned Display Radiance Fields with Lensless Cameras</p> <p>Presented by: Ziyang Chen, Yuta Itoh and Kaan Ak\u015fit</p> <p>Location: Hong Kong, China</p> <p>Time: 08:00 am - 06:00 pm, 15-18 December 2025</p> <p>Session: Technical Communications Track</p> <p>Ziyang Chen, a second-year Ph.D. student in the Computational Light Laboratory, is making significant strides in the fields of computational imaging, computational displays and deep learning. Ziyang's innovative research on \u201cLearned Display Radiance Fields with Lensless Cameras\u201d has been accepted as a technical communications paper at the prestigious ACM SIGGRAPH Asia 2025, the premier conference for computer graphics and interactive techniques.</p> <p> </p> our work co-designs a lensless camera and an Implicit Neural Representation based algorithm for capturing display characteristics from various viewpoints. <p>Ziyang's work represents a cutting-edge approach to integrating lensless cameras with advanced radiance field techniques to build a new easy-to-use display calibration tool, pushing the boundaries of how we capture and render visual information from a traditional or an immersive display.  Ziyang\u2019s dedication to his research is further evidenced by his proactive planning to present his findings in Hong Kong of China, where Ziyang will also engage with leading experts and expand his professional network. Beyond the conference, Ziyang Chen will also visit our close collaborator Dr. Yuta Itoh of Institute of Science Tokyo in Tokyo to foster international collaborative research visits, underscores his potential as a future leader in computational imaging and interactive technologies.</p>"},{"location":"documentation/year_at_a_glance_2025/#foveation-improves-payload-capacity-in-steganography","title":"Foveation Improves Payload Capacity in Steganography","text":"<p>Foveation Improves Payload Capacity in Steganography</p> <p>Presented by: Lifeng Qiu Lin, Henry Kam, Qi Sun and Kaan Ak\u015fit</p> <p>Location: Hong Kong, China</p> <p>Time: 08:00 am - 06:00 pm, 15-18 December 2025</p> <p>Session: Poster Track</p> <p>Lifeng Qiu Lin and Henry Kam exemplify the power of international collaboration and innovative research in perceptual graphics and steganography. Henry, who joined our Computational Light Laboratory as a remote intern in February 2024, focused on leveraging human visual perception to increase the capacity for imperceptibly hiding information in images, a breakthrough with applications in digital watermarking and immersive technologies. Meanwhile, Lifeng, a Master\u2019s student in Artificial Intelligence at University College London, developed a method to increase data storage in images by up to five times, building on insights from the human visual system and Henry's earlier findings in the context of Lifeng's master thesis. Their collaboration culminated in a novel, learned approach using variational autoencoders  to embed additional data in images by exploiting peripheral vision degradation.</p> <p> </p> With support of efficient latent representations and foveated rendering, we trained models that improve existing capacity limits from 100 to 500 bits. <p>This joint work was accepted as a poster, \u201cFoveation Improves Payload Capacity in Steganography,\u201d at ACM SIGGRAPH Asia 2025, the premier venue for computer graphics and interactive technologies. Beyond research, both Henry and Lifeng demonstrated exceptional collaborative and teaching skills, organizing regular meetings, sharing knowledge with the broader lab, and contributing to educational resources on human visual perception. Their success from independent research to high-impact publication showcases the transformative potential of curiosity, teamwork, and perseverance in academic and professional growth. Lifeng Qiu Lin continues his academic journey as a visiting student at the prestigious Tsinghua University under the supervision of Dr. Xiaolin Hu, while Henry Kam is finalizing his master studies under the supervision Dr. Qi Sun at New York University.</p>"},{"location":"documentation/year_at_a_glance_2025/#learned-single-pass-multitasking-perceptual-graphics-for-immersive-displays","title":"Learned Single-Pass Multitasking Perceptual Graphics for Immersive Displays","text":"<p>Learned Single-Pass Multitasking Perceptual Graphics for Immersive Displays</p> <p>Presented by: Do\u011fa Y\u0131lmaz, He Wang, Towaki Takikawa, Duygu Ceylan and Kaan Ak\u015fit</p> <p>Location: Dublin, Ireland</p> <p>Time: 4:30 pm - 6:00 pm, 31 October 2025</p> <p>Session: Regular Papers</p> <p>Do\u011fa Y\u0131lmaz reached Kaan Ak\u015fit via email around mid-September in 2023, expressing his interest in becoming a research intern at the Computational Light Laboratory. These early discussions between Kaan and Do\u011fa have led to identifying Do\u011fa's motivations in research and long-term plans involving obtaining a Ph.D. degree at reputable universities in the United Kingdom and Northern Ireland. Specifically, Kaan and Do\u011fa have agreed to collaborate on themes of perceptual graphics and deep learning methods aimed at applications such as virtual reality. During Do\u011fa's internship at the Computational Light Laboratory, he demonstrated persistent motivation and progress toward developing multitasking convolutional neural networks and perceptual graphics methods. His work involved studying human vision and exploring rendering techniques for immersive displays that cater to human observers in a highly realistic manner. These studies included foveated rendering methods used in virtual reality headsets, chromastereopsis rendering for pseudo-3D effects on all display types, denoising techniques for improving image quality, and quantization methods for low-and-high dynamic images.</p> <p> </p>  Given RGB images and text-prompts, our model performs text-described perceptual tasks in a single inference step. <p>A significant achievement during his internship was the development of a single deep learning model that could modify images according to text prompts. This enabled a unified approach for computer graphics applications across immersive displays such as virtual reality headsets, augmented reality glasses, and 3D desktop displays. Do\u011fa's well-established development and carefully written documentation led him to submit his work to the prestigious ACM Multimedia 2025 conference, which subsequently accepted it in their regular paper track. This accomplishment not only resulted in a high-tier publication but also provided solid evidence for his Ph.D. applications. Currently, Do\u011fa is a first-year Ph.D. student supervised by Dr. He Wang at the University College London's computer science department. Do\u011fa's academic progress and achievements through his internship, as well as his success in publishing and securing a Ph.D. admission, demonstrate an exemplary success story for students aiming to enhance their academic profiles through hard work and persistence.</p>"},{"location":"documentation/year_at_a_glance_2025/#assessing-learned-methods-for-hologram-compression","title":"Assessing Learned Methods for Hologram Compression","text":"<p>Assessing Learned Models for Phase-only Hologram Compression</p> <p>Presented by: Zicong Peng, Yicheng Zhan, Josef Spjut, and Kaan Ak\u015fit</p> <p>Location: Convention Centre, Vancouver, BC, Canada</p> <p>Time: Monday-Thursday, 10-14 August 2025</p> <p>Session: Poster Track</p> <p>Zicong Peng demonstrated exceptional motivation and outstanding implementation skills throughout his master\u2019s thesis work. Zicong dove into a topic entirely new to him: \"Deep Learning based Compression methods for Holographic Displays\". These displays are an emerging field with transformative potential for next-generation augmented reality glasses, virtual reality headsets, and desktop 3D displays. Firstly, Zicong quickly mastered the replication of established hologram simulation algorithms involving light propagation techniques in free space, and learned to simulate holograms as if displayed using a holographic display. Through a rigorous series of experiments, Zicong generated structured guidance on the performance of learned hologram compression algorithms. In this context, Zicong is focused on Variational Autoencoder and Neural Implicit Representation structures. Zicong's well prepared documentation and assessments formulated the basis of a submission to the prestigious ACM SIGGRAPH 2025 Poster Track.</p> <p> </p> We evaluate the performance of four common learned models utilizing Implicit Neural Representation (INR) and Variational Autoencoder (VAE) structures for compressing phase-only holograms in holographic displays. <p>Zicong further elevated the quality of his work by collaborating with industrial and academic partners, including Josef Spjut of NVIDIA, Yicheng Zhan of UCL and Kaan Ak\u015fit. As of 10 August 2025, Zicong Peng's work was accepted and presented at SIGGRAPH 2025. Zicong\u2019s journey serves as an exemplary model for beginners in any scientific field, proving that motivation, attentive mentorship, and active engagement with the collaborators can lead to remarkable success. Throughout this process, he developed a robust technical skillset, a deep understanding of scientific methodology, and improved communication skills\u2014opening doors to future Ph.D. studentship opportunities in Ko\u00e7 University under the guidance of Professor Hakan Urey.</p>"},{"location":"documentation/year_at_a_glance_2025/#implicit-neural-representations-for-optical-raytracing","title":"Implicit Neural Representations for Optical Raytracing","text":"<p>Efficient Proxy Raytracer for Optical Systems using Implicit Neural Representations</p> <p>Presented by: Shiva Sinaei, Chuanjun Zheng, Kaan Ak\u015fit, and Daisuke Iwai.</p> <p>Location: Convention Centre, Vancouver, BC, Canada</p> <p>Time: Monday-Thursday, 10-14 August 2025</p> <p>Session: Poster Track</p> <p>Chuanjun Zheng concluded his remote internship in our laboratory, collaborating with external partners from the University of Osaka, more specifically Professor Daisuke Iwai's team. During this collaboration, Chuanjun mentored first year Ph.D. student Shiva Sinaei by sharing insights from his earlier experimtns on implicit neural representations. Together, Chuanjun and Shiva identified an implicit neural representation capable of modeling optical lenses as a learned component.</p> <p> </p> We propose Ray2Ray, a novel method that leverages implicit neural representations to model optical systems with greater efficiency, eliminating the need for surface-by-surface computations in a single pass end-to-end model. <p>Their work demonstrated that this learned lens representation could accurately raytrace optical beams for simple imaging tasks, an interesting starting point for a more complete investigation in the future. Chuanjun and Shiva's work, as of 10 August 2025, was accepted and presented at ACM SIGGRAPH 2025. As Chuanjun wraps up his internship, Chuanjun is set to begin a new chapter at the University of Hawai'i at M\u0101noa, where Chuanjun will work under the supervision of Dr. Huaijin (George) Chen.</p>"},{"location":"documentation/year_at_a_glance_2025/#outreach","title":"Outreach","text":"<p>We host a Slack group with more than 250 members. This Slack group focuses on the topics of rendering, perception, displays and cameras. The group is open to public and you can become a member by following this link.</p>"},{"location":"documentation/year_at_a_glance_2025/#contact-us","title":"Contact Us","text":"<p>Warning</p> <p>Please reach us through email to provide your feedback and comments.</p>"},{"location":"documentation/computing/creating_a_virtual_environment_for_Python_development/","title":"Creating a virtual environment for Python Development","text":""},{"location":"documentation/computing/creating_a_virtual_environment_for_Python_development/#creating-a-virtual-environment-for-python-development","title":"Creating a virtual environment for Python development","text":"<p>Written by Kaan Ak\u015fit, 20 February 2025</p> <p>Disclamer: This article is only tested under Ubuntu 24.10.</p> <p>Development often requires installing custom components in a system. It is helpful to consider having a virtual environment, which you can also carry over to the other computers as needed. To create your own virtual environment, please use the following syntax in your favorite terminal program:</p> <pre><code>python3 -m venv PATH\n</code></pre> <p>Here, <code>PATH</code> should be replaced with the directory location where you want to install your virtual envorinment. Once you created your virtual environment, you can activate the environment by typing the following syntax in your terminal:</p> <pre><code>source PATH/bin/activate\n</code></pre> <p>If you need this working envorinment to be default, you can add the above line at the end of your  <code>~/bash.rc</code> in your home directory. This way, it will automatically be activated when you start a new terminal session. Given you have successfully created your virtual environment, you can now install packages using <code>pip3</code> without breaking the system packages:</p> <pre><code>pip3 install PACKAGE\n</code></pre> <p>where, <code>PACKAGE</code> is the name of the package you want to install.</p>"},{"location":"documentation/computing/how_to_add_a_user_with_controlled_access_to_resources/","title":"How to add a user with controlled access resources","text":""},{"location":"documentation/computing/how_to_add_a_user_with_controlled_access_to_resources/#how-to-add-a-user-with-controlled-access-to-resources","title":"How to add a user with controlled access to resources","text":"<p>Written by Kaan Ak\u015fit, 18 August 2025</p> <p>Disclamer: This article is only tested under Ubuntu 24.10.</p> <p>We use Linux based servers on our computers dedicated to computational tasks. In a dynamic setting like our research laboratory, there are many students that join and leave the group as they complete their studies. The access to these computational resources is cruical for the students and they needed to be provided an isolated session, where they do not interfere with other users' resources. This guide delves into the user management of students in our servers.</p>"},{"location":"documentation/computing/how_to_add_a_user_with_controlled_access_to_resources/#setting-important-variables","title":"Setting important variables","text":"<p>There are two things important in this exercise as variables. These are the username and user directory location:</p> <pre><code>export USERNAMEGOESHERE=\"test\"\nexport USERDIRECTORY=\"/YOURDIRECTORY/$USERNAMEGOESHERE\"\n</code></pre>"},{"location":"documentation/computing/how_to_add_a_user_with_controlled_access_to_resources/#creating-a-user","title":"Creating a user","text":"<p>We can create a new user using the syntax below:</p> <pre><code>sudo adduser $USERNAMEGOESHERE\n</code></pre> <p>To see which groups this newly created user belongs to, we can use the following syntax at our command line:</p> <pre><code>group $USERNAMEGOESHERE\n</code></pre> <p>Some cases demand that the user is restricted in terms of access to certain resources. This could be established by considering various aspects, but one simple aspect is related to user rights. In other terms, it is important to carefully determine which user groups that the user belongs to. In order to add this new user to a specific group, we user the following syntax at our command line:</p> <pre><code>sudo usermod -aG GROUPNAME $USERNAMEGOESHERE\n</code></pre>"},{"location":"documentation/computing/how_to_add_a_user_with_controlled_access_to_resources/#deleting-or-locking-a-user","title":"Deleting or locking a user","text":"<p>As a student leaves or a certain resource has to be removed from a certain user, deleting or locking a user account becomes a topic to consider. Let's dive deep into how we can do that in these subsections.</p>"},{"location":"documentation/computing/how_to_add_a_user_with_controlled_access_to_resources/#deletion","title":"Deletion","text":"<p>The created user could be deleted at any time using the following commands:</p> <pre><code>sudo deluser $USERNAMEGOESHERE\n</code></pre> <p>In order to remove the home directory of the user, the following commands should be used instead:</p> <pre><code>sudo deluser --remove-home $USERNAMEGOESHERE\nsudo rm -Rf $USERDIRECTORY\nsudo rm -Rf /home/$USERNAMEGOESHERE\n</code></pre>"},{"location":"documentation/computing/how_to_add_a_user_with_controlled_access_to_resources/#locking-and-unlocking","title":"Locking and unlocking","text":"<p>We can lock a user's password to prevent that user from authenticating. Therefore, the user will not be able to reach their account through graphical user interface or terminal or ssh. To lock a user, the following syntax must be used in a linux shell:</p> <pre><code>sudo passwd -l $USERNAMEGOESHERE\n</code></pre> <p>To unlock the user account at any given time, the following syntax must be used:</p> <pre><code>sudo passwd -u $USERNAMEGOESHERE\nsudo usermod --expiredate \"\" $USERNAMEGOESHERE\n</code></pre>"},{"location":"documentation/computing/how_to_add_a_user_with_controlled_access_to_resources/#restricting-access-to-home-directory-only","title":"Restricting access to home directory only","text":"<p>By default, a new user can only access their own home directory and some system directories (like /tmp). To further restrict access, you can set their shell to a restricted shell or use chmod to limit directory permissions.</p>"},{"location":"documentation/computing/how_to_add_a_user_with_controlled_access_to_resources/#restricting-a-users-shelll","title":"Restricting a user's shelll","text":"<pre><code>sudo usermod -s /bin/rbash $USERNAMEGOESHERE\n</code></pre> <p>Above commands will restrict the user to their home directory and prevent them from using cd to navigate outside of it. To remove this restriction:</p> <pre><code>sudo usermod -s /bin/bash $USERNAMEGOESHERE\n</code></pre>"},{"location":"documentation/computing/how_to_add_a_user_with_controlled_access_to_resources/#creating-a-custom-environment","title":"Creating a custom environment","text":"<p>Firstly, create directories for the user in their home directory:</p> <pre><code>sudo mkdir -p $USERDIRECTORY/$USERNAMEGOESHERE/{bin,lib,lib/python3,lib/python3/dist-packages/,lib/x86_64-linux-gnu,lib64,dev,etc,usr,usr/bin,usr/lib,usr/lib/python3.*,usr/lib/x86_64-linux-gnu,usr/local/cuda-12.8/targets/x86_64-linux/lib/stubs/,usr/lib/x86_64-linux-gnu/stubs,home,home/%USERNAME}\n</code></pre> <p>Collect information on required libraries for your shell:</p> <pre><code>ldd /bin/bash\n</code></pre> <p>At the time of this writing, I have identified the following:</p> <pre><code>sudo cp -v /lib/x86_64-linux-gnu/libc.so.6 $USERDIRECTORY/$USERNAMEGOESHERE/lib/x86_64-linux-gnu/\nsudo cp -v /lib64/ld-linux-x86-64.so.2 $USERDIRECTORY/$USERNAMEGOESHERE/lib64/\n\nsudo cp -v /usr/lib/x86_64-linux-gnu/libcuda.* $USERDIRECTORY/$USERNAMEGOESHERE/usr/lib/x86_64-linux-gnu/\nsudo cp -v /usr/lib/x86_64-linux-gnu/libnvidia-* $USERDIRECTORY/$USERNAMEGOESHERE/usr/lib/x86_64-linux-gnu/\nsudo cp -v /lib64/ld-linux-x86-64.so.2 $USERDIRECTORY/$USERNAMEGOESHERE/lib64/\n\nsudo cp -v /usr/lib/x86_64-linux-gnu/{libc.so.6,libpcre2-8.so.0,libselinux.so.1,libtinfo.so.6,libnvidia-ml.so.1,libnvidia-ml.so.560.35.05,librt.so.1,libexpat.so.1,libz.so.1,libpthread.so.0,libdl.so.2,libnvidia-ml.so.1} $USERDIRECTORY/$USERNAMEGOESHERE/usr/lib/x86_64-linux-gnu\n\nsudo cp -rv /usr/lib/python3.* $USERDIRECTORY/$USERNAMEGOESHERE/usr/lib/\n\nsudo cp -v /usr/lib/x86_64-linux-gnu/stubs/libnvidia-ml.so $USERDIRECTORY/$USERNAMEGOESHERE/usr/lib/x86_64-linux-gnu/stubs/\n\nsudo cp -Rv /usr/lib/python3.12 $USERDIRECTORY/$USERNAMEGOESHERE/usr/lib\nsudo cp -Rv /usr/lib/python3 $USERDIRECTORY/$USERNAMEGOESHERE/usr/lib\nsudo cp -rv /usr/lib/python3.* $USERDIRECTORY/$USERNAMEGOESHERE/usr/lib/\n\nsudo cp -v /usr/local/cuda-12.8/targets/x86_64-linux/lib/stubs/libnvidia-ml.so $USERDIRECTORY/$USERNAMEGOESHERE/usr/local/cuda-12.8/targets/x86_64-linux/lib/stubs/\n</code></pre> <p>We also need the nodes:</p> <pre><code>sudo mknod -m 666 $USERDIRECTORY/$USERNAMEGOESHERE/dev/null c 1 3\nsudo mknod -m 666 $USERDIRECTORY/$USERNAMEGOESHERE/dev/zero c 1 5\nsudo mknod -m 666 $USERDIRECTORY/$USERNAMEGOESHERE/dev/random c 1 8\nsudo mknod -m 666 $USERDIRECTORY/$USERNAMEGOESHERE/dev/urandom c 1 9\n\nsudo cp -a /dev/nvidia* $USERDIRECTORY/$USERNAMEGOESHERE/dev/\n</code></pre> <p>Make sure that the user has the access to the right set of tools:</p> <pre><code>sudo cp -v /bin/{bash,rbash,nano,vim,ls,mv,clear_console,cp,python3,pip3,nvidia-smi} $USERDIRECTORY/$USERNAMEGOESHERE/bin\n\nsudo cp -v /usr/bin/{pip3,pip,python3} $USERDIRECTORY/$USERNAMEGOESHERE/usr/bin\nsudo cp -vf /etc/{passwd,group} $USERDIRECTORY/$USERNAMEGOESHERE/etc/\n\nsudo grep root /etc/passwd | sudo tee $USERDIRECTORY/$USERNAMEGOESHERE/etc/passwd\nsudo grep root /etc/group | sudo tee $USERDIRECTORY/$USERNAMEGOESHERE/etc/group\necho \"$USERNAMEGOESHERE:x:$(id -u $USERNAMEGOESHERE):$(id -g $USERNAMEGOESHERE):User:$USERDIRECTORY/$USERNAMEGOESHERE:/bin/bash\" | sudo tee -a $USERDIRECTORY/$USERNAMEGOESHERE/etc/passwd\n</code></pre> <p>Make sure that the rights are also correct for the user directories:</p> <pre><code>sudo chown -R $USERNAMEGOESHERE:$USERNAMEGOESHERE $USERDIRECTORY/$USERNAMEGOESHERE\nsudo chmod 700 $USERDIRECTORY/$USERNAMEGOESHERE\nsudo chmod 700 $USERDIRECTORY/$USERNAMEGOESHERE/.ssh\nsudo chmod 600 $USERDIRECTORY/$USERNAMEGOESHERE/.ssh/authorized_keys\nsudo chown root:root $USERDIRECTORY/$USERNAMEGOESHERE\nsudo chmod 755 $USERDIRECTORY/$USERNAMEGOESHERE\n</code></pre> <p>Given your user will connect via ssh, you would also need to make sure that ssh is configured correctly, for that modify <code>/etc/ssh/sshd_config</code> file and append the following at the end of the file:</p> <pre><code>Match User $USERNAMEGOESHERE\n    ChrootDirectory $USERDIRECTORY/$USERNAMEGOESHERE\n    AllowTcpForwarding no\n    X11Forwarding no\n</code></pre> <p>You may also want to make sure that this user is allowed or denied at any time by using the following lines in the <code>/etc/ssh/sshd_config</code>:</p> <pre><code>DenyUsers USER0 USER1\nAllowUsers USER2, $USERNAMEGOESHERE\n</code></pre> <p>Once modified and saved, simply restart the SSH server:</p> <pre><code>sudo systemctl restart ssh\n</code></pre>"},{"location":"documentation/computing/how_to_add_a_user_with_controlled_access_to_resources/#an-automated-script-to-create-a-user","title":"An automated script to create a user","text":"<p>In the light of the above findings and exploration, I have created the following script to create users with GPU access on our computational resources:</p> <pre><code>#!/bin/bash\n\nUSERNAMEGOESHERE=\"test\"\nNAMEOFYOURDIRECTORY=\"FULLPATHGOESHERE\"\nCRONTABSCRIPT=\"FULLPATHGOESHERE\"\nUSERDIRECTORY=\"/$NAMEOFYOURDIRECTORY/$USERNAMEGOESHERE\"\nUBUNTUCODENAME=\"plucky\"\n\nsudo chown root:root /$NAMEOFYOURDIRECTORY\nsudo chown root:root /$USERDIRECTORY\nsudo chmod 0755 /$NAMEOFYOURDIRECTORY\nsudo chmod 0755 /$USERDIRECTORY\n\nsudo adduser $USERNAMEGOESHERE\nsudo usermod -d $USERDIRECTORY $USERNAMEGOESHERE\n\nSSH_KEY0=\"ssh-ed25519 YOURKEY0 COMPUTER0\"\nSSH_KEY1=\"ssh-ed25519 YOURKEY1 COMPUTER1\"\n\nsudo debootstrap --variant=minbase --include=python3,git,vim,nano,rsync $UBUNTUCODENAME $USERDIRECTORY\n\nsudo chown root:root $USERDIRECTORY\nsudo chmod 0755 $USERDIRECTORY\n\n\nsudo chroot $USERDIRECTORY /bin/bash -c \"echo \\\"deb http://archive.ubuntu.com/ubuntu $UBUNTUCODENAME main restricted universe multiverse\\\" | tee -a /etc/apt/sources.list\"\nsudo chroot $USERDIRECTORY /bin/bash -c \"apt update\"\nsudo chroot $USERDIRECTORY /bin/bash -c \"apt install ca-certificates -y\"\nsudo chroot $USERDIRECTORY /bin/bash -c \"apt install git -y\"\nsudo chroot $USERDIRECTORY /bin/bash -c \"apt install wget -y\"\nsudo chroot $USERDIRECTORY /bin/bash -c \"apt install ssh -y\"\nsudo chroot $USERDIRECTORY /bin/bash -c \"apt install keychain -y\"\nsudo chroot $USERDIRECTORY /bin/bash -c \"apt install tmux -y\"\nsudo chroot $USERDIRECTORY /bin/bash -c \"apt install sshfs -y\"\nsudo chroot $USERDIRECTORY /bin/bash -c \"apt install libglu1-mesa-dev -y\"\nsudo chroot $USERDIRECTORY /bin/bash -c \"apt install iputils-ping -y\"\nsudo chroot $USERDIRECTORY /bin/bash -c \"apt install python3-pip -y\"\nsudo chroot $USERDIRECTORY /bin/bash -c \"apt install python3-venv -y\"\n\n\nsudo cp -vf /etc/{passwd,group} $USERDIRECTORY/etc/\nsudo grep root /etc/passwd | sudo tee $USERDIRECTORY/etc/passwd\nsudo grep root /etc/group | sudo tee $USERDIRECTORY/etc/group\nsudo grep $USERNAMEGOESHERE /etc/group | sudo tee $USERDIRECTORY/etc/group\necho \"$USERNAMEGOESHERE:x:$(id -u $USERNAMEGOESHERE):$(id -g $USERNAMEGOESHERE):User:$USERDIRECTORY/$USERNAMEGOESHERE:/bin/bash\" | sudo tee -a $USERDIRECTORY/etc/passwd\n\nsudo cp -v /bin/nvidia-* $USERDIRECTORY/bin\nsudo cp -v /usr/lib/x86_64-linux-gnu/libcuda.* $USERDIRECTORY/usr/lib/x86_64-linux-gnu/\nsudo cp -v /usr/lib/x86_64-linux-gnu/libnvidia-* $USERDIRECTORY/usr/lib/x86_64-linux-gnu/\nsudo mkdir -p $USERDIRECTORY/usr/local/cuda-12.8/targets/x86_64-linux/lib/stubs/\nsudo cp -v /usr/local/cuda-12.8/targets/x86_64-linux/lib/stubs/libnvidia-ml.so $USERDIRECTORY/usr/local/cuda-12.8/targets/x86_64-linux/lib/stubs/\n\nsudo mkdir -p $USERDIRECTORY/./$USERDIRECTORY\nsudo chown -R $USERNAMEGOESHERE:$USERNAMEGOESHER $USERDIRECTORY/$USERDIRECTORY\nsudo cp /etc/resolv.conf $USERDIRECTORY/etc/resolv.conf\n\nsudo mount -o bind /dev $USERDIRECTORY/dev\nsudo mount -t proc none $USERDIRECTORY/proc\nsudo mount -t sysfs none $USERDIRECTORY/sys\nsudo mount -t devpts none $USERDIRECTORY/dev/pts\nsudo chmod 777 $USERDIRECTORY/dev/shm\n\nsudo mkdir -p $USERDIRECTORY/.ssh\nsudo touch $USERDIRECTORY/.ssh/authorized_keys\nsudo echo \"$SSH_KEY0\" | sudo tee -a $USERDIRECTORY/.ssh/authorized_keys\nsudo echo \"$SSH_KEY1\" | sudo tee -a $USERDIRECTORY/.ssh/authorized_keys\nsudo chown -R $USERNAMEGOESHERE:$USERNAMEGOESHERE $USERDIRECTORY/.ssh/\n\nsudo echo \" \" | sudo tee -a /etc/ssh/sshd_config\nsudo echo \"Match User $USERNAMEGOESHERE\" | sudo tee -a /etc/ssh/sshd_config\nsudo echo \"    ChrootDirectory $USERDIRECTORY\" | sudo tee -a /etc/ssh/sshd_config\nsudo echo \"    Subsystem    sftp    /usr/lib/openssh/sftp-server\" | sudo tee -a /etc/ssh/sshd_config\nsudo echo \"    AllowTcpForwarding no\" | sudo tee -a /etc/ssh/sshd_config\nsudo echo \"    X11Forwarding no\" | sudo tee -a /etc/ssh/sshd_config\nsudo service ssh restart\n\n\nsudo touch $USERDIRECTORY/$USERDIRECTORY/.bashrc\nsudo touch $USERDIRECTORY/$USERDIRECTORY/.bash_profile\nsudo echo \"if [ -f ~/.bashrc ]; then\" | sudo tee -a $USERDIRECTORY/$USERDIRECTORY/.bash_profile\nsudo echo \"    . ~/.bashrc\" | sudo tee -a $USERDIRECTORY/$USERDIRECTORY/.bash_profile\nsudo echo \"fi\" | sudo tee -a $USERDIRECTORY/$USERDIRECTORY/.bash_profile\n\n\nsudo echo \" \" | sudo tee -a $CRONTABSCRIPT\nsudo echo \" \" | sudo tee -a $CRONTABSCRIPT\nsudo echo \"USERDIRECTORY=\\\"$USERDIRECTORY\\\"\" | sudo tee -a $CRONTABSCRIPT\nsudo echo \"sudo mount -t devtmpfs devtmpfs $USERDIRECTORY/dev\" | sudo tee -a $CRONTABSCRIPT\nsudo echo \"sudo mount -t devpts devpts $USERDIRECTORY/dev/pts\" | sudo tee -a $CRONTABSCRIPT\nsudo echo \"sudo mount -t proc proc $USERDIRECTORY/proc\" | sudo tee -a $CRONTABSCRIPT\nsudo echo \"sudo mount -t sysfs sysfs $USERDIRECTORY/sys\" | sudo tee -a $CRONTABSCRIPT\n\nsudo chown -R $USERNAMEGOESHERE:$USERNAMEGOESHERE $USERDIRECTORY/$USERDIRECTORY/\n</code></pre> <p>You may also need to have a separate script to mount required folders at each reboot for a complete setup:</p> <pre><code>#!/bin/bash\n\nUSERDIRECTORY=\"/users/chroot/directory/goes/here\"\nsudo mount -t devtmpfs devtmpfs $USERDIRECTORY/dev\nsudo mount -t devpts devpts $USERDIRECTORY/dev/pts\nsudo mount -t proc proc $USERDIRECTORY/proc\nsudo mount -t sysfs sysfs $USERDIRECTORY/sys\n</code></pre> <p>You can run the above setup at each boot using <code>crontab</code>.</p>"},{"location":"documentation/computing/how_to_add_a_user_with_controlled_access_to_resources/#limiting-what-users-can-monitor","title":"Limiting what users can monitor","text":"<p>You may want to limit users' ability to monitor other people's login times and active processes belonging to other users. Firstly, you can edit <code>/etc/ssh/sshd_config</code> by making sure the following values are registered accurately:</p> <pre><code>PrintMotd no\nPrintLastLog no\n</code></pre> <p>Secondly, you can mask user names in process monitoring by adding the following line to <code>sudo nano /etc/pam.d/common-session</code>:</p> <pre><code>session    optional    perm_setcred:/etc/xdm/xlock\n</code></pre> <p>As you edit these lines, as usual, restart <code>ssh</code> service by typing <code>sudo service ssh restart</code> and reboot the operating system using <code>sudo reboot</code> commands.</p>"},{"location":"documentation/computing/how_to_change_name_server_on_ubuntu_permanently/","title":"How to change name server on Ubuntu permanently","text":""},{"location":"documentation/computing/how_to_change_name_server_on_ubuntu_permanently/#how-to-change-name-server-on-ubuntu-permanently","title":"How to change name server on Ubuntu permanently","text":"<p>Written by Kaan Ak\u015fit, 20 February 2025</p> <p>Disclamer: This article is only tested under Ubuntu 24.10.</p> <p>I encountered this issue with an institution that I had a few computers running in their office spaces. Any ethernet connection I had in that place with any computers lead to this strange nameserver and blocking me having an access to webpages over the internet. </p> <p>To force a certain name server in <code>Ubuntu</code>, firstly add these name servers to <code>/etc/systemd/resolved.conf</code> as in this example:</p> <pre><code>#  This file is part of systemd.\n#\n#  systemd is free software; you can redistribute it and/or modify it under the\n#  terms of the GNU Lesser General Public License as published by the Free\n#  Software Foundation; either version 2.1 of the License, or (at your option)\n#  any later version.\n#\n# Entries in this file show the compile time defaults. Local configuration\n# should be created by either modifying this file (or a copy of it placed in\n# /etc/ if the original file is shipped in /usr/), or by creating \"drop-ins\" in\n# the /etc/systemd/resolved.conf.d/ directory. The latter is generally\n# recommended. Defaults can be restored by simply deleting the main\n# configuration file and all drop-ins located in /etc/.\n#\n# Use 'systemd-analyze cat-config systemd/resolved.conf' to display the full config.\n#\n# See resolved.conf(5) for details.\n\n[Resolve]\n# Some examples of DNS servers which may be used for DNS= and FallbackDNS=:\n# Cloudflare: 1.1.1.1#cloudflare-dns.com 1.0.0.1#cloudflare-dns.com 2606:4700:4700::1111#cloudflare-dns.com 2606:4700:4700::1001#cloudflare-dns.com\n# Google:     8.8.8.8#dns.google 8.8.4.4#dns.google 2001:4860:4860::8888#dns.google 2001:4860:4860::8844#dns.google\n# Quad9:      9.9.9.9#dns.quad9.net 149.112.112.112#dns.quad9.net 2620:fe::fe#dns.quad9.net 2620:fe::9#dns.quad9.net\nDNS= 1.1.1.1 9.9.9.9\nFallbackDNS= 8.8.8.8\n#Domains=\n#DNSSEC=no\n#DNSOverTLS=no\n#MulticastDNS=no\n#LLMNR=no\n#Cache=yes\n#CacheFromLocalhost=no\nDNSStubListener=no\n#DNSStubListenerExtra=\n#ReadEtcHosts=yes\n#ResolveUnicastSingleLabel=no\n#StaleRetentionSec=0\n</code></pre> <p>Note that it is critical to set <code>DNSStubListener=no besides</code>DNS<code>and</code>FallbackDNS` keys. Later on, you can use the classical network manager to set the name server of your choice as in this screenshot:</p> <p></p> <p>Once you made these changes, simply reboot your machine. This should give you a permanent name server setting which you could change as you wish in the future.</p>"},{"location":"documentation/computing/how_to_copy_or_restore_tmux_settings/","title":"How to copy or restore tmux settings","text":""},{"location":"documentation/computing/how_to_copy_or_restore_tmux_settings/#how-to-copy-or-restore-tmux-settings","title":"How to copy or restore tmux settings","text":"<p>Written by Kaan Ak\u015fit, 7 March 2025</p> <p>Disclamer: This article is only tested under Ubuntu 24.10.</p> <p><code>tmux</code> is a powerful terminal tool for keeping tabs in your terminal alive. Their source code and documentation is readily available in their codebase. This document can help a person willing to restore or copy for various reasons (e.g., broken installation, cloning settings to a new computer or alike).</p>"},{"location":"documentation/computing/how_to_copy_or_restore_tmux_settings/#harvesting-settings","title":"Harvesting settings","text":"<p>In order to harvest settings from a computer that has <code>tmux</code> installed, please first start a <code>tmux</code> session by simply typing in your terminal:</p> <pre><code>tmux\n</code></pre> <p>As the new session start, you can copy the existing settings from that session to a file located in  <code>~/.tmux.conf</code> using the following syntax:</p> <pre><code>tmux show -g | sed 's/^/set -g /' &gt; ~/.tmux.conf\n</code></pre> <p>Once complete, you can type <code>exit</code> in the same session you created to exit the session and copy <code>~/.tmux.conf</code> any location or computer you desire.</p>"},{"location":"documentation/computing/how_to_copy_or_restore_tmux_settings/#feeding-settings-to-a-computer","title":"Feeding settings to a computer","text":"<p>Assuming that you have copied the settings harvested from another computer to another and the settings file is now located at <code>~/.tmux.conf</code> on that new computer, first start a new session using <code>tmux</code> syntax in your terminal. Now you can simply pass the following syntax to load the settings file:</p> <pre><code>tmux source-file ~/.tmux.conf\n</code></pre> <p>This will load the configuration file for your  <code>tmux</code>.</p>"},{"location":"outreach/","title":"Outreach","text":""},{"location":"outreach/#outreach","title":"Outreach","text":""},{"location":"outreach/#research-hub","title":"Research Hub","text":"<p>Info</p> <p>We started a public Slack group dedicated to scientists researching computer graphics, human visual perception, computational photography and computational displays. We aim to build a single hub for everyone and provide all members with a gateway to:</p> <ul> <li>meet others in the field,</li> <li>find collaborators worldwide,</li> <li>introduce open-source tools for research,</li> <li>announce and plan events in major conferences (e.g., SIGGRAPH, CVPR, IEEE VR, SPIE PW),</li> <li>advertise opportunities for others (e.g., internships, jobs, initiatives, grants),</li> <li>promote their most recent research,</li> <li>find subjects for their experiments,</li> </ul> <p>But most of all, the primary goal is to stay connected to sustain a healthy research field.  To join our Slack channel and contribute to future conversations, please use the provided below link:</p> <p>Subscribe to our Slack</p> <p>Please do not hesitate to share the invitation link with other people in your field. If you encounter any issue with the link, please do not hesitate to reach us using kaanaksit@kaanaksit.com.</p>"},{"location":"outreach/#seminars","title":"Seminars","text":"<p>We organize a seminar series named <code>High-Beams</code>. High-Beams seminar series is an exclusive event where we host experts across the industry and academia. Overall, seminars are a blend of internal and external presenters.</p> <p>Question</p> <p>If you are wondering how to get an invitation to the next seminar series, please do not hesitate to email Kaan Ak\u015fit or subscribe yourself to our mailing list (open to public).</p> <p>Subscribe to our mailing list</p>"},{"location":"outreach/#2025","title":"2025","text":"<p>These seminars are organized by Kaan Ak\u015fit.  Kaan Ak\u015fit has received support in identifying the speaker for this round from Do\u011fa Y\u0131lmaz and Binglun Wang.</p>"},{"location":"outreach/#ziyang-chen-zicong-peng-lifeng-qiu-lin-university-college-london","title":"Ziyang Chen, Zicong Peng, Lifeng Qiu Lin (University College London)","text":"Details <p>Date: 26th November 2025</p> <p>Presenters: Ziyang Chen, Ph.D. student, University College London, Lifeng Qiu Lin, M.Sc. student, University College London, Zicong Peng, M.Sc. student, University College London</p> <p>Title: Computational Light Laboratory Special Session</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#philippe-weier-university-of-saarland","title":"Philippe Weier (University of Saarland)","text":"Details <p>Date: 19th November 2025</p> <p>Presenter: Philippe Weier, Ph.D. student, University of Saarland</p> <p>Title: Towards Hybrid and Multiscale Representations for Physically-based Light Transport</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#jose-echevarria-adobe","title":"Jose Echevarria (Adobe)","text":"Details <p>Date: 12th November 2025</p> <p>Presenter: Jose Echevarria, Research Scientist, Adobe</p> <p>Title: Color Geometries and Applications</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#akshay-paruchuri-university-of-north-carolina-at-chapel-hill","title":"Akshay Paruchuri (University of North Carolina at Chapel Hill)","text":"Details <p>Date: 5th November 2025</p> <p>Presenter: Akshay Paruchuri, Ph.D. student, University of North Carolina at Chapel Hill</p> <p>Title: Toward All-Day Ambient and Wearable Intelligence for Healthcare</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#amrita-mazumdar-nvidia","title":"Amrita Mazumdar (NVIDIA)","text":"Details <p>Date: 29th October 2025</p> <p>Presenter: Amrita Mazumdar, Research Scientist, NVIDIA Research</p> <p>Title: Towards Practical Volumetric Video: Immersive 4D Experiences with QUEEN</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#shlomi-steinberg-university-of-waterloo","title":"Shlomi Steinberg (University of Waterloo)","text":"Details <p>Date: 15th October 2025</p> <p>Presenter: Shlomi Steinberg, Assistant Professor at the University of Waterloo</p> <p>Title: Wave-Optical Light Transport</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#jianyuan-wang-university-of-oxford","title":"Jianyuan Wang (University of Oxford)","text":"Details <p>Date: 8th October 2025</p> <p>Presenter: Jianyuan Wang, Ph.D. Student at University of Oxford</p> <p>Title: VGGT: Visual Geometry Grounded Transformer</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#doga-ylmaz-university-college-london","title":"Do\u011fa Y\u0131lmaz (University College London)","text":"Details <p>Date: 1st October 2025</p> <p>Presenter: Do\u011fa Y\u0131lmaz, Ph.D. Student at University College London</p> <p>Title: Learned Single-Pass Multitasking Perceptual Graphics for Immersive Displays</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#michael-murdoch-rochester-institute-of-technology","title":"Michael Murdoch (Rochester Institute of Technology)","text":"Details <p>Date: 12th September 2025</p> <p>Presenter: Michael Murdoch, Head of the Integrated Sciences Academy</p> <p>Title: Color Appearance and Scission in Transparent Augmented Reality</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#ethan-tseng-princeton-university","title":"Ethan Tseng (Princeton University)","text":"Details <p>Date: 16th April 2025</p> <p>Presenter: Ethan Tseng, Ph.D. student at Princeton University</p> <p>Title: Neural Cameras and Displays: Building Machine Learning Frameworks for Optical System Design</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#ziyang-chen-university-college-london","title":"Ziyang Chen (University College London)","text":"Details <p>Date: 9th April 2025</p> <p>Presenter: Ziyang Chen, Ph.D. student at University College London</p> <p>Title: Lensless Tracking and Slicing Method for CGH</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#akshat-dave-massachusetts-institute-of-technology","title":"Akshat Dave (Massachusetts Institute of Technology)","text":"Details <p>Date: 2nd April 2025</p> <p>Presenter: Akshat Dave, Postdoctoral Associate at Massachusetts Institute of Technology</p> <p>Title: Superhuman Vision by co-designing Cameras, Graphics &amp; AI</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#niladri-shekhar-dutt-university-college-london","title":"Niladri Shekhar Dutt (University College London)","text":"Details <p>Date: 26th March 2025</p> <p>Presenter: Niladri Shekhar Dutt, ELLIS PhD student at University College London</p> <p>Title: Decoding Correspondence and Rig-Free Transformations</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#qi-guo-purdue-university","title":"Qi Guo (Purdue University)","text":"Details <p>Date: 19th March 2025</p> <p>Presenter: Qi Guo, Assistant Professor at Purdue University</p> <p>Title: Low Light Depth from Defocus</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#delio-vicini-google","title":"Delio Vicini (Google)","text":"Details <p>Date: 12th March 2025</p> <p>Presenter: Delio Vicini, Senior Research Scientist, Google</p> <p>Title: Differentiable Monte Carlo integration for inverse rendering and PDE problems</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#ezgi-ozylkan-new-york-university","title":"Ezgi \u00d6zy\u0131lkan (New York University)","text":"Details <p>Date: 26th February 2025</p> <p>Presenter: Ezgi \u00d6zy\u0131lkan, Ph.D. Candidate, New York University</p> <p>Title: From Nonlinear Transform Coding to Learned Distributed Compression</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#zhen-xu-zhejiang-university","title":"Zhen Xu (Zhejiang University)","text":"Details <p>Date: 19th February 2025</p> <p>Presenter: Zhen Xu, Ph.D. Candidate, Zhejiang University</p> <p>Title: Representing Long Volumetric Video with Temporal Gaussian Hierarchy</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#axel-paris-adobe","title":"Axel Paris (Adobe)","text":"Details <p>Date: 14th February 2025</p> <p>Presenter: Axel Paris, Research Scientist, Adobe</p> <p>Title: Terrain Modeling and Simulation</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#dorian-chan-carnegie-mellon-university","title":"Dorian Chan (Carnegie Mellon University)","text":"Details <p>Date: 5th February 2025</p> <p>Presenter: Dorian Chan, Ph.D. Candidate, Carnegie Mellon Universtiy</p> <p>Title: Holographic Displays for Computer Vision</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#2024","title":"2024","text":"<p>These seminars are organized by Kaan Ak\u015fit. </p>"},{"location":"outreach/#vahid-pourreza-ghoushci-university-of-rochester","title":"Vahid Pourreza Ghoushci (University of Rochester)","text":"Details <p>Date: 27th November 2024</p> <p>Presenter: Vahid Pourreza Ghoushci, Postdoctoral Associate, University of Rochester</p> <p>Title: Investigation of time-varying defocus and accommodation dynamics in the human eye</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#ryuji-hirayama-university-college-london","title":"Ryuji Hirayama (University College London)","text":"Details <p>Date: 21st November 2024</p> <p>Presenter: Ryuji Hirayama, Lecturer, University College London</p> <p>Title: Acoustic Levitation for Displays and Fabrication</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#kenny-chen-new-york-university","title":"Kenny Chen (New York University)","text":"Details <p>Date: 13th November 2024</p> <p>Presenter: Kenny Chen, Ph.D. student, New York University</p> <p>Title: Perceptually-Inspired Algorithms for Power Optimization in XR Displays</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#dongyeon-kim-university-of-cambridge","title":"Dongyeon Kim (University of Cambridge)","text":"Details <p>Date: 30th October 2024</p> <p>Presenter: Donyeon Kim, Postdoctoral Researcher, University of Cambridge</p> <p>Title: Towards passing the visual Turing test with holographic displays</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#cagatay-isil-university-of-california-los-angeles","title":"Cagatay Isil (University of California, Los Angeles)","text":"Details <p>Date: 23rd October 2024</p> <p>Presenter: Cagatay Isil, Ph.D. Candidate, University of California, Los Angeles</p> <p>Title: Diffractive processors enable all-optical image denoising and super-resolution image display</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#manu-gopakumar-stanford-university","title":"Manu Gopakumar (Stanford University)","text":"Details <p>Date: 16th October 2024</p> <p>Presenter: Manu Gopakumar, Ph.D. Candidate, Stanford University</p> <p>Title: Full-color 3D holographic augmented reality displays with metasurface waveguides</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#guosheng-hu-university-of-bristol","title":"Guosheng Hu (University of Bristol)","text":"Details <p>Date: 10th October 2024</p> <p>Presenter: Guosheng Hu, Senior Lecturer, University of Bristol</p> <p>Title: Reduce AI\u2019s Carbon Footprint</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#binglun-wang-university-college-london","title":"Binglun Wang (University College London)","text":"Details <p>Date: 2nd October 2024</p> <p>Presenter: Binglun Wang, Ph.D. candidate at University College London</p> <p>Title: 3D Editings using Diffusion Models</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#henry-fuchs-university-of-north-carolina-at-chapel-hill","title":"Henry Fuchs (University of North Carolina at Chapel Hill)","text":"Details <p>Date: 20th June 2024</p> <p>Presenter: Henry Fuchs, Professor at the University of North Carolina at Chapel Hill </p> <p>Title: Everyday Augmented Reality Glasses: Past Predictions, Present Problems, Future Possibilities</p> <p>Watch: Not recorded</p>"},{"location":"outreach/#zian-wang-university-of-toronto-and-nvidia","title":"Zian Wang (University of Toronto and NVIDIA)","text":"Details <p>Date: 24th April 2024</p> <p>Presenter: Zian Wang, PhD student at the University of Toronto </p> <p>Title: Hybrid Rendering: Bridging Volumetric and Surface Representations for Efficient 3D Content Modeling </p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#litu-rout-the-university-of-texas-austin","title":"Litu Rout (The University of Texas Austin)","text":"Details <p>Date: 10th April 2024</p> <p>Presenter: Litu Rout, PhD student at the University of Texas, Austin </p> <p>Title: On Solving Inverse Problems using Latent Diffusion </p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#yingsi-qin-carnegie-mellon-university","title":"Yingsi Qin (Carnegie Mellon University)","text":"Details <p>Date: 3rd April 2024</p> <p>Presenter: Yingsi Qin, PhD Candidate at Carnegie Mellon University</p> <p>Title: Split-Lohmann Multifocal Displays</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#seung-hwan-baek-postech","title":"Seung-Hwan Baek (Postech)","text":"Details <p>Date: 20th March 2024</p> <p>Presenter: Seung-Hwan Baek, Assistant Professor at POSTECH</p> <p>Title: High-dimensional Visual Computing</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#divya-kothandaraman-university-of-maryland-college-park","title":"Divya Kothandaraman (University of Maryland College Park)","text":"Details <p>Date: 13th March 2024</p> <p>Presenter: Divya Kothandaraman, PhD student at the University of Maryland College Park</p> <p>Title: Text Controlled Aerial-View Synthesis from a Single Image using Diffusion Models</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#cheng-zheng-massachusetts-institute-of-technology","title":"Cheng Zheng (Massachusetts Institute of Technology)","text":"Details <p>Date: 6th March 2024</p> <p>Presenter: Cheng Zheng, PhD student at Massachusetts</p> <p>Title: Neural Lithography: Close the Design to Manufacturing Gap in Computational Optics</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#taimoor-tariq-universita-della-svizzera-italiana","title":"Taimoor Tariq (Universit\u00e0 della Svizzera Italiana)","text":"Details <p>Date: 28th February 2024</p> <p>Presenter: Taimoor Tariq, PhD student at Universit\u00e0 della Svizzera Italiana</p> <p>Title: Its all in the Eyes: Towards Perceptually Optimized Real-Time VR</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#mose-sakashita-cornell-university","title":"Mose Sakashita (Cornell University)","text":"Details <p>Date: 21st February 2024</p> <p>Presenter: Mose Sakashita, PhD student at Cornell University</p> <p>Title: Enhancing Remote Design Collaboration through Motion-Controlled Telepresence Robots</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#ruoshi-liu-columbia-university","title":"Ruoshi Liu (Columbia University)","text":"Details <p>Date: 14th February 2024</p> <p>Presenter: Ruoshi Liu, PhD student at Columbia University</p> <p>Title: Neural Network Inversion for Imaging, Vision, Robotics, and Beyond</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#madalina-nicolae-saarland-university-and-polytechnic-institute-of-paris","title":"Madalina Nicolae (Saarland University and Polytechnic Institute of Paris)","text":"Details <p>Date: 7th February 2024</p> <p>Presenter: Madalina Nicolae, PhD student at Saarland University and Polytechnic Institute of Paris</p> <p>Title: Towards Digital Biofabrication and Sustainable Innovation</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#2023","title":"2023","text":"<p>These seminars are organized by Kaan Ak\u015fit.  Simon Julier invited Stephen Ellis and moderated the session.</p>"},{"location":"outreach/#daiseku-iwai-osaka-university","title":"Daiseku Iwai (Osaka University)","text":"Details <p>Date: 29th November 2023</p> <p>Presenter: Daisuke Iwai, Associate Professor at Osaka University</p> <p>Title: Computational displays in projection mapping</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#lior-yariv-weizmann-institute-of-science","title":"Lior Yariv (Weizmann Institute of Science)","text":"Details <p>Date: 22nd November 2023</p> <p>Presenter: Lior Yariv, PhD student at Weizmann Institute of Science </p> <p>Title: MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#ziya-erkoc-technical-university-of-munich","title":"Ziya Erko\u00e7 (Technical University of Munich)","text":"Details <p>Date: 15th November 2023</p> <p>Presenter: Ziya Erko\u00e7, PhD student at the Technical University of Munich </p> <p>Title: Generative Modeling with Neural Field Weights</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#guillaume-thekkadath-national-research-council-of-canada","title":"Guillaume Thekkadath (National Research Council of Canada)","text":"Details <p>Date: 8th November 2023</p> <p>Presenter: Guillaume Thekkadath, Postdoctoral Fellow at National Research Council of Canada</p> <p>Title: Intensity correlation holography: applications in single photon and remote imaging</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#kenan-bektas-university-of-saint-gallen","title":"Kenan Bektas (University of Saint Gallen)","text":"Details <p>Date: 1st November 2023</p> <p>Presenter: Kenan Bektas, Postdoctoral Researcher at the University of Saint Gallen</p> <p>Title: Gaze-Enabled Mixed Reality for Human Augmentation in Ubiquitous Computing Environments</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#koya-narumi-the-university-of-tokyo","title":"Koya Narumi (The University of Tokyo)","text":"Details <p>Date: 25th October 2023</p> <p>Presenter: Koya Narumi, Assistant Professor at the University of Tokyo</p> <p>Title: Computational Origami Fabrication</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#stephen-ellis-nasa","title":"Stephen Ellis (NASA)","text":"Details <p>Date: 11th October 2023</p> <p>Presenter: Stephen Ellis</p> <p>Title: Complexity -complicated</p> <p>Watch: N/A</p>"},{"location":"outreach/#simeng-qiu-king-abdullah-university-of-science-and-technology","title":"Simeng Qiu (King Abdullah University of Science and Technology)","text":"Details <p>Date: 4th October 2023</p> <p>Presenter: Simeng Qiu, PhD Candidate at King Abdullah University of Science and Technology</p> <p>Title: MoireTag: Angular Measurement and Tracking with a Passive Marker</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#suyeon-choi-stanford-university","title":"Suyeon Choi (Stanford University)","text":"Details <p>Date: 27th September 2023</p> <p>Presenter: Suyeon Choi, PhD student at Stanford University </p> <p>Title: Neural Holography for Next-generation Virtual and Augmented Reality Displays</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#ulugbek-kamilov-university-of-washington-in-saint-louis","title":"Ulugbek Kamilov (University of Washington in Saint Louis)","text":"Details <p>Date: 20th September 2023</p> <p>Presenter: Ulugbek Kamilov, Associate Professor of Electrical &amp; Systems Engineering and Computer Science &amp; Engineering at Washington University in St. Louis </p> <p>Title: Plug-and-Play Models for Large-Scale Computational Imaging</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#shariq-bhat-king-abdullah-university-of-science-and-technology","title":"Shariq Bhat (King Abdullah University of Science and Technology)","text":"Details <p>Date: 13th September 2023</p> <p>Presenter: Shariq Bhat, PhD Student at King Abdullah University of Science and Technology</p> <p>Title: A Journey Towards State-of-the-art Monocular Depth Estimation Using Adaptive Bins</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#congli-wang-university-of-california-berkeley","title":"Congli Wang (University of California, Berkeley)","text":"Details <p>Date: 6th September 2023</p> <p>Presenter: Congli Wang, Postdoctoral Researcher at University of California, Berkeley</p> <p>Title: Computational sensing with intelligent optical instruments</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#silvia-sellan-university-of-toronto","title":"Silvia Sell\u00e1n (University of Toronto)","text":"Details <p>Date: 14th June 2023</p> <p>Presenter: Silvia Sell\u00e1n, PhD student at University Toronto</p> <p>Title: Uncertain Surface Reconstruction</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#omer-shapira-nvidia","title":"Omer Shapira (NVIDIA)","text":"Details <p>Date: 26th May 2023</p> <p>Presenter: Omer Shapira, Engineer and Researcher at NVIDIA</p> <p>Title: Cloud Computing Around the Body: Theoretical Limits and Practical Applications</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#michael-fischer-university-college-london","title":"Michael Fischer (University College London)","text":"Details <p>Date: 17th May 2023</p> <p>Presenter: Michael Fischer, PhD student at University College London</p> <p>Title: Advanced Machine Learning for Rendering</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#michael-proulx-meta-reality-labs-and-university-of-bath","title":"Michael Proulx (Meta Reality Labs and University of Bath)","text":"Details <p>Date: 3rd May 2023</p> <p>Presenter: Michael Proulx, Reader at University of Bath and Research Scientist at Meta Reality Labs</p> <p>Title: Visual interactions in Extended Reality</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#inci-ayhan-bogazici-university","title":"\u0130nci Ayhan (Bogazici University)","text":"Details <p>Date: 26th April 2023</p> <p>Presenter: \u0130nci Ayhan, Associate Professor at Bogazici University</p> <p>Title: Cognitive Embodiment and Affordance Perception in the Virtual Reality Environment</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#zerrin-yumak-utrecht-university","title":"Zerrin Yumak (Utrecht University)","text":"Details <p>Date: 12th April 2023</p> <p>Presenter: Zerrin Yumak, Assistant Professor at Utrecht University</p> <p>Title: AI-driven Virtual Humans with Non-verbal Communication Skills</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#elia-gatti-university-college-london","title":"Elia Gatti (University College London)","text":"Details <p>Date: 5th April 2023</p> <p>Presenter: Elia Gatti, Assistant Professor at University College London</p> <p>Title: AI-driven Virtual Humans with Non-verbal Communication Skills</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#yuhao-zhu-university-of-rochester","title":"Yuhao Zhu (University of Rochester)","text":"Details <p>Date: 29th March 2023</p> <p>Presenter: Yuhao Zhu, University of Rochester</p> <p>Title: Rethinking Imaging-Computing Interface</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#taejun-kim-kaist","title":"Taejun Kim (KAIST)","text":"Details <p>Date: 22nd March 2023</p> <p>Presenter: Taejun Kim, PhD Student at KAIST</p> <p>Title: Interface Control with Eye Movement</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#josef-spjut-nvidia","title":"Josef Spjut (NVIDIA)","text":"Details <p>Date: 15th March 2023</p> <p>Presenter: Josef Spjut, Senior Research Scientist at NVIDIA</p> <p>Title: Esports Rendering and Display: Psychophysical Experimentation</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#ruth-rosenholtz-massachusetts-institute-of-technology","title":"Ruth Rosenholtz (Massachusetts Institute of Technology)","text":"Details <p>Date: 1st March 2023</p> <p>Presenter: Ruth Rosenholtz, Principal Research Scientist at Massachusetts Institute of Technology</p> <p>Title: Human vision at a glance</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#qi-sun-nyu","title":"Qi Sun (NYU)","text":"Details <p>Date: 21st February 2023</p> <p>Presenter: Qi Sun, Assistant Professor at NYU</p> <p>Title: Co-Optimizing Human-System Performance in VR/AR</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#towaki-takikawa-nvidia","title":"Towaki Takikawa (NVIDIA)","text":"Details <p>Date: 8th February 2023</p> <p>Presenter: Towaki Takikawa, Research Scientist at NVIDIA</p> <p>Title: Towards Volumetric Multimedia Compression and Transport with Neural Fields</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#2022","title":"2022","text":"<p>The seminar series of 2022 is conducted with the help of several key people at University College London. Many of these seminars are coordinated by Kaan Ak\u015fit.  Kaan has received help from Simon Julier, Oliver Kingshott, Klara Brandst\u00e4tter, and Felix Thiel for the moderation and organization of several of these events.</p>"},{"location":"outreach/#ernst-kruijff-bonn-rhein-sieg-university-of-applied-sciences","title":"Ernst Kruijff (Bonn-Rhein-Sieg University of Applied Sciences)","text":"Details <p>Date: 29th November 2022</p> <p>Presenter: Ernst Kruijff, Professor of Human Computer INteraction at Bonn-Rhein-Sieg University</p> <p>Title: Multi-sensory feedback for 3D User Interfaces</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#aykut-erdem-koc-university","title":"Aykut Erdem (Ko\u00e7 University)","text":"Details <p>Date: 23th November 2022</p> <p>Presenter: Aykut Erdem, Associate Professor at Ko\u00e7 University.</p> <p>Title: Disentangling Content and Motion for Text-Based Neural Video Manipulation</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#gul-varol-ecole-des-ponts-paristech","title":"G\u00fcl Varol (\u00c9cole des Ponts ParisTech)","text":"Details <p>Date: 16th November 2022</p> <p>Presenter: G\u00fcl Varol, Assistant Professor at \u00c9cole des Ponts ParisTech </p> <p>Title: Controllable 3D human motion synthesis</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#ana-serrano-universidad-de-zaragoza","title":"Ana Serrano (Universidad de Zaragoza)","text":"Details <p>Date: 2nd November 2022</p> <p>Presenter: Ana Serrano, Universidad de Zaragoza</p> <p>Title: Material Appearance Perception and Applications</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#praneeth-chakravarthula-princenton-university","title":"Praneeth Chakravarthula (Princenton University)","text":"Details <p>Date: 27th October 2022</p> <p>Presenter: Praneeth Chakravarthula, Research Scholar at Princenton University</p> <p>Title: The Present Developments and Future Challenges of Holographic Near-Eye Displays</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#koki-nagano-nvidia","title":"Koki Nagano (NVIDIA)","text":"Details <p>Date: 12th October 2022</p> <p>Presenter: Koki Nagano, Senior Research Scientist at NVIDIA</p> <p>Title: Frontiers of Neural Human Synthesis</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#peter-wonka-king-abdullah-university-of-science-and-technology","title":"Peter Wonka (King Abdullah University of Science and Technology)","text":"Details <p>Date: 28th September 2022</p> <p>Presenter: Peter Wonka, Computer Science at King Abdullah University of Science and Technology (KAUST) and Interim Director of the Visual Computing Center (VCC)</p> <p>Title: Recent Research Efforts for Building 3D GANs</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#rob-lindeman-university-of-canterbury","title":"Rob Lindeman (University of Canterbury)","text":"Details <p>Date: 21st September 2022</p> <p>Presenter: Rob Lindeman, Professor at the University of Canterbury</p> <p>Title: Comfortable VR: Supporting Regular and Long-term Immersion</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#felix-heide-princenton-university","title":"Felix Heide (Princenton University)","text":"Details <p>Date: 7th September 2022</p> <p>Presenter: Felix Heide, Assistant Professor at Princeton University and Co-Founder and Chief Technology Officer of self-driving vehicle startup Algolux</p> <p>Title: Neural Nanophotonic Cameras</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#yulia-gryaditskaya-surrey-institute-for-people-centred-artifical-intelligence","title":"Yulia Gryaditskaya (Surrey Institute for People-Centred Artifical Intelligence)","text":"Details <p>Date: 1st June 2022</p> <p>Presenter: Yulia Gryadistkaya, Assistant Professor at CVSSP and Surrey Institute for People-Centered Artifical Intelligence</p> <p>Title: Amateur Sketches</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#michael-bauer-nvidia","title":"Michael Bauer (NVIDIA)","text":"Details <p>Date: 25th May 2022</p> <p>Presenter: Michael Bauer, Principal Scientist at NVIDIA</p> <p>Title: Running Unmodified NumPy Programs on Hundreds of GPUs with cuNumeric</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#mark-pauly-epfl","title":"Mark Pauly (EPFL)","text":"Details <p>Date: 18th May 2022</p> <p>Presenter: Mark Pauly, Professor of Computer Graphics at \u00c9cole polytechnique f\u00e9d\u00e9rale de Lausanne </p> <p>Title: Computational Inverse Design of Deployable Structures</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#tuanfeng-wang-adobe","title":"Tuanfeng Wang (Adobe)","text":"Details <p>Date: 11th May 2022</p> <p>Presenter: Tuanfeng Wang, Research Scientist at Adobe</p> <p>Title: Synthesizing dynamic human appearance</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#tim-weyrich-fau-and-ucl","title":"Tim Weyrich (FAU and UCL)","text":"Details <p>Date: 4th May 2022</p> <p>Presenter: Tim Weyrich, Friedrich-Alexander-Universit\u00e4t Erlangen-N\u00fcrnberg and Professor of Visual Computing at University College London</p> <p>Title: Digital Reality: Visual Computing Interacting With The Real World</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#sanjeev-muralikrishnan-ucl","title":"Sanjeev Muralikrishnan (UCL)","text":"Details <p>Date: 27th April 2022</p> <p>Presenter: Sanjeev Muralikrishnan, PhD student at UCL</p> <p>Title: GLASS: Geometric Latent Augmentation For Shape Spaces</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#valentin-deschaintre-adobe","title":"Valentin Deschaintre (Adobe)","text":"Details <p>Date: 20th April 2022</p> <p>Presenter: Valentin Deschaintre, Research Scientist at Adobe</p> <p>Title: Material Creation for Virtual Environments</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#dan-archer-university-college-london-and-animesh-karnewar-university-college-london","title":"Dan Archer (University College London) and Animesh Karnewar (University College London)","text":"Details <p>Date: 23rd March 2022</p> <p>Presenter:</p> <ul> <li>Dan Archer, PhD Student at University College London</li> <li>Animesh Karnewar, Phd Student at University College London</li> </ul> <p>Title:</p> <ul> <li>Optimizing Performance through Stress and Embodiment Levels in Virtual Reality Using Autonomic Responses</li> <li>ReLU Fields: The Little Non-linearity That Could ...</li> </ul> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#oya-celiktutan-kings-college-london","title":"Oya Celiktutan (King's College London)","text":"Details <p>Date: 23rd March 2022</p> <p>Presenter: Oya Celiktutan, Assistant Professor at King's College London</p> <p>Title: Towards Building Socially Informed and Adaptive Robotic Systems</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#iuri-frosio-nvidia","title":"Iuri Frosio (NVIDIA)","text":"Details <p>Date: 17th March 2022</p> <p>Presenter: Iuri Frosio, Principal Research Scientist at NVIDIA</p> <p>Title: Research &amp; videogames @ NVIDIA \u2013 the cases of saliency estimation and cheating prevention</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#avi-bar-zeev-realityprime","title":"Avi Bar-Zeev (RealityPrime)","text":"Details <p>Date: 9th March 2022</p> <p>Presenter: Avi Bar-Zeev</p> <p>Title: Beyond Meta - AR and the Road Ahead</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#vinoba-vinayagamoorthy-british-broadcasting-corporation","title":"Vinoba Vinayagamoorthy (British Broadcasting Corporation)","text":"Details <p>Date: 2nd March 2022</p> <p>Presenter: Vinoba Vinayagamoorthy, Researcher at British Broadcasting Corporation</p> <p>Title: Designing for the Future: Exploring the Impact of (Immersive) Experiences on BBC Audiences</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#lauria-waller-university-of-california-berkeley","title":"Lauria Waller (University of California, Berkeley)","text":"Details <p>Date: 23rd February 2022</p> <p>Presenter: Laura Waller, Associate Professor, Department of Electrical Engineering and Computer Sciences, University of California, Berkeley    </p> <p>Title: Computational Microscopy</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#doga-dogan-massachusetts-institute-of-technology","title":"Do\u011fa Do\u011fan (Massachusetts Institute of Technology)","text":"Details <p>Date: 16th February 2022</p> <p>Presenter: Do\u011fa Do\u011fan, Phd Candidate at Massachusetts Institute of Technology</p> <p>Title: Unobtrusive Machine-Readable Tags for Seamless Ineractions with Real-World Objects</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#anthony-steed-university-college-london","title":"Anthony Steed (University College London)","text":"Details <p>Date: 2nd February 2022</p> <p>Presenter: Anthony Steed, Professor at University College London</p> <p>Title: So you want to build a Metaverse</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#2021","title":"2021","text":"<p>The seminar series of 2021 is conducted with the help of several key people at University College London. Many of these seminars are coordinated by Kaan Ak\u015fit.  Kaan has received help from Klara Brandst\u00e4tter, Felix Thiel, Oliver Kingshott, Tobias Ritschel, Tim Weyrich and Anthony Steed for moderation and organization of several of these events.</p>"},{"location":"outreach/#sebastian-friston-university-college-london","title":"Sebastian Friston (University College London)","text":"Details <p>Date: 24th November 2021</p> <p>Presenter: Sebastian Friston, Research Associate at University College London</p> <p>Title: Ubiq</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#wolfgang-sturzlinger-simon-fraser-university","title":"Wolfgang St\u00fcrzlinger (Simon Fraser University)","text":"Details <p>Date: 17th November 2021</p> <p>Presenter: Wolfgang St\u00fcrzlinger, Professor at Simon Fraser University</p> <p>Title: Current Challenges and Solutions for Virtual and Augmented Reality</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#nels-numan-university-college-london-and-koray-kavakl-koc-university","title":"Nels Numan (University College London) and Koray Kavakl\u0131 (Ko\u00e7 University)","text":"Details <p>Date: 10th November 2021</p> <p>Presenters: </p> <ul> <li>Koray Kavakl\u0131, MSc student at Ko\u00e7 University</li> <li>Nels Numan, PhD student at University College London</li> </ul> <p>Title:</p> <ul> <li>Learned Holographic Light Transport</li> <li>Asymmetric Collaborative Mixed Reality</li> </ul> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#david-swapp-university-college-london","title":"David Swapp (University College London)","text":"Details <p>Date: 3th November 2021</p> <p>Presenters: David Swapp, PhDSenior Research Fellow at University College London</p> <p>Title: Who are VR systems designed for?</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#katharina-krosl-vrvis-zentrum-fur-virtual-reality-and-visualisierung","title":"Katharina Kr\u00f6sl (VRVis Zentrum f\u00fcr Virtual Reality and Visualisierung)","text":"Details <p>Date: 20th October 2021</p> <p>Presenters: Katharina Kr\u00f6sl, Researcher at VRVis Zentrum f\u00fcr Virtual Reality und Visualisierung</p> <p>Title: Simulating Vision Impairments in XR</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#morgan-mcguire-roblox","title":"Morgan Mcguire (Roblox)","text":"Details <p>Date: 14th October 2021</p> <p>Presenters: Morgan Mcguire, Chief Scientist at Roblox</p> <p>Title: Metaverse Research</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#wenzel-jakob-ecole-polytechnique-federale-de-lausanne","title":"Wenzel Jakob (\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne)","text":"Details <p>Date: 6th October 2021</p> <p>Presenters: Wenzel Jakob, Assistant Professor at \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne</p> <p>Title: Differentiable Simulation of Light</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#gordon-wetzstein-stanford-university","title":"Gordon Wetzstein (Stanford University)","text":"Details <p>Date: 29th September 2021</p> <p>Presenters: Gordon Wetzstein, Associate Professor at Stanford University</p> <p>Title: Towards Neural Signal Processing and Imaging</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#anjul-patney-nvidia","title":"Anjul Patney (NVIDIA)","text":"Details <p>Date: 22nd September 2021</p> <p>Presenters: Anjul Patney, Principal Scientist at NVIDIA</p> <p>Title: Peripheral Perception &amp; Pixels</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#douglas-lanman-facebook","title":"Douglas Lanman (Facebook)","text":"Details <p>Date: 15th September 2021</p> <p>Presenters: Douglas Lanman, Director of Display Systems Research at Facebook Reality Labs, Affiliate Instructor at University of Washington</p> <p>Title: How to Pass the Visual Turing Test with AR/VR Displays</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#sylvia-xueni-pan-gold-smiths-university-of-london","title":"Sylvia Xueni Pan (Gold Smiths, University of London)","text":"Details <p>Date: 8th September 2021</p> <p>Presenters: Sylvia Xueni Pan, Lecturer in Graphics, Gold Smiths, University of London</p> <p>Title: Virtual Social Interaction in VR</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#duygu-ceylan-adobe","title":"Duygu Ceylan (Adobe)","text":"Details <p>Date: 28th July 2021</p> <p>Presenters: Duygu Ceylan, Senior Research Scientist, Adobe</p> <p>Title: Neural Dynamic Characters</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#oliver-kingshott-and-michael-fischer-university-college-london","title":"Oliver Kingshott and Michael Fischer (University College London)","text":"Details <p>Date: 21th July 2021</p> <p>Presenters:</p> <ul> <li>Oliver Kingshott, MSc student at University College London</li> <li>Michael Fischer, PhD student at University College London</li> </ul> <p>Title:</p> <ul> <li>Lensless Learning</li> <li>Learning to Overfit</li> </ul> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#yuta-itoh-tokyo-institute-of-technology","title":"Yuta Itoh (Tokyo Institute of Technology)","text":"Details <p>Date: 14th July 2021</p> <p>Presenters: Yuta Itoh, Project Associate Professor at the University of Tokyo</p> <p>Title: Vision Augmentation: overwriting our visual world via computation</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#kaan-aksit-university-college-london","title":"Kaan Ak\u015fit (University College London)","text":"Details <p>Date: 7th July 2021</p> <p>Presenters: Kaan Ak\u015fit, Associate Professor at University College London</p> <p>Title: Towards remote pixelless displays</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#cengiz-oztireli-university-of-cambridge-google","title":"Cengiz \u00d6ztireli (University of Cambridge, Google)","text":"Details <p>Date: 28th June 2021</p> <p>Presenters: Cengiz \u00d6ztireli, Associate Professor at University of Cambridge, Senior Researcher at Google</p> <p>Title: 3D Digital Reality - Modeling for Perception</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#paul-linton-city-university-of-london","title":"Paul Linton (City, University of London)","text":"Details <p>Date: 23rd June 2021</p> <p>Presenters: Paul Linton, Research Fellow, Centre for Applied Vision Research, City, University of London</p> <p>Title: Size and Distance Perception for Virtual Reality</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#luca-morreale-and-lisa-izzouzi-university-college-london","title":"Luca Morreale and Lisa Izzouzi (University College London)","text":"Details <p>Date: 16th June 2021</p> <p>Presenters:</p> <ul> <li>Luca Morreale, PhD student at University College London</li> <li>Lisa Izzouzi, Phd student at University College London</li> </ul> <p>Title: - Interpretable Neural Surface Maps - Meaningful meetups in Virtual Reality</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#rafa-mantiuk-cambridge-university","title":"Rafa\u0142 Mantiuk (Cambridge University)","text":"Details <p>Date: 9th June 2021</p> <p>Presenter: Rafa\u0142 Mantiuk, Reader in Graphics and Displays at the University of Cambridge</p> <p>Title: Modelling the quality of high frame-rate graphics for adaptive refresh rate and resolution</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#peter-shirley-nvidia","title":"Peter Shirley (NVIDIA)","text":"Details <p>Date: 2nd June 2021</p> <p>Presenter: Peter Shirley, Distinguished Research Scientist at NVIDIA</p> <p>Title: A tour of the rapidly moving target of computer graphics</p> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#david-walton-and-rafel-kuffner-dos-anjos-university-college-london","title":"David Walton and Rafel Kuffner dos Anjos (University College London)","text":"Details <p>Date:  26th May 2021</p> <p>Presenters: </p> <ul> <li>David Walton, Postdoctoral researcher at University College London</li> <li>Rafael Kuffner dos Anjos, Postdoctoral researcher at University College London</li> </ul> <p>Title:</p> <ul> <li>Beyond Blur: Ventral Metamers for Foveated Rendering </li> <li>Metameric Inpainting for Image Warping</li> </ul> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#tobias-ritschel-university-college-london","title":"Tobias Ritschel (University College London)","text":"Details <p>Date: 19th May 2021</p> <p>Presenters: Tobias Ritschel, Professor of Computer Graphics at University College London</p> <p>Title: Blue noise plots</p> <p>Watch: Not recorded</p>"},{"location":"outreach/#philip-henzler-and-david-griffiths-university-college-london","title":"Philip Henzler and David Griffiths (University College London)","text":"Details <p>Date: 12th May 2021</p> <p>Presenters: </p> <ul> <li>Philip Henzler, PhD student at University College London</li> <li>David Griffiths, PhD student at University College London</li> </ul> <p>Title: </p> <ul> <li>Generative Modelling of BRDF Textures from Flash Images</li> <li>3D object detection without scene labels</li> </ul> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#klara-brandstatter-and-felix-thiel-university-college-london","title":"Klara Brandst\u00e4tter and Felix Thiel (University College London)","text":"Details <p>Date: 5th May 2021</p> <p>Presenters: </p> <ul> <li>Klara Brandst\u00e4tter, PhD student at University College London</li> <li>Felix Thiel, PhD student at University College London</li> </ul> <p>Title: </p> <ul> <li>Creating Lively Interactive Populated Environments </li> <li>You have control. I have control</li> </ul> <p>Watch: Recording (Password protected)</p>"},{"location":"outreach/#victoria-rege-and-alex-titterton-graphcore","title":"Victoria Rege and Alex Titterton (Graphcore)","text":"Details <p>Date: 28th April 2021</p> <p>Presenters:</p> <ul> <li>Victoria Rege, Director, Alliances &amp; Strategic Partnerships at Graphcore</li> <li>Alex Titterton, Field Engineer at Graphcore (and former CERN Physicist)</li> </ul> <p>Title: Next in Machine Intelligence</p> <p>Watch: Recording (Password protected)</p>"},{"location":"people/","title":"Current members and alumni","text":""},{"location":"people/#people","title":"People","text":"<p>This page will give you a complete of our current members. At the end of the page, you will also find our alumni list as a separate section.</p>"},{"location":"people/#current-members","title":"Current members","text":"<p>All our current members are located in <code>169 Euston Road, London NW1 2AE, United Kingdom</code>.</p>"},{"location":"people/#faculty","title":"Faculty","text":"<p>Kaan Ak\u015fit</p> <p>Associate Professor of Computational Light</p> <p> E-mail</p> <p> Office: R409 </p>"},{"location":"people/#doctoral-students","title":"Doctoral students","text":"<p>Yicheng Zhan (\u6218\u5f08\u8bda)</p> <p>Ph.D. Student (University College London)</p> <p> E-mail</p> <p> Office: R404.188 </p> <p>Ziyang Chen (\u9648\u5b50\u626c)</p> <p>Ph.D. Student (University College London)</p> <p> E-mail</p> <p> Office: R404.187 </p>"},{"location":"people/#interns","title":"Interns","text":"<p>Xinyao Zhuang</p> <p>MSc Student (University College London)</p> <p> E-mail</p> <p> Office: Hybrid (R404.186) </p> <p>Jihao (Geo) Gu (\u8c37\u7eaa\u8c6a)</p> <p>MSc Student (University College London)</p> <p> E-mail</p> <p> Office: Hybrid (R404.186) </p> <p>Xiaoyue (Merry) Fan</p> <p>MSc Student (University College London)</p> <p> E-mail</p> <p> Office: Hybrid (R404.186) </p> <p>Lifeng Qiu Lin</p> <p>MSc Student (University College London)</p> <p> E-mail</p> <p> Office: Hybrid (R404.186) </p> <p>Zicong Peng</p> <p>MSc Student (University College London)</p> <p> E-mail</p> <p> Office: Hybrid (R404.186) </p> <p>Henry Kam (\u7518\u7693\u5b87)</p> <p>MSc Student (New York University)</p> <p> E-mail</p> <p> Office: Virtual </p> <p>Chuanjun Zheng</p> <p>MSc Student (Shenzhen University)</p> <p> E-mail</p> <p> Office: Virtual </p>"},{"location":"people/#alumni","title":"Alumni","text":""},{"location":"people/#post-doctoral-researchers","title":"Post-Doctoral Researchers","text":"<ul> <li>David Robert Walton, <code>Investigation on perceptually guided display technology,</code> 2021-2022, Next: Lecturer at Birmingham City University.</li> </ul>"},{"location":"people/#master-and-bachelor-students","title":"Master and Bachelor Students","text":"<ul> <li>Chengkun Li, <code>Neural Optical Beam Propagation,</code> 2021, Next: Ph.D. student at the Chinese University of Hong Kong.</li> <li>Chuanjun Zheng, <code>Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions</code>, 2025, Next: Ph.D. student at University of Hawai\u02bbi at M\u0101noa. </li> <li>Chung-Yu (Kevin) Wei, <code>Novel 3D Representations for assessing damages in cars</code>, 2025, Next: Research Intern at Tractable.</li> </ul> <ul> <li>Gbemisola Akinola-Alli, <code>Differentiable Ray Tracing for Designing Optical Parts,</code> 2022, Next: Senior Engineer at MBDA.</li> </ul> <ul> <li>Hongbin Zhang, <code>2D Gaussian Splatting with Lensless Cameras</code>, 2025, Next: -.</li> </ul> <ul> <li>Jeanne Beyazian, <code>Hologram Compression</code>, 2022, Next: Computer Vision Developer at Glimpse Analytics.</li> <li>Jiasheng Yang, <code>Localization aware Steganography</code>, 2025, Next: Research Intern at University College London.</li> <li>Jiayi Wang, <code>Modeling Keystroke Dynamics via Pairwise Timing Estimation</code>, 2025, Next: -.</li> <li>Jihao (Geo) Gu, <code>Text-guided Fine-Grained Video Anomaly Detection</code>, 2025, Next: Research Intern at University College London.</li> <li>Jingxuan Zhong, <code>Summaries Improve Consistency in LLM-based NPCs</code>, 2025, Next: M.Sc. Student at University College London.</li> <li>Junhao Cheng, <code>Migrating Differentiable Rendering to Latent Space for designing immersive displays</code>, 2025, Next: High-School teacher.</li> </ul> <ul> <li>Lifeng Qiu Lin, <code>Foveation Improves Payload Capacity in Steganography</code>, 2025, Next: Visiting Researcher at Tsinghua University.</li> </ul> <ul> <li>Oliver Kingshott, <code>Learned Point-spread Functions for Lensless Imaging,</code> 2021, Next: Ph.D. Student at University College London.</li> </ul> <ul> <li>Pengze Li, <code>Text to hologram</code>, 2024, Next: -.</li> </ul> <ul> <li>Tianwen Zhou, <code>Editing Physiological Signals in Videos Using Latent Representations,</code> 2025, Next: Researcher in Huawei Technology Ltd. </li> <li>Toma Kolev, <code>Learned methods for Lensless cameras,</code> 2025, Next: Software Engineer at Smartsoft Healthcare. </li> </ul> <ul> <li>Weijie Xie, <code>Learned Method for Computer Generated Hologram</code>, 2024, Next: Intern Researcher at University College London.</li> </ul> <ul> <li>Xiaoyue (Merry) Fan, <code>Compressing Double-Phase Holograms using 2D Gaussians</code>, 2025, Next: Research Intern at University College London.</li> <li>Xinyao Zhuang, <code>Prompt-Driven Color Accessibility in Image Generation</code>, 2025, Next: Research Intern at University College London.</li> <li>Xinyu (Lina) Gu, <code>Novel 3D Representations for assessing damages in cars</code>, 2025, Next: Research Intern at Tractable.</li> </ul> <ul> <li>Yilin Qu, <code>Predicting Next Frames of a RGBD video,</code> 2022, Next: Machine Learning Software Engineer at Qualcomm Incorporated.</li> <li>Yuze Yang, <code>Learned 3D Representations: Point Cloud, Depth Maps and Holograms,</code> 2021, Next: -.</li> </ul> <ul> <li>Zhiren Zan, <code>Detecting Fabrication Defects from images using multi-modal Large Language Models</code>, 2024, Next: -. </li> <li>Zicong Peng, <code>Assessing Learned Models for Phase-only Hologram Compression</code>, 2025, Next: Ph.D. student at Ko\u00e7 University.</li> <li>Zhuoxuan Jiang, <code>Enhancing Color and Brightness Contrast in Diffusion-Based Image Generation via Prompt Engineering</code>, 2025, Next: -.</li> <li>Ziyang Chen (\u9648\u5b50\u626c), <code>Speckle imaging with a lensless camera</code>, 2023, Next: Ph.D. student at University College London.</li> </ul>"},{"location":"people/#research-interns","title":"Research Interns","text":"<ul> <li>Abubakar Sharif, as a part of In2Science UK programme, 2022, Next: -.</li> <li>Ahmet Hamdi G\u00fczel,<code>Perceptual Prescription Correction,</code> 2022-2024, Next: Ph.D. Student at University College London.</li> <li>Debosmit Neogi, <code>Compressing RGBD data,</code> 2022, Next: Master of Science at University at Buffalo.</li> <li>Do\u011fa Y\u0131lmaz, <code>Learned Single-Pass Multitasking Perceptual Graphics for Immersive Displays</code>, 2024-2025, Next: Ph.D. student at University College London.</li> <li>Henry Kam, <code>Foveation Improves Payload Capacity in Steganography</code>, 2025, Next: Graduate student at New York University.</li> <li>Kerem Ero\u011flu, <code>Embedding data to images,</code> 2022, Next: MEng at University College London.</li> <li>Koray Kavakl\u0131, <code>Towards Improving Visual Quality in Computer-Generated Holography,</code> 2021, Next: Ph.D. Student at Ko\u00e7 University.</li> <li>Nerea Sainz De La Maza, <code>Printable camera casing design,</code> 2022, Next: Bachelor of Science at University College London.</li> <li>Serhat Aksoy, <code>Volume rendering tool,</code> 2022, Next: Bachelor of Science at Istanbul Technical University.</li> <li>Josh Kaizer, as a part of In2Science UK programme, 2022, Next: -.</li> <li>Yichen Zou, <code>3D Dataset generation,</code> 2022, Next: Graduate Student at McGill Univesity.</li> </ul>"},{"location":"publications/","title":"List of publications","text":""},{"location":"publications/#publications","title":"Publications","text":""},{"location":"publications/#2025","title":"2025","text":"<p>Learned Display Radiance Fields with Lensless Cameras</p> <p></p> <p>Ziyang Chen, Yuta Itoh, and Kaan Ak\u015fit</p> <p> Project site Manuscript Supplementary Code Project video</p>  Bibtex <pre><code>    @inproceedings{chen2025lensless,\n      author       = {Ziyang Chen and Yuta Itoh and Kaan Ak{\\c{s}}it},\n      title        = {Learned Display Radiance Fields with Lensless Cameras},\n      booktitle    = {SIGGRAPH Asia 2025 Technical Communications (SA Technical Communications '25)},\n      year         = {2025},\n      month        = {December 16--18},\n      publisher    = {ACM},\n      location     = {Hong Kong, China},\n      pages        = {4},\n      doi          = {https://doi.org/10.1145/3757376.3771381}\n    }\n</code></pre> <p></p> <p>Foveation Improves Payload Capacity in Steganography</p> <p></p> <p>Lifeng Qiu Lin, Henry Kam, Qi Sun, and Kaan Ak\u015fit</p> <p> Project site Manuscript Poster Supplementary Code Project video</p>  Bibtex <pre><code>    @inproceedings{lin2025foveation,\n      author = {Lifeng Qiu Lin and Henry Kam and Qi Sun and Kaan Ak{\\c{s}}it},\n      title = {Foveation Improves Payload Capacity in Steganography},\n      booktitle = {SIGGRAPH Asia 2025 Posters (SA Posters '25)},\n      year = {2025},\n      location = {Hong Kong, China},\n      publisher = {ACM},\n      address = {New York, NY, USA},\n      pages = {2},\n      doi = {10.1145/3757374.3771423},\n      url = {10.1145/3757374.3771423},\n      month = {December 16--18}\n    }\n</code></pre> <p></p> <p>Learned Single-Pass Multitasking Perceptual Graphics for Immersive Displays</p> <p></p> <p>Do\u011fa Y\u0131lmaz, He Wang, Towaki Takikawa, Duygu Ceylan, and Kaan Ak\u015fit</p> <p> Project site Manuscript Supplementary Code</p>  Bibtex <pre><code>@inproceedings{yilmaz2025perceptual,\n  author = {Y{\\i}lmaz, Do{\\u{g}}a and Wang, He and Takikawa, Towaki and Ceylan, Duygu and Ak{\\c{s}}it, Kaan},\n  title = {Learned Single-Pass Multitasking Perceptual Graphics for Immersive Displays},\n  booktitle = {Proceedings of the 33rd ACM International Conference on Multimedia},\n  year = {2025},\n  location = {Dublin, Ireland},\n  publisher = {ACM},\n  address = {New York, NY, USA},\n  pages = {9},\n  doi = {10.1145/3746027.3754801},\n  url = {https://doi.org/10.1145/3746027.3754801},\n  month = {October 27--31}}\n</code></pre> <p>```</p> <p></p> <p>Assessing Learned Models for Phase-only Hologram Compression</p> <p></p> <p>Zicong Peng, Yicheng Zhan, Josef Spjut, and Kaan Ak\u015fit</p> <p> Project site Manuscript Poster Supplementary Code Project video</p>  Bibtex <pre><code>    @inproceedings{peng2025assessing,\n      author = {Zicong Peng and Yicheng Zhan and Josef Spjut and Kaan Ak{\\c{s}}it},\n      title = {Assessing Learned Models for Phase-only Hologram Compression},\n      booktitle = {SIGGRAPH 2025 Posters (SA Posters '25)},\n      year = {2025},\n      location = {Vancouver, BC, Canada},\n      publisher = {ACM},\n      address = {New York, NY, USA},\n      pages = {2},\n      doi = {10.1145/3721250.3742993},\n      url = {https://doi.org/10.1145/3721250.3742993},\n      month = {August 10--14}\n    }\n</code></pre> <p></p> <p>Efficient Proxy Raytracer for Optical Systems using Implicit Neural Representations</p> <p></p> <p>Shiva Sinaei, Chuanjun Zheng, Kaan Ak\u015fit, and Daisuke Iwai</p> <p> Manuscript Poster Supplementary Code</p>  Bibtex <pre><code>    @inproceedings{Shiva2025Proxy,\n      author = {Shiva Sinaei and Chuanjun Zheng and Kaan Ak{\\c{s}}it and Daisuke Iwai},\n      title = {Efficient Proxy Raytracer for Optical Systems using Implicit Neural Representations},\n      booktitle = {SIGGRAPH 2025 Posters (SA Posters '25)},\n      year = {2025},\n      location = {Vancouver, BC, Canada},\n      publisher = {ACM},\n      address = {New York, NY, USA},\n      pages = {2},\n      doi = {10.1145/3721250.3742994},\n      url = {https://doi.org/10.1145/3721250.3742994},\n      month = {August 10--14}\n    }\n</code></pre> <p></p>"},{"location":"publications/#2024","title":"2024","text":"<p>Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions</p> <p></p> <p>Chuanjun Zheng, Yicheng Zhan, Liang Shi, Ozan Cakmakci, and Kaan Ak\u015fit</p> <p> Project site Manuscript Supplementary Code</p>  Bibtex <pre><code>    @inproceedings{zheng2024focalholography,\n      title={Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions},\n      author={Zheng, Chuanjun and Zhan, Yicheng and Shi, Liang and Cakmakci, Ozan and Ak{\\c{s}}it, Kaan},\n      booktitle = {SIGGRAPH Asia 2024 Technical Communications (SA Technical Communications '24)},\n      keywords = {Computer-Generated Holography, Light Transport, Optimization},\n      location = {Tokyo, Japan},\n      series = {SA '24},\n      month={December},\n      year={2024},\n      doi={https://doi.org/10.1145/3681758.3697989}\n    }\n</code></pre> <p></p> <p>SpecTrack: Learned Multi-Rotation Tracking via Speckle Imaging</p> <p></p> <p> Honorable Mention Award</p> <p>Ziyang Chen, Mustafa Do\u011fa Do\u011fan, Josef Spjut, and Kaan Ak\u015fit</p> <p> Project site Manuscript Poster Code Project video</p>  Bibtex <pre><code>    @inproceedings{chen2024spectrack,\n      author = {Ziyang Chen and Mustafa Dogan and Josef Spjut and Kaan Ak{\\c{s}}it},\n      title = {SpecTrack: Learned Multi-Rotation Tracking via Speckle Imaging},\n      booktitle = {SIGGRAPH Asia 2024 Posters (SA Posters '24)},\n      year = {2024},\n      location = {Tokyo, Japan},\n      publisher = {ACM},\n      address = {New York, NY, USA},\n      pages = {2},\n      doi = {10.1145/3681756.3697875},\n      url = {https://doi.org/10.1145/3681756.3697875},\n      month = {December 03--06}\n    }\n</code></pre> <p></p> <p>All-optical image denoising using a diffractive visual processor</p> <p></p> <p>\u00c7a\u011fatay I\u015f\u0131l, Tianyi Gan, Fazil Onuralp, Koray Mentesoglu, Jagrit Digani, Huseyin Karaca, Hanlong Chen, Jingxi Li, Deniz Mengu, Mona Jarrahi, Kaan Ak\u015fit, and Ozcan Aydogan</p> <p> Publisher site Manuscript</p>  Bibtex <pre><code>    @article{I\u015f\u0131l2024,\n      author = {I{\\c{s}}{\\i}l, {\\c{C}}a{\\u{g}}atay and Gan, Tianyi and Ardic, Fazil Onuralp and Mentesoglu, Koray and Digani, Jagrit and Karaca, Huseyin and Chen, Hanlong and Li, Jingxi and Mengu, Deniz and Jarrahi, Mona and Ak{\\c{s}}it, Kaan and Ozcan, Aydogan},\n      title = {All-optical image denoising using a diffractive visual processor},\n      journal = {Light: Science {\\&amp;} Applications},\n      year = {2024},\n      month = feb,\n      day = {04},\n      volume = {13},\n      number = {1},\n      pages = {43},\n      issn = {2047-7538},\n      doi = {10.1038/s41377-024-01385-6},\n      url = {https://doi.org/10.1038/s41377-024-01385-6}\n   }\n</code></pre> <p></p> <p>Autocolor: Learned Light Power Control for Multi-Color Holograms</p> <p></p> <p>Yicheng Zhan, Hakan Urey, Qi Sun, and Kaan Ak\u015fit</p> <p> Project site Manuscript Code</p>  Bibtex <pre><code>    @article{zhan2023autocolor,\n      title = {AutoColor: Learned Light Power Control for Multi-Color Holograms},\n      author = {Zhan, Yicheng and Sun, Qi and Ak\u015fit, Kaan},\n      journal  = \"arxiv\",\n      year = {2023},\n      month = may,\n    }\n</code></pre> <p></p>"},{"location":"publications/#2023","title":"2023","text":"<p>Multi-color Holograms Improve Brightnes in Holographic Displays</p> <p></p> <p>Koray Kavakl\u0131, Liang Shi, Hakan Urey, Wojciech Matusik, and Kaan Ak\u015fit </p> <p> Project site Manuscript Code Project video</p>  Bibtex <pre><code>    @inproceedings{kavakli2023multicolor,\n      title={Multi-color Holograms Improve Brightness in Holographic Displays},\n      author={Kavakl\u0131, Koray and Shi, Liang and Urey, Hakan and Matusik, Wojciech and Ak\u015fit, Kaan},\n      booktitle = {SIGGRAPH Asia 2023 Conference Papers},\n      articleno = {20},\n      numpages = {11},\n      keywords = {Brightness, Computer-generated holography, Holographic displays},\n      location = {Sydney, NSW, Australia},\n      series = {SA '23},\n      month={December},\n      year={2023},\n      doi={https://doi.org/10.1145/3610548.3618135}\n    }\n</code></pre> <p></p> <p>ChromaCorrect: Prescription Correction in Virtual Reality Headsets through Perceptual Guidance</p> <p></p> <p>Ahmet G\u00fczel, Jeanne Beyazian, Praneeth Chakravarthula, and Kaan Ak\u015fit</p> <p> Project site Manuscript Code Project video</p>  Bibtex <pre><code>    @ARTICLE{guzel2022prescription,\n      title    = \"ChromaCorrect: Prescription Correction in Virtual Reality Headsets through Perceptual Guidance\",\n      author   = \"G\u00fczel, Ahmet and Beyazian, Jeanne and Chakravarthula, Praneeth and Ak\u015fit, Kaan\",\n      journal  = \"Biomedical Optics Express\",\n      month    =  jan,\n      year     =  2023,\n    }\n</code></pre> <p></p> <p>HoloBeam: Paper-Thin Near-Eye Displays</p> <p></p> <p>Kaan Ak\u015fit and Yuta Itoh</p> <p> Project site Manuscript Code</p>  Bibtex <pre><code>    @ARTICLE{aksit2022holobeam,\n      title    = \"HoloBeam: Paper-Thin Near-Eye Displays\",\n      author   = \"Ak\u015fit, Kaan and Itoh, Yuta\",\n      journal  = \"IEEE VR 2023\",\n      month    =  Mar,\n      year     =  2023,\n    }\n</code></pre> <p></p> <p>Realistic Defocus Blur for Multiplane Computer-Generated Holography</p> <p></p> <p>Koray Kavakl\u0131, Yuta Itoh, Hakan Urey and Kaan Ak\u015fit</p> <p> Project site Manuscript Project video Code</p>  Bibtex <pre><code>    @misc{kavakli2022realisticdefocus,\n      doi = {10.48550/ARXIV.2205.07030},\n      url = {https://arxiv.org/abs/2205.07030},\n      author = {Kavakl\u0131, Koray and Itoh, Yuta and Urey, Hakan and Ak\u015fit, Kaan},\n      keywords = {Computer Vision and Pattern Recognition (cs.CV), Graphics (cs.GR), FOS: Computer and information sciences, FOS: Computer and information sciences, I.3.3},\n      title = {Realistic Defocus Blur for Multiplane Computer-Generated Holography},\n      publisher = {IEEE VR 2023},\n      month = {Mar},\n      year = {2023},\n      copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}\n    }\n</code></pre> <p></p>"},{"location":"publications/#2022","title":"2022","text":"<p>Metameric Inpainting for Image Warping</p> <p></p> <p>Rafael Kuffner Dos Anjos, David R. Walton, Kaan Ak\u015fit, Sebastian Friston, David Swapp, Anthony Steed and Tobias Ritschel</p> <p> Publisher site Manuscript</p>  Bibtex <pre><code>    @ARTICLE{Kuffner_Dos_Anjos2022-hm,\n        title    = \"Metameric inpainting for image warping\",\n        author   = \"Kuffner Dos Anjos, Rafael and Walton, David R and Ak\u015fit, Kaan and\n                    Friston, Sebastian and Swapp, David and Steed, Anthony and\n                    Ritschel, Tobias\",\n        journal  = \"IEEE Trans. Vis. Comput. Graph.\",\n        volume   = \"PP\",\n        month    =  oct,\n        year     =  2022,\n    }\n</code></pre> <p></p> <p>Optimizing vision and visuals: lectures on cameras, displays and perception</p> <p></p> <p>Koray Kavakl\u0131, David Robert Walton, Nick Antipa, Rafa\u0142 Mantiuk, Douglas Lanman and Kaan Ak\u015fit</p> <p> Project site Publisher site Manuscript Project video Code</p>  Bibtex <pre><code>    @incollection{kavakli2022optimizing,\n      title = {Optimizing vision and visuals: lectures on cameras, displays and perception},\n      author = {Kavakl\u0131, Koray and Walton, David Robert and Antipa, Nick and Mantiuk, Rafa\u0142 and Lanman, Douglas and Ak{\\c{s}}it, Kaan},\n      booktitle = {ACM SIGGRAPH 2022 Courses},\n      pages = {1--66},\n      year = {2022},\n      doi = {https://doi.org/10.1145/3532720.3535650},\n      video = {https://youtu.be/z_AtSgct6_I},\n    }\n</code></pre> <p></p> <p></p> <p>Unrolled Primal-Dual Networks for Lensless Cameras</p> <p></p> <p>Oliver Kingshott, Nick Antipa, Emrah Bostan and Kaan Ak\u015fit</p> <p> Manuscript Publisher site Supplementary Code</p>  Bibtex <pre><code>    @article{kingshott2022unrolled,\n       selected={true},\n       title={Unrolled Primal-Dual Networks for Lensless Cameras},\n       author={Kingshott, Oliver and Antipa, Nick and Bostan, Emrah and Ak\u015fit, Kaan},\n       journal={Optics Express},\n       year={2022},\n       doi={https://doi.org/10.48550/arXiv.2203.04353}\n    }\n</code></pre> <p></p> <p>Metameric Varifocal Holograms</p> <p></p> <p>David R. Walton, Koray Kavakl\u0131, Rafael Kuffner Dos Anjos, David Swapp, Tim Weyrich, Hakan Urey, Anthony Steed, Tobias Ritschel and Kaan Ak\u015fit</p> <p> Project site Manuscript Project video Code</p>  Bibtex <pre><code>    @article{walton2021metameric,\n             title={Metameric Varifocal Holography},\n             author={Walton, David R and Kavakl{\\i}, Koray and Anjos, Rafael Kuffner dos and Swapp, David and Weyrich, Tim and Urey, Hakan and Steed, Anthony and Ritschel, Tobias and Ak{\\c{s}}it, Kaan},\n             publisher = {IEEE VR},\n             month = {March},\n             year={2022}\n            }\n</code></pre> <p></p> <p>Learned holographic light transport</p> <p></p> <p> Invited</p> <p>Koray Kavakl\u0131, Hakan Urey and Kaan Ak\u015fit</p> <p> Publisher site Manuscript Code Dataset</p>  Bibtex <pre><code>    @article{Kavakli:22,\n      author = {Koray Kavakl{i} and Hakan Urey and Kaan Ak\\c{s}it},\n      journal = {Appl. Opt.},\n      keywords = {Holographic displays; Holographic recording; Holographic techniques; Image quality; Image reconstruction; Visible light communications},\n      number = {5},\n      pages = {B50--B55},\n      publisher = {OSA},\n      title = {Learned holographic light transport: invited},\n      volume = {61},\n      month = {Feb},\n      year = {2022},\n      url = {http://www.osapublishing.org/ao/abstract.cfm?URI=ao-61-5-B50},\n      doi = {10.1364/AO.439401},\n    }\n</code></pre> <p></p>"},{"location":"publications/#2021","title":"2021","text":"<p>Telelife: the future of remote living</p> <p></p> <p>Jason Orlosky, Misha Sra, Kenan Bekta\u015f, Huaishu Peng, Jeeeun Kim, Nataliya Kosmyna, Tobias Hollerer, Anthony Steed, Kiyoshi Kiyokawa and Kaan Ak\u015fit</p> <p> Publisher site Manuscript</p>  Bibtex <pre><code>@ARTICLE{10.3389/frvir.2021.763340,\nAUTHOR={Orlosky, Jason and Sra, Misha and Bekta\u015f, Kenan and Peng, Huaishu and Kim, Jeeeun and Kos\u2019myna, Nataliya and H\u00f6llerer, Tobias and Steed, Anthony and Kiyokawa, Kiyoshi and Ak\\c{s}it, Kaan},   \nTITLE={Telelife: The Future of Remote Living},      \nJOURNAL={Frontiers in Virtual Reality},      \nVOLUME={2},      \nPAGES={147},     \nYEAR={2021},      \nURL={https://www.frontiersin.org/article/10.3389/frvir.2021.763340},       \nDOI={10.3389/frvir.2021.763340},      \nISSN={2673-4192},   \n}\n</code></pre> <p></p> <p>SensiCut: material-aware laser cutting using speckle sensing and deep learning</p> <p></p> <p>Mustafa Doga Dogan, Steven Vidal Acevedo Colon, Varnika Sinha, Kaan Ak\u015fit and Stefanie Mueller</p> <p> Publisher site Project site Manuscript Project video Presentation recording</p>  Bibtex <pre><code>@inproceedings{dogan2021sensicut,\n  title={SensiCut: Material-Aware Laser Cutting Using Speckle Sensing and Deep Learning},\n  author={Dogan, Mustafa Doga and Acevedo Colon, Steven Vidal and Sinha, Varnika and Ak{\\c{s}}it, Kaan and Mueller, Stefanie},\n  booktitle={The 34th Annual ACM Symposium on User Interface Software and Technology},\n  pages={24--38},\n  year={2021}\n}\n</code></pre> <p></p> <p>Beyond blur: ventral metamers for foveated rendering</p> <p></p> <p>David R. Walton, Rafael Kuffner Dos Anjos, Sebastian Friston, David Swapp, Kaan Ak\u015fit, Anthony Steed and Tobias Ritschel</p> <p> Publisher site Project site Manuscript</p>  Bibtex <pre><code>@article{walton2021beyond,\n    author = {David R. Walton and Rafael Kuffner Dos Anjos and Sebastian Friston and David Swapp and Kaan Ak\u015fit and Anthony Steed and Tobias Ritschel},\n    title    = {Beyond Blur: Ventral Metamers for Foveated Rendering},\n    journal = {ACM Trans. Graph. (Proc. SIGGRAPH 2021)},\n    year = {2021},\n    volume = {40},\n    number = {4},\n}\n</code></pre> <p></p> <p>Beaming displays</p> <p></p> <p> Best paper nominee at IEEE VR 2021</p> <p>Yuta Itoh, Takumi Kaminokado and Kaan Ak\u015fit</p> <p> Publisher site Manuscript Project video Presentation recording</p>  Bibtex <pre><code>@article{itoh2021beaming,\n    author = {Yuta Itoh, Takumi Kaminokado, and Kaan Ak{s}it},\n    keywords = {Near-eye displays},\n    publisher = {IEEE VR},\n    title = {Beaming Displays},\n    month = {April},\n    year = {2021}\n}\n</code></pre> <p></p>"},{"location":"publications/#2020","title":"2020","text":"<p>Optical gaze tracking with spatially-sparse single-pixel detectors</p> <p></p> <p>Richard Li, Eric Whitmire, Michael Stengel, Ben Boudaoud, Jan Kautz, David Luebke, Shwetak Patel and Kaan Ak\u015fit</p> <p> Publisher site Project site Manuscript Presentation recording</p>  Bibtex <pre><code>@article{li2020opticalgaze,\n    author = {Richard Li, Eric Whitmire, Michael Stengel, Ben Boudaoud, Jan Kautz, David Luebke, Shwetak Patel, and Kaan Ak{s}it},\n    keywords = {Gaze tracking, eye tracking, LEDs, photodiodes},\n    publisher = {ISMAR},\n    title = {Optical Gaze Tracking with Spatially-Sparse Single-Pixel Detectors},\n    month = {Nov},\n    year = {2020}\n}\n</code></pre> <p></p> <p>Patch scanning displays: spatiotemporal enhancement for displays</p> <p></p> <p>Kaan Ak\u015fit</p> <p> Publisher site Manuscript Project video</p>  Bibtex <pre><code>@article{aksit2020patch,\n    author = {Kaan Ak\\c{s}it},\n    journal = {Opt. Express},\n    keywords = {Digital micromirror devices; Image quality; Image reconstruction; Light sources; Optical components; Three dimensional imaging},\n    number = {2},\n    pages = {2107--2121},\n    publisher = {OSA},\n    title = {Patch scanning displays: spatiotemporal enhancement for displays},\n    volume = {28},\n    month = {Jan},\n    year = {2020},\n    url = {http://www.opticsexpress.org/abstract.cfm?URI=oe-28-2-2107}\n}\n</code></pre> <p></p>"},{"location":"publications/#2019","title":"2019","text":"<p>Near-eye display and tracking technologies for virtual and augmented reality</p> <p></p> <p>George Alex Koulieris Kaan Ak\u015fit, Michael Stengel, Rafa\u0142 Mantiuk, Katerina Mania and Christian Richardt</p> <p> Publisher site Manuscript Project video</p>  Bibtex <pre><code>@article{NearEyeDisplayAndTrackingSTAR,\nauthor  = {George Alex Koulieris and Kaan Ak{\\c{s}}it and Michael Stengel and Rafa{\\l} K. Mantiuk and Katerina Mania and Christian Richardt},\ntitle   = {Near-Eye Display and Tracking Technologies for Virtual and Augmented Reality},\njournal = {Computer Graphics Forum},\nyear    = {2019},\nvolume  = {38},\nnumber  = {2},\nurl     = {https://richardt.name/nedtt/},\n}\n</code></pre> <p></p> <p>Foveated AR: dynamically-foveated augmented reality display</p> <p></p> <p> Emerging Technology best in show award at SIGGRAPH 2019</p> <p>Jonghyun Kim, Youngmo Jeong, Michael Stengel, Kaan Ak\u015fit, Rachel Albert, Ben Boudaoud, Trey Greer, Joohwan Kim, Ward Lopes, Zander Majercik, Peter Shirley, Josef Spjut, Morgan Mcguire and David Luebke</p> <p> Publisher site Manuscript Project video</p>  Bibtex <pre><code>@article{kim2019foveated,\n  title={Foveated AR: dynamically-foveated augmented reality display},\n  author={Kim, Jonghyun and Jeong, Youngmo and Stengel, Michael and Ak{\\c{s}}it, Kaan and Albert, Rachel and Boudaoud, Ben and Greer, Trey and Kim, Joohwan and Lopes, Ward and Majercik, Zander and others},\n  journal={ACM Transactions on Graphics (TOG)},\n  volume={38},\n  number={4},\n  pages={1--15},\n  year={2019},\n  publisher={ACM New York, NY, USA}\n}\n</code></pre> <p></p>"},{"location":"publications/#2018","title":"2018","text":"<p>FocusAR: auto-focus augmented reality eyeglasses for both real and virtual</p> <p></p> <p> Best paper award at ISMAR 2018</p> <p> Presented at SIGGRAPH ASIA 2018</p> <p>Praneeth Chakravarthula, David Dunn,  Kaan Ak\u015fit and Henry Fuchs</p> <p> Publisher site Manuscript Presentation recording Presentation source</p>  Bibtex <pre><code>@article{chakravarthula2018focusar,\n  title={focusar: auto-focus augmented reality eyeglasses for both real and virtual},\n  author={chakravarthula, praneeth and dunn, david and ak{\\c{s}}it, kaan and fuchs, henry},\n  journal={ieee transactions on visualization and computer graphics},\n  year={2018},\n  publisher={ieee}\n}\n</code></pre> <p></p> <p>Manufacturing application-driven foveated near-eye displays</p> <p></p> <p> Best paper nominee at IEEE VR 2018</p> <p> Emerging Technology best in show award at SIGGRAPH 2018</p> <p>Kaan Ak\u015fit, Praneeth Chakravarthula, Kishore Rathinavel, Youngmo Jeong, Rachel Albert, Henry Fuchs and David Luebke</p> <p> Publisher site Manuscript Project video Presentation recording Presentation source</p>  Bibtex <pre><code>@article{akcsit2019manufacturing,\n  title={Manufacturing application-driven foveated near-eye displays},\n  author={Ak{\\c{s}}it, Kaan and Chakravarthula, Praneeth and Rathinavel, Kishore and Jeong, Youngmo and Albert, Rachel and Fuchs, Henry and Luebke, David},\n  journal={IEEE transactions on visualization and computer graphics},\n  volume={25},\n  number={5},\n  pages={1928--1939},\n  year={2019},\n  publisher={IEEE}\n}\n</code></pre> <p></p>"},{"location":"publications/#2017","title":"2017","text":"<p>Near-Eye varifocal augmented reality display using see-through screens</p> <p></p> <p>Kaan Ak\u015fit, Ward Lopes, Jonghyun Kim, Peter Shirley and David Luebke</p> <p> Publisher site Manuscript Video</p>  Bibtex <pre><code>@Article{Aksit2017Varifocal,\nTitle      = {Near-Eye Varifocal Augmented Reality Display using See-Through Screens},\nAuthor     = {K. Ak{\\c{s}}it and W. Lopes and J. Kim and P. Shirley and D. Luebke},\njournal    = {ACM Trans. Graph. (SIGGRAPH)},\nissue      = {36},\nnumber     = {6},\nyear = {2017}}\n</code></pre> <p></p> <p>Wide field of view varifocal near-eye display using see-through deformable membrane mirrors</p> <p></p> <p> Best paper award at IEEE VR 2017</p> <p> SIGGRAPH 2017 Emerging Technologies DCEXPO Special Prize</p> <p>David Dunn,  Cary Tippets,  Kent Torell,  Petr Kellnhofer,  Kaan Ak\u015fit,  Piotr Didyk,  Karol Myszkowski,  David Luebke and Henry Fuchs</p> <p> Publisher site Project site Manuscript Video</p>  Bibtex <pre><code>@article{dunn2017wide,\ntitle={Wide Field Of View Varifocal Near-Eye Display Using See-Through Deformable Membrane Mirrors},\nauthor={Dunn, David and Tippets, Cary and Torell, Kent and Kellnhofer, Petr and Ak{\\c{s}}it, Kaan and Didyk, Piotr and Myszkowski, Karol and Luebke, David and Fuchs, Henry},\njournal={IEEE Transactions on Visualization and Computer Graphics},\nvolume={23},\nnumber={4},\npages={1322--1331},\nyear={2017},\npublisher={IEEE}\n}}\n</code></pre> <p></p>"},{"location":"publications/#2016","title":"2016","text":"<p>Gaze-sensing LEDs for head mounted displays</p> <p></p> <p>Kaan Ak\u015fit,  Jan Kautz  and David Luebke</p> <p> Publisher site Manuscript Video</p>  Bibtex <pre><code>@article{akcsit2020gaze,\n  title={Gaze-sensing leds for head mounted displays},\n  author={Ak{\\c{s}}it, Kaan and Kautz, Jan and Luebke, David},\n  journal={arXiv preprint arXiv:2003.08499},\n  year={2020}\n}\n</code></pre> <p></p>"},{"location":"publications/#2015","title":"2015","text":"<p>Slim near-eye display using pinhole aperture arrays</p> <p></p> <p>Kaan Ak\u015fit,  Jan Kautz  and David Luebke</p> <p> Publisher site Project site Manuscript Video</p>  Bibtex <pre><code>@article{Aksit:15, \nauthor = {Kaan Ak\\c{s}it and Jan Kautz and David Luebke}, \njournal = {Appl. Opt.}, \nkeywords = {Apertures; Vision - binocular and stereopsis ; Computational imaging},\nnumber = {11}, \npages = {3422--3427}, \npublisher = {OSA},\ntitle = {Slim near-eye display using pinhole aperture arrays}, \nvolume = {54}, \nmonth = {Apr},\nyear = {2015},\nurl = {http://ao.osa.org/abstract.cfm?URI=ao-54-11-3422},\ndoi = {10.1364/AO.54.003422},\nabstract = {We report a new technique for building a wide-angle, lightweight, thin-form-factor, cost-effective, easy-to-manufacture near-eye head-mounted display (HMD) for virtual reality applications. Our approach adopts an aperture mask containing an array of pinholes and a screen as a source of imagery. We demonstrate proof-of-concept HMD prototypes with a binocular field of view (FOV) of 70\\&amp;amp;\\#xB0;\\&amp;amp;\\#xD7;45\\&amp;amp;\\#xB0;, or total diagonal FOV of 83\\&amp;amp;\\#xB0;. This FOV should increase with increasing display panel size. The optical angular resolution supported in our prototype can go down to 1.4\\&amp;amp;\\#x2013;2.1 arcmin by adopting a display with 20\\&amp;amp;\\#x2013;30\\&amp;amp;\\#xA0;\\&amp;amp;\\#x3BC;m pixel pitch.},\n}\n</code></pre> <p></p>"},{"location":"publications/#2014","title":"2014","text":"<p>Head-worn mixed reality projection display application</p> <p></p> <p>Kaan Ak\u015fit, Daniel Kade, O\u011fuzhan \u00d6zcan  and Hakan Urey</p> <p> Publisher site Manuscript Video</p>  Bibtex <pre><code>@inproceedings{Aksit:2014:HMR:2663806.2663826,\n author = {Ak\\c{s}it, Kaan and Kade, Daniel and \\\"{O}zcan, O\\u{g}uzhan and \\\"{U}rey, Hakan},\n title = {Head-worn Mixed Reality Projection Display Application},\n booktitle = {Proceedings of the 11th Conference on Advances in Computer Entertainment Technology},\n series = {ACE '14},\n year = {2014},\n isbn = {978-1-4503-2945-3},\n location = {Funchal, Portugal},\n pages = {11:1--11:9},\n articleno = {11},\n numpages = {9},\n url = {http://doi.acm.org/10.1145/2663806.2663826},\n doi = {10.1145/2663806.2663826},\n acmid = {2663826},\n publisher = {ACM},\n address = {New York, NY, USA},\n keywords = {head-mounted projection display, immersive environments, laser projector, mixed reality, motion capture},\n} \n</code></pre> <p></p> <p>Super stereoscopy technique for comfortable and realistic 3D displays</p> <p></p> <p>Kaan Ak\u015fit, Amir Niaki,  Erdem Ulusoy  and Hakan Urey</p> <p> Publisher site Manuscript</p>  Bibtex <pre><code>@article{Aksit:14, \nauthor = {Kaan Ak\\c{s}it and Amir Hossein Ghanbari Niaki and Erdem Ulusoy and Hakan Urey}, \njournal = {Opt. Lett.}, \nkeywords = {Displays; Vision - binocular and stereopsis ; Visual optics, accommodation},\nnumber = {24}, \npages = {6903--6906}, \npublisher = {OSA},\ntitle = {Super stereoscopy technique for comfortable and realistic 3D displays}, \nvolume = {39}, \nmonth = {Dec},\nyear = {2014},\nurl = {http://ol.osa.org/abstract.cfm?URI=ol-39-24-6903},\ndoi = {10.1364/OL.39.006903},\nabstract = {Two well-known problems of stereoscopic displays are the accommodation-convergence conflict and the lack of natural blur for defocused objects. We present a new technique that we name Super Stereoscopy (SS3D) to provide a convenient solution to these problems. Regular stereoscopic glasses are replaced by SS3D glasses which deliver at least two parallax images per eye through pinholes equipped with light selective filters. The pinholes generate blur-free retinal images so as to enable correct accommodation, while the delivery of multiple parallax images per eye creates an approximate blur effect for defocused objects. Experiments performed with cameras and human viewers indicate that the technique works as desired. In case two, pinholes equipped with color filters per eye are used; the technique can be used on a regular stereoscopic display by only uploading a new content, without requiring any change in display hardware, driver, or frame rate. Apart from some tolerable loss in display brightness and decrease in natural spatial resolution limit of the eye because of pinholes, the technique is quite promising for comfortable and realistic 3D vision, especially enabling the display of close objects that are not possible to display and comfortably view on regular 3DTV and cinema.},\n}\n</code></pre> <p></p> <p>From Sound to Sight: Using Audio Processing to enable Visible Light Communication</p> <p></p> <p>Stefan Schmid,  D. Schwyn,  Kaan Ak\u015fit,  Giorgio Corbellini,  Thomas Gross  and Stefan Mangold</p> <p> Publisher site Manuscript</p>  Bibtex <pre><code>@INPROCEEDINGS{7063484,\nauthor={S. Schmid and D. Schwyn and K. Ak\u015fit and G. Corbellini and T. R. Gross and S. Mangold},\nbooktitle={2014 IEEE Globecom Workshops (GC Wkshps)},\ntitle={From sound to sight: Using audio processing to enable visible light communication},\nyear={2014},\npages={518-523},\nkeywords={audio signal processing;light emitting diodes;mobile handsets;optical communication;photodiodes;protocols;audio jack;audio processing;communication protocols;electrical signals;light signals;microphone input;mobile phones;on-board audio signal processing;passive components;peripheral device;photodiode;visible light communication;Decoding;Hardware;Lifting equipment;Light emitting diodes;Photodiodes;Protocols;Throughput},\ndoi={10.1109/GLOCOMW.2014.7063484},\nISSN={2166-0077},\nmonth={Dec},}\n</code></pre> <p></p> <p>Connecting Networks of Toys and Smartphones with Visible Light Communication</p> <p></p> <p>Giorgio Corbellini,  Kaan Ak\u015fit,  Stefan Mangold Stefan Schmid  and Thomas R. Gross</p> <p> Publisher site Manuscript Video</p>  Bibtex <pre><code>@ARTICLE{6852086,\nauthor={G. Corbellini and K. Aksit and S. Schmid and S. Mangold and T. R. Gross},\njournal={IEEE Communications Magazine},\ntitle={Connecting networks of toys and smartphones with visible light communication},\nyear={2014},\nvolume={52},\nnumber={7},\npages={72-78},\nkeywords={light emitting diodes;optical communication;optical receivers;smart phones;LED;VLC systems;brightness;consumer electronics;illumination;light emitting diodes;light receivers;microcontrollers;public environment;residential environment;smartphones;toys;visible light communication;wireless communication interface;Cameras;Commercialization;Frequency measurement;Illumination;Light emitting diodes;Microcontrollers;Receivers;Smart phones;Transceivers},\ndoi={10.1109/MCOM.2014.6852086},\nISSN={0163-6804},\nmonth={July},}\n</code></pre> <p></p>"},{"location":"publications/#2013","title":"2013","text":"<p>Dynamic exit pupil trackers for autostereoscopic displays</p> <p></p> <p>Kaan Ak\u015fit,  Hadi Baghsiahi,  P. Surman,  Selim \u04e6l\u00e7er,  E. Willman,  David R. Selviah,  Sally Day and Hakan Urey</p> <p> Publisher site Manuscript Video</p>  Bibtex <pre><code>@article{Aksit:13, \nauthor = {Kaan Ak\\c{s}it and Hadi Baghsiahi and Phil Surman and Selim \u04e6l\\c{c}er and Eero Willman and David R. Selviah and Sally Day and Hakan Urey}, \njournal = {Opt. Express}, \nkeywords = {Displays; Optical systems; Optoelectronics; Laser beam shaping; Vision - binocular and stereopsis},\nnumber = {12}, \npages = {14331--14341}, \npublisher = {OSA},\ntitle = {Dynamic exit pupil trackers for autostereoscopic displays}, \nvolume = {21}, \nmonth = {Jun},\nyear = {2013},\nurl = {http://www.opticsexpress.org/abstract.cfm?URI=oe-21-12-14331},\ndoi = {10.1364/OE.21.014331},\nabstract = {This paper describes the first demonstrations of two dynamic exit pupil (DEP) tracker techniques for autostereoscopic displays. The first DEP tracker forms an exit pupil pair for a single viewer in a defined space with low intraocular crosstalk using a pair of moving shutter glasses located within the optical system. A display prototype using the first DEP tracker is constructed from a pair of laser projectors, pupil-forming optics, moving shutter glasses at an intermediate pupil plane, an image relay lens, and a Gabor superlens based viewing screen. The left and right eye images are presented time-sequentially to a single viewer and seen as a 3D image without wearing glasses and allows the viewer to move within a region of 40 cm {\\texttimes} 20 cm in the lateral plane, and 30 cm along the axial axis. The second DEP optics can move the exit pupil location dynamically in a much larger 3D space by using a custom spatial light modulator (SLM) forming an array of shutters. Simultaneous control of multiple exit pupils in both lateral and axial axes is demonstrated for the first time and provides a viewing volume with an axial extent of 0.6{\\textminus}3 m from the screen and within a lateral viewing angle of {\\textpm} 20{\\textdegree} for multiple viewers. This system has acceptable crosstalk (\\&amp;lt; 5\\%) between the stereo image pairs. In this novel version of the display the optical system is used as an advanced dynamic backlight for a liquid crystal display (LCD). This has advantages in terms of overall display size as there is no requirement for an intermediate image, and in image quality. This system has acceptable crosstalk (\\&amp;lt; 5\\%) between the stereo image pairs.},\n}\n</code></pre> <p></p> <p>Multi-view autostereoscopic projection display using rotating screen</p> <p></p> <p> Spotlight on Optics</p> <p>Osman Eldes,  Kaan Ak\u015fit  and Hakan Urey</p> <p> Publisher site Manuscript Video</p>  Bibtex <pre><code>@article{Eldes:13,\nauthor = {Osman Eldes and Kaan Ak\\c{s}it and Hakan Urey},\njournal = {Opt. Express},\nkeywords = {Displays; Diffusers; Vision - binocular and stereopsis ; Autostereoscopic displays; Brightness; Fresnel lenses; Image registration; Pico projectors; Systems design},\nnumber = {23},\npages = {29043--29054},\npublisher = {OSA},\ntitle = {Multi-view autostereoscopic projection display using rotating screen},\nvolume = {21},\nmonth = {Nov},\nyear = {2013},\nurl = {http://www.osapublishing.org/oe/abstract.cfm?URI=oe-21-23-29043},\ndoi = {10.1364/OE.21.029043},\nabstract = {A new technique for multi-view autostereoscopic projection display is proposed, and demonstrated. The technique uses two mobile projectors, a rotating retro-reflective diffuser screen, and a head-tracking camera. As two dynamic viewing slits are created at the viewer's position, the slits can track the position of the eyes by rotating the screen. The display allows a viewer to move approximately 700 mm along the horizontal axis, and 500 mm along the vertical axis with an average crosstalk below 5 \\%. Two screen prototypes with different diffusers have been tried, and they provide luminance levels of 60 Cd/m2, and 160 Cd/m2 within the viewing field.},\n}\n</code></pre> <p></p>"},{"location":"publications/#2012","title":"2012","text":"<p>Portable 3D Laser Projector Using Mixed Polarization Technique</p> <p></p> <p> Best 3D product award of International 3D Society (4th year)</p> <p>Kaan Ak\u015fit,  Osman Elde\u015f,  Selvan Viswanathan,  Mark Freeman  and Hakan Urey</p> <p> Publisher site Manuscript Video</p>  Bibtex <pre><code>@ARTICLE{6297485,\n  author={Aksit, Kaan and Eldes, Osman and Viswanathan, Selvan and Freeman, Mark O. and Urey, Hakan},\n  journal={Journal of Display Technology}, \n  title={Portable 3D Laser Projector Using Mixed Polarization Technique}, \n  year={2012},\n  volume={8},\n  number={10},\n  pages={582-589},\n  doi={10.1109/JDT.2012.2205664}}\n</code></pre> <p></p>"},{"location":"publications/#2010","title":"2010","text":"<p>Heart rate monitoring via remote photoplethysmography with motion artifacts reduction</p> <p></p> <p>Giovanni Cennini,  Jeremie Arguel,  Kaan Ak\u015fit and Arno van Leest</p> <p> Publisher site Manuscript Video</p>  Bibtex <pre><code>@article{Cennini:10, \nauthor = {Giovanni Cennini and Jeremie Arguel and Kaan Ak\\c{s}it and Arno van Leest}, \njournal = {Opt. Express}, \nkeywords = {Medical optics instrumentation; Optical devices; Optical sensing and sensors},\nnumber = {5}, \npages = {4867--4875}, \npublisher = {OSA},\ntitle = {Heart rate monitoring via remote photoplethysmography with motion artifacts reduction}, \nvolume = {18}, \nmonth = {Mar},\nyear = {2010},\nurl = {http://www.opticsexpress.org/abstract.cfm?URI=oe-18-5-4867},\ndoi = {10.1364/OE.18.004867},\nabstract = {In this paper, we present a novel photoplethysmographic device that operates remotely, i.e. not in contact with the skin. The device allows for real time measurements of heart rate with motion artifact reduction from a distance of a few centimeters up to several meters. High mobility of users is achieved in assessment of vital body signs, such as heart rate.},\n}\n</code></pre> <p></p>"},{"location":"publications/assess_hologram_compression/","title":"Assessing Learned Models for Phase-only Hologram Compression","text":""},{"location":"publications/assess_hologram_compression/#assessing-learned-models-for-phase-only-hologram-compression","title":"Assessing Learned Models for Phase-only Hologram Compression","text":""},{"location":"publications/assess_hologram_compression/#people","title":"People","text":"<p>Zicong Peng<sup>1</sup></p> <p>Yicheng Zhan<sup>1</sup></p> <p>Josef Spjut<sup>2</sup></p> <p>Kaan Ak\u015fit<sup>1</sup></p> <p> <sup>1</sup>University College London, <sup>2</sup>NVIDIA </p> <p>SIGGRAPH 2025 Poster</p>"},{"location":"publications/assess_hologram_compression/#resources","title":"Resources","text":"<p> Manuscript Poster Supplementary Code</p>  Bibtex <pre><code>@inproceedings{peng2025assessing,\n  author = {Zicong Peng and Yicheng Zhan and Josef Spjut and Kaan Ak{\\c{s}}it},\n  title = {Assessing Learned Models for Phase-only Hologram Compression},\n  booktitle = {SIGGRAPH 2025 Posters (SA Posters '25)},\n  year = {2025},\n  location = {Vancouver, BC, Canada},\n  publisher = {ACM},\n  address = {New York, NY, USA},\n  pages = {2},\n  doi = {10.1145/3721250.3742993},\n  url = {https://doi.org/10.1145/3721250.3742993},\n  month = {August 10--14}\n}\n</code></pre>"},{"location":"publications/assess_hologram_compression/#video","title":"Video","text":""},{"location":"publications/assess_hologram_compression/#abstract","title":"Abstract","text":"<p>We evaluate the performance of four common learned models utilizing Implicit Neural Representation (INR) and Variational Autoencoder (VAE) structures for compressing phase-only holograms in holographic displays. The evaluated models include a vanilla MLP, SIREN, and FilmSIREN, with TAESD as the representative VAE model. Our experiments reveal that a pretrained image VAE, TAESD, with 2.2M parameters struggles with phase-only hologram compression, revealing the need for task-specific adaptations. Among the INRs, SIREN with 4.9k parameters achieves \\(40\\%\\) compression with high quality in the reconstructed 3D images (PSNR = 34.54 dB). These results emphasize the effectiveness of INRs and identify the limitations of pretrained image compression VAE s for hologram compression task.</p> <p></p>"},{"location":"publications/assess_hologram_compression/#proposed-method","title":"Proposed Method","text":"<p>Our assessments involve using single-color double-phase encoded phase-only holograms, \\(P \\in \\mathbb{R}^{3 \\times 512 \\times 512}\\), using three color primaries. These \\(P\\)s are calculated for three wavelengths, \\(\\{473, 515, 639\\}\\) nm and a fixed pixel pitch, \\(3.74\\,\\mu\\text{m}\\) (Jasper Display JD7714). We adopt an off-the-shelf TAESD trained for image compression task. Specifically, the TAESD with \\(2.2M\\) parameters encodes \\(P\\) to a \\(\\text{bottleneck} \\in \\mathbb{R}^{16 \\times 64 \\times 64}\\) and later decodes into the original resolution of \\(3 \\times 512 \\times 512\\). Our teaser (above) shows pretrained TAESD fails, requiring dedicated training for generalization. Feature size comparison yields only 92% reduction (excluding TAESD params). Thus, we choose to explore INR based models to see if the feature size could be further reduced while accepting longer training times as INRs typically are overfitted on a single data at a time.</p> <p></p> <p>In our study, we compare three foundational INR architectures (vanillaMLP, SIREN, and FilmSIREN), aiming for \\(\\sim \\%40\\) feature reduction as the starting point of the experiment, and the aim is to strike a balance between the quality of the reconstructed image and the compression ratio.  \\(P\\)s are split into patches (e.g., \\(3 \\times 64 \\times 64\\)), a separate model is trained for each patch (initialized from prior weights), and their outputs are combined for full reconstruction. Experiments that we are going to detail in the next section utilize ten different holograms (The purpose of selecting a small but diverse set of initial samples in this study is to demonstrate the comparative trends among different methods. The large-scale validation work will be addressed in the subsequent research.) and turns them into patches by following the choices listed in table. All INRs use Adam (lr=0.0001) with StepLR (gamma=0.5 every 5000 epochs), trained for 5000 epochs.</p>"},{"location":"publications/assess_hologram_compression/#conclusions","title":"Conclusions","text":"<p>SIREN and FilmSIREN provide strong compression, outperforming vanillaMLP, with SIREN showing best consistency. In our current experiments in table below, SIREN achieves the highest fidelity with a PSNR of 42.29 dB and SSIM of 0.99 at \\(3 \\times 64 \\times 64\\) patch size,  while its 3D reconstruction quality (PSNR = 34.54 dB, SSIM = 0.96, LPIPS = 0.10) marginally outperforms FilmSIREN (PSNR = 33.27 dB, SSIM = 0.94, LPIPS = 0.15).  Additionally, under identical training schedules, both SIREN and FilmSIREN frequently satisfied the early stopping criterion near 2000 epochs.  This consistency implies a relatively smooth optimization process, suggesting that these models can converge effectively without compromising image quality,  which is a favorable property in hologram compression task. Computational cost (\\(T\\) hours/hologram) is justified by SIREN's quality.</p> <p></p> <p>These observations suggest that specialized INR architectures require further investigation for the hologram compression task,  potentially opening new solutions for efficient 3D scene representation in holographic displays. Achieving robust compression remains an open challenge; our study guides future work on efficient 3D holographic rendering/storage.</p>"},{"location":"publications/assess_hologram_compression/#photo-gallery","title":"Photo gallery","text":"<p>Here, we release photographs from our visit to the conference, highlighting parts of our SIGGRAPH 2025 experience.</p> <p> </p>"},{"location":"publications/assess_hologram_compression/#relevant-research-works","title":"Relevant research works","text":"<p>Here are relevant research works from the authors:</p> <ul> <li>Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions</li> <li>Autocolor: Learned Light Power Control for Multi-Color Holograms</li> <li>Multi-color Holograms improve Brightness in Holographic Displays</li> <li>HoloBeam: Paper-Thin Near-Eye Displays</li> <li>Realistic Defocus for Multiplane Computer-Generated Holography</li> <li>Optimizing Vision and Visuals: Lectures on Cameras, Displays, and Perception</li> <li>Odak</li> </ul>"},{"location":"publications/assess_hologram_compression/#outreach","title":"Outreach","text":"<p>We host a Slack group with more than 250 members. This Slack group focuses on the topics of rendering, perception, displays and cameras. The group is open to public and you can become a member by following this link.</p>"},{"location":"publications/assess_hologram_compression/#contact-us","title":"Contact Us","text":"<p>Warning</p> <p>Please reach us through email to provide your feedback and comments.</p>"},{"location":"publications/focal_surface_light_transport/","title":"Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions","text":""},{"location":"publications/focal_surface_light_transport/#focal-surface-holographic-light-transport-using-learned-spatially-adaptive-convolutions","title":"Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions","text":""},{"location":"publications/focal_surface_light_transport/#people","title":"People","text":"<p>Chuanjun Zheng<sup>1</sup></p> <p>Yicheng Zhan<sup>1</sup></p> <p>Liang Shi<sup>2</sup></p> <p>Ozan Cakmakci<sup>3</sup></p> <p>Kaan Ak\u015fit<sup>1</sup></p> <p> <sup>1</sup>University College London, <sup>2</sup>Massachusetts Institute of Technology, <sup>3</sup>Google </p> <p>SIGGRAPH Asia 2024 Technical Communications </p>"},{"location":"publications/focal_surface_light_transport/#resources","title":"Resources","text":"<p> Manuscript Supplementary Code</p>  Bibtex <pre><code>@inproceedings{zheng2024focalholography,\n  title={Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions},\n  author={Chuanjun Zheng, Yicheng Zhan, Liang Shi, Ozan Cakmakci, and Kaan Ak{\\c{s}}it},\n  booktitle = {SIGGRAPH Asia 2024 Technical Communications (SA Technical Communications '24)},\n  keywords = {Computer-Generated Holography, Light Transport, Optimization},\n  location = {Tokyo, Japan},\n  series = {SA '24},\n  month={December},\n  year={2024},\n  doi={https://doi.org/10.1145/3681758.3697989}\n}\n</code></pre>"},{"location":"publications/focal_surface_light_transport/#abstract","title":"Abstract","text":"<p>Computer-Generated Holography (CGH) is a set of algorithmic methods for identifying holograms that reconstruct Three-Dimensional (3D) scenes  in holographic displays. CGH algorithms decompose 3D scenes into multiplanes at different depth levels and rely on simulations of light that propagated from a source plane to a targeted plane. Thus, for \\(n\\) planes, CGH typically optimizes holograms using \\(n\\) plane-to-plane  light transport simulations, leading to major time and computational demands. Our work replaces multiple planes with a focal surface and introduces  a learned light transport model that could propagate a light field from a source plane to the focal surface in a single inference. Our model leverages  spatially adaptive convolution to achieve depth-varying propagation demanded by targeted focal surfaces. The proposed model reduces the hologram  optimization process up to \\(1.5x\\), which contributes to hologram dataset generation and the training of future learned CGH models.</p>"},{"location":"publications/focal_surface_light_transport/#focal-surface-holographic-light-transport","title":"Focal Surface Holographic Light Transport","text":"<p>Simulating light propagation among multiple planes in a 3D volume is computationally  demanding, as a 3D volume is represented with multiple planes and each plane requires a separate calculation of light propagation to reconstruct the target image. Thus,  for \\(n\\) planes, conventional light transport simulation methods require \\(n\\) plane-to-plane  simulations, leading to major time and computational demands. Our work replaces multiple planes with a focal surface and introduces a learned light transport model that could  propagate a light field from a source plane to the focal surface in a single inference, reducing simulation time by \\(10x\\).</p> <p></p>"},{"location":"publications/focal_surface_light_transport/#results","title":"Results","text":"<p>When simulating a full-color, all-in-focus 3D image across a focal surface, conventional  Angular Spectrum Method (ASM) requires eighteen forward passes to simulate the 3D image  with six depth planes given there are three color primaries. </p> <p></p> <p>Our work enables a new way to overcome this computational complexity  arising from plane to plane treatment, and unlocks a new rendering method that could propagate light beams from a plane to a focal surface. This new model could help reduce computational complexity in simulating light. Specifically, it could help verify and calculate holograms for holographic displays with much ease and lesser computation.</p> <p></p> <p>We utilize our model for a 3D phase-only hologram optimization application under  \\(0 mm\\) propagation distance. Optimizing holograms with six target planes using ASM  is denoted as ASM 6, while Ours 6 represents optimizing holograms using our model with six  focal surfaces. When comparing the simulation results, all holograms are reconstructed using ASM for performance assessment.  Ours 6 achieves comparable results with about \\(70\\%\\) of the optimization time compared to ASM 6.</p> <p></p> <p>We also apply our model for a 3D phase-only hologram optimization application under \\(10 mm\\) propagation distance.</p> <p></p>"},{"location":"publications/focal_surface_light_transport/#photo-gallery","title":"Photo gallery","text":"<p>Here, we release photographs from our visit to the conference, highlighting parts of our SIGGRAPH ASIA 2024 experience.</p> <p> </p>"},{"location":"publications/focal_surface_light_transport/#relevant-research-works","title":"Relevant research works","text":"<p>Here are relevant research works from the authors:</p> <ul> <li>Multi-color Holograms Improve Brightness in Holographic Displays</li> <li>HoloBeam: Paper-Thin Near-Eye Displays</li> <li>Realistic Defocus for Multiplane Computer-Generated Holography</li> <li>Optimizing Vision and Visuals: Lectures on Cameras, Displays, and Perception</li> <li>Learned Holographic Light Transport</li> <li>Metameric Varifocal Holograms</li> <li>Odak</li> </ul>"},{"location":"publications/focal_surface_light_transport/#outreach","title":"Outreach","text":"<p>We host a Slack group with more than 250 members. This Slack group focuses on the topics of rendering, perception, displays and cameras. The group is open to public and you can become a member by following this link.</p>"},{"location":"publications/focal_surface_light_transport/#contact-us","title":"Contact Us","text":"<p>Warning</p> <p>Please reach us through email to provide your feedback and comments.</p>"},{"location":"publications/foveated_steganography/","title":"Foveation Improves Payload Capacity in Steganography","text":""},{"location":"publications/foveated_steganography/#foveation-improves-payload-capacity-in-steganography","title":"Foveation Improves Payload Capacity in Steganography","text":""},{"location":"publications/foveated_steganography/#people","title":"People","text":"<p>Lifeng Qiu Lin<sup>1</sup></p> <p>Henry Kam<sup>2</sup></p> <p>Qi Sun<sup>2</sup></p> <p>Kaan Ak\u015fit<sup>1</sup></p> <p> <sup>1</sup>University College London, <sup>2</sup>New York University </p> <p>SIGGRAPH Asia 2025 Poster</p>"},{"location":"publications/foveated_steganography/#resources","title":"Resources","text":"<p> Manuscript Poster Supplementary Code</p>  Bibtex <pre><code>@inproceedings{qiu2025foveation,\n  author = {Lifeng Qiu Lin and Henry Kam and Qi Sun and Kaan Ak{\\c{s}}it},\n  title = {Foveation Improves Payload Capacity in Steganography},\n  booktitle = {SIGGRAPH Asia 2025 Posters (SA Posters '25)},\n  year = {2025},\n  location = {Hong Kong, China},\n  publisher = {ACM},\n  address = {New York, NY, USA},\n  pages = {2},\n  doi = {https://doi.org/10.1145/3757374.3771423},\n  month = {December 16--18}\n}\n</code></pre>"},{"location":"publications/foveated_steganography/#video","title":"Video","text":""},{"location":"publications/foveated_steganography/#abstract","title":"Abstract","text":"<p>Steganography finds its use in visual medium such as providing metadata and watermarking. With support of efficient latent representations and foveated rendering, we trained models that improve existing capacity limits from 100 to 500 bits, while achieving better accuracy of up to 1 failure bit out of 2000, at 200K test bits. Finally, we achieve a comparable visual quality of 31.47 dB PSNR and 0.13 LPIPS, showing the effectiveness of novel perceptual design in creating multi-modal latent representations in steganography.</p> <p></p>"},{"location":"publications/foveated_steganography/#proposed-method","title":"Proposed Method","text":"<p>Considering a message payload \\(P \\in \\{0, 1\\}^k\\) consisting of \\(k\\) bits and a input image (cover) \\(I \\in \\mathbb{R}^{h \\times w \\times c}\\), find two functions \\(H\\) and \\(R\\), to produce an output image (stego), \\(H(I, P) = I' \\in \\mathbb{R}^{h \\times w \\times c}\\), and output payload, \\(R(I') = P' \\in \\{0, 1\\}^k\\). The aim is to reduce the distortion between \\(I\\) and \\(I'\\) while maximizing the accuracy between \\(P\\) and \\(P'\\). Our framework approaches this problem as depicted below:</p> <p></p> <p>In the hiding stage, a frozen image encoder, \\(E\\), transforms input image into a latent representation, \\(E(I) = Z_i\\).  Payload embedder, \\(F\\), creates also a learned representation, \\(P(I) = Z_p\\).  Together, they are manipulated by the merger, \\(M\\), producing a merged latent, \\(M(Z_i, Z_p) = Z_m\\), which a frozen image generator, \\(G\\), uses to reconstruct the output image,  \\(G(Z_m) = I'\\).  Finally, a payload retriever, \\(R\\), extracts the output payload, \\(R(I') = P'\\). The loss function is defined as the combination of payload and image quality losses, being BCE and Metameric Foveated Rendering (defaulted to center) losses respectively. Formally,\\(\\mathcal{L}_{total} = \\mathcal{L}_{payload} + \\lambda_i \\cdot \\mathcal{L}_{image} \\nonumber = \\operatorname{BCE}(P, P') + \\lambda_i \\cdot (MetamericLoss(I, I')),\\) where \\(\\lambda_i\\) controls the trade-off between the two losses.</p> <p>The dataset is a balanced mixture of 2000 training, 400 validation, and 400 test images from MetFaces and CLIC datasets.  For preprocessing, images are randomly cropped and padded to the size of input and normalized as autoencoder requires.  Notably, this dataset is much smaller than typical datasets used for the same purpose, but is found sufficient to learn performing 100-bits steganography, within controlled computing resources, about two hours on a single RTX 4090 GPU.</p>"},{"location":"publications/foveated_steganography/#conclusion","title":"Conclusion","text":"<p>The frozen pair of image encoder and image generator to create a high-quality latent representation, is the F4-with-attention version autoencoder from LDM VQGAN series. After evaluating empirically, we found its high reconstruction quality is suitable for the embedding process. Compared to other backbones, this one converges slower at payload embedding, but achieves better image quality in the end. Keeping payload embedder as a fully connected layers is sufficient to encode the information after experimentation.  For merger, the best performing architecture is adding two convolutional layer sandwiching the sum of image and payload latent, to soften the transition.  Finally, ResNet50 was used as payload retriever as a popular and well-studied architecture. </p> <p>Apart from common metrics, we also report Metameric Loss, which is a perceptual criterion akin to foveated gaze. Modeling the human visual system, this loss is more forgiving of visual distortions in the periphery and more harsh in the fovea.</p> <p>The main results are shown below: </p> <p></p> <p>Our baseline is trained in multiple resolutions and payload capacities, including the native resolution of RoSteALS, the benchmarked method. The baseline in the smallest setting achieves a bit accuracy of \\(99.99\\%\\), failing to decode only 4 out of 40K test bits.  Noticeably, we achieve \\(100\\%\\) recovery in the same setting as the benchmark, while other settings also all exceed \\(99.95\\%\\) compared to benchmark failing to reach \\(99.5\\%\\). Nevertheless, RoSteALS has better perceptual image quality, although it could be explained with lack of incorporation of LPIPS and finer-grained optimization in the training.</p> <p>Another important observation is that Metameric Loss consistently and significantly improves the quality of the reconstructed images while keeping same level of bit accuracy.  This shows effectiveness of this visual technique in enhancing perceptual fidelity of images.</p> <p>Despite successfully unlocking higher message length, we notice tangible limits of payload capacity, such as failing to learn 500-bit payload at 128 resolution. Resolution bounds the upper payload capacity under similar perceptual fidelity of images, and we hope to enhance this by introducing gaze as a new parameter. This work provides a light-weighted, human-centered, latent-based steganography framework which boosts payload capacity and accuracy while maintaining image quality. By satiating the need of large capacity in transmitting messages, we step towards practical applications of steganography in real-world scenarios.</p>"},{"location":"publications/foveated_steganography/#photo-gallery","title":"Photo gallery","text":"<p>Here, we release photographs from our visit to the conference, highlighting parts of our SIGGRAPH Asia 2025 experience.</p> <p> </p>"},{"location":"publications/foveated_steganography/#relevant-research-works","title":"Relevant research works","text":"<p>Here are relevant research works from the authors:</p> <ul> <li>ChromaCorrect: Prescription Correction in Virtual Reality Headsets through Perceptual Guidance</li> <li>Metameric Inpainting for Image Warping</li> <li>Metameric Varifocal Holograms</li> <li>Optimizing vision and visuals: lectures on cameras, displays and perception</li> <li>Beyond blur: ventral metamers for foveated rendering</li> <li>Foveated AR: dynamically-foveated augmented reality display</li> <li>Odak</li> </ul>"},{"location":"publications/foveated_steganography/#outreach","title":"Outreach","text":"<p>We host a Slack group with more than 250 members. This Slack group focuses on the topics of rendering, perception, displays and cameras. The group is open to public and you can become a member by following this link.</p>"},{"location":"publications/foveated_steganography/#contact-us","title":"Contact Us","text":"<p>Warning</p> <p>Please reach us through email to provide your feedback and comments.</p>"},{"location":"publications/holobeam/","title":"HoloBeam: Paper-Thin Near-Eye Displays","text":""},{"location":"publications/holobeam/#holobeam-paper-thin-near-eye-displays","title":"HoloBeam: Paper-Thin Near-Eye Displays","text":""},{"location":"publications/holobeam/#people","title":"People","text":"<p>Kaan Ak\u015fit<sup>1</sup></p> <p>Yuta Itoh<sup>2</sup></p> <p><sup>1</sup>University College London, <sup>2</sup>The University of Tokyo</p> <p>IEEE VR 2023</p>"},{"location":"publications/holobeam/#resources","title":"Resources","text":"<p> Manuscript Code</p>  Bibtex <pre><code>@inproceedings{aksit2022holobeam,\n  title = \"HoloBeam: Paper-Thin Near-Eye Displays\",\n  author = \"Ak\u015fit, Kaan and Itoh, Yuta\",\n  booktitle ={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},\n  pages = {581--591},\n  year = {2023},\n}\n</code></pre>"},{"location":"publications/holobeam/#presentation","title":"Presentation","text":""},{"location":"publications/holobeam/#abstract","title":"Abstract","text":"<p>An emerging alternative to conventional Augmented Reality (AR) glasses designs, Beaming displays promise slim AR glasses free from challenging design trade-offs, including battery-related limits or computational budget-related issues. These beaming displays remove active components such as batteries and electronics from AR glasses and move them to a projector that projects images to a user from a distance (1-2 meters), where users wear only passive optical eyepieces. However, earlier implementations of these displays delivered poor resolutions (7 cycles per degree) without any optical focus cues and were introduced with a bulky form-factor eyepiece (\\(\\sim50~mm\\) thick). This paper introduces a new milestone for beaming displays, which we call HoloBeam. In this new design, a custom holographic projector populates a micro-volume located at some distance (1-2 meters) with multiple planes of images. Users view magnified copies of these images from this small volume with the help of an eyepiece that is either a Holographic Optical Element (HOE) or a set of lenses. Our HoloBeam prototypes demonstrate the thinnest AR glasses to date with a submillimeter thickness (\\eg HOE film is only \\(120~\\mu m\\) thick). In addition, HoloBeam prototypes demonstrate near retinal resolutions (\\(24\\) cycles per degree) with a \\(70\\) degrees wide field of view.</p>"},{"location":"publications/holobeam/#results","title":"Results","text":"<p>As a next step in Beaming Displays , our work offers the thinnest and lightweight near-eye display to date. Our wearable eyepieces could just be a lens or a holographic optical element.</p> <p></p> <p>In order to beam images to our eyepieces, we built a phase-only holographic projector.</p> <p></p> <p>We also show that a cheaper alternative of this projector could be built using common spatial light modulators.</p> <p></p> <p>In this work, we demonstrate the first Beaming Displays that can generate multiplane images using Computer-Generated Holography. Below image is a moving animation showing a focal sweep of images.</p> <p></p> <p>Although we showed monochrome results mostly, HoloBeam can also show full color images.</p> <p></p>"},{"location":"publications/holobeam/#relevant-research-works","title":"Relevant research works","text":"<p>Here are relevant research works from the authors:</p> <ul> <li>Beaming Displays</li> <li>Realistic Defocus for Multiplane Computer-Generated Holography</li> <li>Optimizing Vision and Visuals: Lectures on Cameras, Displays, and Perception</li> <li>Learned Holographic Light Transport</li> <li>Metameric Varifocal Holograms</li> <li>Odak</li> </ul>"},{"location":"publications/holobeam/#outreach","title":"Outreach","text":"<p>We host a Slack group with more than 250 members. This Slack group focuses on the topics of rendering, perception, displays and cameras. The group is open to public and you can become a member by following this link.</p>"},{"location":"publications/holobeam/#contact-us","title":"Contact Us","text":"<p>Warning</p> <p>Please reach us through email to provide your feedback and comments.</p>"},{"location":"publications/holobeam/#acknowledgements","title":"Acknowledgements","text":"<p>The authors would like to thank reviewers for their valuable feedback. The authors wish to thank Koray Kavakl\u0131 for fruitful discussions.</p> <p>Kaan Ak\u015fit is supported by the Royal Society's RGS\\R2\\212229 - Research Grants 2021 Round 2 in building the hardware prototype. Kaan Ak\u015fit is also supported by Meta Reality Labs inclusive rendering initiative 2022.  </p> <p>Yuta Itoh is supported by the JST FOREST Grant Number JPMJFR206E and JSPS KAKENHI Grant Number JP20J14971, 20H05958, and 21K19788, Japan.  </p>"},{"location":"publications/lensless_display_radiance_field/","title":"Learned Display Radiance Fields with Lensless Cameras","text":""},{"location":"publications/lensless_display_radiance_field/#learned-display-radiance-fields-with-lensless-cameras","title":"Learned Display Radiance Fields with Lensless Cameras","text":""},{"location":"publications/lensless_display_radiance_field/#people","title":"People","text":"<p>Ziyang Chen<sup>1</sup></p> <p>Yuta Itoh<sup>2</sup></p> <p>Kaan Ak\u015fit<sup>1</sup></p> <p> <sup>1</sup>University College London, <sup>2</sup>Institute of Science Tokyo </p> <p>SIGGRAPH Asia 2025 Technical Communications</p>"},{"location":"publications/lensless_display_radiance_field/#resources","title":"Resources","text":"<p> Manuscript Supplementary Code</p>  Bibtex <pre><code>@inproceedings{chen2025lensless,\n  author       = {Ziyang Chen and Yuta Itoh and Kaan Ak{\\c{s}}it},\n  title        = {Learned Display Radiance Fields with Lensless Cameras},\n  booktitle    = {SIGGRAPH Asia 2025 Technical Communications (SA Technical Communications '25)},\n  year         = {2025},\n  month        = {December 16--18},\n  publisher    = {ACM},\n  location     = {Hong Kong, China},\n  pages        = {4},\n  doi          = {https://doi.org/10.1145/3757376.3771381},\n  keywords     = {lensless imaging, display technology, radiance field, computational optics},\n}\n</code></pre>"},{"location":"publications/lensless_display_radiance_field/#slides","title":"Slides","text":""},{"location":"publications/lensless_display_radiance_field/#video","title":"Video","text":"<p> (a) Our lensless camera placed in front of the display captures the light field emitted from the display pixels.  (b) A conventional camera captures a real-world photograph of the test pattern on the display.  (c) Our learned model estimates a rendered view from the viewpoint of the real-world photograph displayed in the middle column.  The right columns in (b) and (c) present the display's angular intensity distributions in spherical coordinates.  The radius denotes the combined angular deviation from the optical axis, computed from the horizontal and vertical incidence angles. These plots illustrate how the relative intensity changes with viewing angle. </p>"},{"location":"publications/lensless_display_radiance_field/#abstract","title":"Abstract","text":"<p>Calibrating displays is a basic and regular task that content creators must perform to maintain optimal visual experience, yet it remains a troublesome issue.  Measuring display characteristics from different viewpoints often requires bulky equipment and a dark room, making it inaccessible to most users.  To avoid such hardware requirements in display calibrations, our work co-designs a lensless camera and an Implicit Neural Representation based algorithm for capturing display characteristics from various viewpoints.  More specifically, our pipeline enables efficient reconstruction of light fields emitted from a display from a viewing cone of 46.6\u00b0 \u00d7 37.6\u00b0. Our emerging pipeline paves the initial steps towards effortless display calibration and characterization.</p>"},{"location":"publications/lensless_display_radiance_field/#proposed-method","title":"Proposed Method","text":"<p>Our method reconstructs the display\u2019s emitted light field from a minimal number of lensless captures. We co-design a compact lensless camera prototype and an Implicit Neural Representation (INR)-based reconstruction network.  The hardware captures angularly varying point spread functions (PSF) from display pixels, while the software learns a continuous radiance field representing the pixel emission pattern across viewing directions.</p>"},{"location":"publications/lensless_display_radiance_field/#lensless-camera-design","title":"Lensless Camera Design","text":"<p> The display pixel captured with a microscope (left). The Point Spread Function (PSF) captured with the proposed lensless camera (middle). The captured lensless image from the display pixel (right). </p> <p>We employ a phase mask and five-aperture array to record the directional light from each display pixel without using lenses.  Each pixel on the display emits light rays that, after passing through the diffuser, form unique caustic patterns on the sensor plane.  By pre-capturing the PSF stack, we expand the Field Of View (FoV) by turning on and off pixels at different locations, which corresponds to different PSF:</p> <p></p> <p>Following this idea, we build a setup that contains a \\(4f\\) system and our proposed prototype is positioned on a linear stage.  We use it to capture the PSFs at different incoming light positions:</p> <p></p>"},{"location":"publications/lensless_display_radiance_field/#implicit-neural-representation","title":"Implicit Neural Representation","text":"<p> Each MLP layer consists of 32 neurons and a sinusoidal activation function.   We apply positional encoding with varying frequency levels (\\(L_f\\)) to each input coordinate group.   After concatenating these encoded features, the model processes them to reconstruct the light field.   We then perform linear convolution between this reconstruction and the pre-captured PSF to generate the predicted lensless image (\\(\\mathbf{I}_\\text{pred}\\)).   Finally, we compute the loss with \\(\\mathbf{I}_\\text{pred}\\) and \\(\\mathbf{I}_\\text{gt}\\).</p> <p> To avoid exhaustive angular sampling, we represent the 4D light field \\(\\mathbf{L}(u,v,s,t)\\) using a coordinate-based multilayer perceptron (MLP).  The network takes spatial coordinates \\((x, y)\\) and angular coordinates \\((u, v, s, t)\\) as input and outputs predicted radiance values. Positional encodingpreserves high-frequency variations, while convolution with the measured PSF produces the predicted lensless image \\(\\mathbf{I}_\\text{pred}\\).  The model is trained end-to-end by minimizing an \\(L_1\\) reconstruction loss with regularization for valid intensity range.  Once trained with captures from only nine display positions, the model generalizes to unseen viewpoints, reconstructing the display\u2019s angular emission profiles.</p> <p></p>"},{"location":"publications/lensless_display_radiance_field/#conclusion","title":"Conclusion","text":"<p>To benchmark our method, we follow the ISO standard (ISO9241-305): align a camera with the display in a dark room, display full-screen white stimuli, and capture images from \\(-10^\\circ\\) to \\(16^\\circ\\) vertical incident angles.</p> <p></p> <p>The above image shows our method reproduces the ISO intensity trend, validating its physical plausibility. We capture light field data from nine display positions without camera rotation or controlled lighting, reducing measurement time and simplifying view-dependent calibration while maintaining comparable accuracy.</p> <p>Our \\(46.6^\\circ\\) angular coverage remains well below the \\(240^\\circ\\) required in professional display calibration. We could extend this range by co-optimizing the apertures and the diffuser design. Beyond angular coverage, pixel-wise training hinders scalability for full-panel characterization. To overcome these constraints, we outline several promising research directions. First, developing an end-to-end pipeline that eliminates PSF convolutions, cropping, and padding. Second, hash encoding or Gaussian splatting could model light fields more efficiently.</p>"},{"location":"publications/lensless_display_radiance_field/#photo-gallery","title":"Photo gallery","text":"<p>Here, we release photographs from our visit to the conference, highlighting parts of our SIGGRAPH Asia 2025 experience.</p> <p> </p>"},{"location":"publications/lensless_display_radiance_field/#relevant-research-works","title":"Relevant research works","text":"<p>Here are relevant research works from the authors:</p> <ul> <li>SpecTrack: Learned Multi-Rotation Tracking via Speckle Imaging</li> <li>Unrolled Primal-Dual Networks for Lensless Cameras</li> <li>Odak</li> </ul>"},{"location":"publications/lensless_display_radiance_field/#outreach","title":"Outreach","text":"<p>We host a Slack group with more than 250 members. This Slack group focuses on the topics of rendering, perception, displays and cameras. The group is open to public and you can become a member by following this link.</p>"},{"location":"publications/lensless_display_radiance_field/#contact-us","title":"Contact Us","text":"<p>Warning</p> <p>Please reach us through email to provide your feedback and comments.</p>"},{"location":"publications/multi_color/","title":"Multi-color Holograms Improve Brightness in Holographic Displays","text":""},{"location":"publications/multi_color/#multi-color-holograms-improve-brightness-in-holographic-displays","title":"Multi-color Holograms Improve Brightness in Holographic Displays","text":""},{"location":"publications/multi_color/#people","title":"People","text":"<p>Koray Kavakl\u0131<sup>1</sup></p> <p>Liang Shi<sup>2</sup></p> <p>Hakan Urey<sup>1</sup></p> <p>Wojciech Matusik<sup>2</sup></p> <p>Kaan Ak\u015fit<sup>3</sup></p> <p> <sup>1</sup>Ko\u00e7 University, <sup>2</sup>Massachusetts Institute of Technology, <sup>3</sup>University College London </p> <p>SIGGRAPH Asia 2023</p>"},{"location":"publications/multi_color/#resources","title":"Resources","text":"<p> Manuscript Supplementary Code Project video</p>  Bibtex <pre><code>@inproceedings{kavakli2023multicolor,\n  title={Multi-color Holograms Improve Brightness in Holographic Displays},\n  author={Kavakl\u0131, Koray and Shi, Liang and Urey, Hakan and Matusik, Wojciech and Ak\u015fit, Kaan},\n  booktitle = {SIGGRAPH Asia 2023 Conference Papers},\n  articleno = {20},\n  numpages = {11},\n  keywords = {Brightness, Computer-generated holography, Holographic displays},\n  location = {Sydney, NSW, Australia},\n  series = {SA '23},\n  month={December},\n  year={2023},\n  doi={https://doi.org/10.1145/3610548.3618135}\n}\n</code></pre>"},{"location":"publications/multi_color/#video","title":"Video","text":""},{"location":"publications/multi_color/#presentation","title":"Presentation","text":""},{"location":"publications/multi_color/#abstract","title":"Abstract","text":"<p>Holographic displays generate Three-Dimensional (3D) images by displaying single-color holograms time-sequentially, each lit by a single-color light source. However, representing each color one by one limits brightness in holographic displays. This paper introduces a new driving scheme for realizing brighter images in holographic displays. Unlike the conventional driving scheme, our method utilizes three light sources to illuminate each displayed hologram simultaneously at various intensity levels. In this way, our method reconstructs a multiplanar three-dimensional target scene using consecutive multi-color holograms and persistence of vision. We co-optimize multi-color holograms and required intensity levels from each light source using a gradient descent-based optimizer with a combination of application-specific loss terms. We experimentally demonstrate that our method can increase the intensity levels in holographic displays up to three times, reaching a broader range and unlocking new potentials for perceptual realism in holographic displays.</p>"},{"location":"publications/multi_color/#results","title":"Results","text":"<p>Conventional holographic displays use a single Spatial Light Modulator (SLM) and reconstruct full-color images by time-sequentially displaying single-color holograms, each dedicated to a color channel. When holographic displays reconstruct scenes with brightness levels beyond the peak intensity of their corresponding color channels, the result could often lead to darker images than the intended levels and produce visual distortions or color mismatches (see conventional case below figure). In such cases, the dynamic range of the target is typically limited to the peak intensity of the light source, which is often not enough to deliver the desired visual experience.</p> <p></p> <p>Without altering hardware, we argue that holographic displays could dedicate extra time to each color channel to improve their perceived brightness levels, as demonstrated in below figure. Our work aims to improve holographic displays' dynamic range by more effectively but aggressively utilizing color primaries and holograms. For this purpose, we introduce a new Computer-Generated Holography (CGH) driving scheme. In this scheme, multi-color holograms simultaneously operate over multiple wavelengths of light and provide 3D multiplanar images.  We calculate multi-color holograms using a Gradient Descent (GD) based solver guided by a combination of application-specific loss functions. In the meantime, we co-optimize the brightness levels required to illuminate each multi-color hologram. We experimentally verify our findings using a holographic display prototype by showing reconstructions of brighter scenes with a broader dynamic range in an artifact-free and color-accurate manner.</p> <p></p> <p>Below figure shows photographs from our holographic display for conventional and our schemes (more sample results in our supplementary). For such a scene, our method can safely support up to \\(\\times1.8\\) peak brightness without causing significant image distortions or artifacts. On the other hand, the conventional hologram fails to support peak brightness higher than \\(\\times1.0\\). Beyond \\(\\times1.8\\) peak brightness levels, images are typically heavily dominated by noise in the conventional case.</p> <p></p> <p>In contrast, our method loses color integrity slightly or generates noises similar to the conventional case's \\(\\times1.2\\) peak brightness case.</p> <p></p> <p>Our method can also support three-dimensional multiplanar images.</p> <p></p>"},{"location":"publications/multi_color/#relevant-research-works","title":"Relevant research works","text":"<p>Here are relevant research works from the authors:</p> <ul> <li>HoloBeam: Paper-Thin Near-Eye Displays</li> <li>Realistic Defocus for Multiplane Computer-Generated Holography</li> <li>Optimizing Vision and Visuals: Lectures on Cameras, Displays, and Perception</li> <li>Learned Holographic Light Transport</li> <li>Metameric Varifocal Holograms</li> <li>Odak</li> </ul>"},{"location":"publications/multi_color/#external-other-links","title":"External Other Links","text":"<p>Here are links related to our project such as videos, articles or podcasts:</p> <ul> <li>ACM SIGGRAPH Asia 2023, Technical Papers Fast Forward (Preview the presentations on 13 Dec, Day 2)</li> </ul>"},{"location":"publications/multi_color/#outreach","title":"Outreach","text":"<p>We host a Slack group with more than 250 members. This Slack group focuses on the topics of rendering, perception, displays and cameras. The group is open to public and you can become a member by following this link.</p>"},{"location":"publications/multi_color/#contact-us","title":"Contact Us","text":"<p>Warning</p> <p>Please reach us through email to provide your feedback and comments.</p>"},{"location":"publications/multi_color/#acknowledgements","title":"Acknowledgements","text":"<p>Kaan Ak\u015fit is supported by the Royal Society's RGS\\R2\\212229 - Research Grants 2021 Round 2 in building the hardware prototype. Kaan Ak\u015fit is also supported by Meta Reality Labs inclusive rendering initiative 2022. Liang Shi is supported by Meta Research PhD fellowship (2021-2023).  </p> <p>Hakan Urey is supported by the European Innovation Council\u2019s HORIZON-EIC-2021-TRANSITION-CHALLENGES program Grant Number 101057672 and T\u00fcbitak\u2019s 2247-A National Lead Researchers Program, Project Number 120C145.  </p>"},{"location":"publications/multitasking_perceptual_graphics/","title":"Learned Single-Pass Multitasking Perceptual Graphics for Immersive Displays","text":""},{"location":"publications/multitasking_perceptual_graphics/#learned-single-pass-multitasking-perceptual-graphics-for-immersive-displays","title":"Learned Single-Pass Multitasking Perceptual Graphics for Immersive Displays","text":""},{"location":"publications/multitasking_perceptual_graphics/#people","title":"People","text":"<p>Do\u011fa Y\u0131lmaz<sup>1</sup></p> <p>He Wang<sup>1</sup></p> <p>Towaki Takikawa<sup>2</sup></p> <p>Duygu Ceylan<sup>3</sup></p> <p>Kaan Ak\u015fit<sup>1</sup></p> <p> <sup>1</sup>University College London, <sup>2</sup>University of Toronto, <sup>3</sup>Adobe Research </p> <p>ACM Multimedia 2025</p>"},{"location":"publications/multitasking_perceptual_graphics/#resources","title":"Resources","text":"<p> Manuscript Supplementary Code</p>  Bibtex <pre><code>@inproceedings{yilmaz2025perceptual,\n  author = {Y{\\i}lmaz, Do{\\u{g}}a and Wang, He and Takikawa, Towaki and Ceylan, Duygu and Ak{\\c{s}}it, Kaan},\n  title = {Learned Single-Pass Multitasking Perceptual Graphics for Immersive Displays},\n  booktitle = {Proceedings of the 33rd ACM International Conference on Multimedia},\n  year = {2025},\n  location = {Dublin, Ireland},\n  publisher = {ACM},\n  address = {New York, NY, USA},\n  pages = {9},\n  doi = {https://doi.org/10.1145/3746027.3754801},\n  month = {October 27--31}\n}\n</code></pre>"},{"location":"publications/multitasking_perceptual_graphics/#video","title":"Video","text":""},{"location":"publications/multitasking_perceptual_graphics/#abstract","title":"Abstract","text":"<p>Emerging immersive display technologies efficiently utilize resources with perceptual graphics methods such as foveated rendering and denoising. Running multiple perceptual graphics methods challenges devices with limited power and computational resources. We propose a computationally-lightweight learned multitasking perceptual graphics model. Given RGB images and text-prompts, our model performs text-described perceptual tasks in a single inference step. Simply daisy-chaining multiple models or training dedicated models can lead to model management issues and exhaust computational resources. In contrast, our flexible method unlocks consistent high quality perceptual effects with reasonable compute, supporting various permutations at varied intensities using adjectives in text prompts (e.g. mildly, lightly). Text-guidance provides ease of use for dynamic requirements such as creative processes. To train our model, we propose a dataset containing source and perceptually enhanced images with corresponding text prompts. We evaluate our model on desktop and embedded platforms and validate perceptual quality through a user study.</p> <p></p>"},{"location":"publications/multitasking_perceptual_graphics/#relevant-research-works","title":"Relevant research works","text":"<p>Here are relevant research works from the authors:</p> <ul> <li>ChromaCorrect: Prescription Correction in Virtual Reality Headsets through Perceptual Guidance</li> <li>Metameric Inpainting for Image Warping</li> <li>Metameric Varifocal Holograms</li> <li>Optimizing vision and visuals: lectures on cameras, displays and perception</li> <li>Beyond blur: ventral metamers for foveated rendering</li> <li>Foveated AR: dynamically-foveated augmented reality display</li> <li>Odak</li> </ul>"},{"location":"publications/multitasking_perceptual_graphics/#outreach","title":"Outreach","text":"<p>We host a Slack group with more than 400 members. This Slack group focuses on the topics of rendering, perception, displays and cameras. The group is open to public and you can become a member by following this link.</p>"},{"location":"publications/multitasking_perceptual_graphics/#contact-us","title":"Contact Us","text":"<p>Warning</p> <p>Please reach us through email to provide your feedback and comments.</p>"},{"location":"publications/realistic_defocus_cgh/","title":"Realistic Defocus for Multiplane Computer-Generated Holography","text":""},{"location":"publications/realistic_defocus_cgh/#realistic-defocus-blur-for-multiplane-computer-generated-holography","title":"Realistic Defocus Blur for Multiplane Computer-Generated Holography","text":""},{"location":"publications/realistic_defocus_cgh/#people","title":"People","text":"<p>Koray Kavakl\u0131<sup>1</sup></p> <p>Yuta Itoh<sup>2</sup></p> <p>Hakan \u00dcrey<sup>1</sup></p> <p>Kaan Ak\u015fit<sup>3</sup></p> <p><sup>1</sup>Ko\u00e7 University, <sup>2</sup>The University of Tokyo <sup>3</sup>University College London</p> <p>IEEE VR 2023</p>"},{"location":"publications/realistic_defocus_cgh/#resources","title":"Resources","text":"<p> Manuscript Project video Code</p>  Bibtex <pre><code>@misc{kavakli2022realisticdefocus,\n  doi = {10.48550/ARXIV.2205.07030},\n  url = {https://arxiv.org/abs/2205.07030},\n  author = {Kavakl\u0131, Koray and Itoh, Yuta and Urey, Hakan and Ak\u015fit, Kaan},\n  keywords = {Computer Vision and Pattern Recognition (cs.CV), Graphics (cs.GR), FOS: Computer and information sciences, FOS: Computer and information sciences, I.3.3},\n  title = {Realistic Defocus Blur for Multiplane Computer-Generated Holography},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}\n}\n</code></pre>"},{"location":"publications/realistic_defocus_cgh/#presentation","title":"Presentation","text":""},{"location":"publications/realistic_defocus_cgh/#video","title":"Video","text":""},{"location":"publications/realistic_defocus_cgh/#abstract","title":"Abstract","text":"<p>This paper introduces a new multiplane CGH computation method to reconstruct artefact-free high-quality holograms with natural-looking defocus blur.  Our method introduces a new targeting scheme and a new loss function. While the targeting scheme accounts for defocused parts of the scene at each depth plane, the new loss function analyzes focused and defocused parts separately in reconstructed images. Our method support phase-only CGH calculations using various iterative (e.g., Gerchberg-Saxton, Gradient Descent) and non-iterative (e.g., Double Phase) CGH techniques.  We achieve our best image quality using a modified gradient descent-based optimization recipe where we introduce a constraint inspired by the double phase method. We validate our method experimentally using our proof-of-concept holographic display, comparing various algorithms, including multi-depth scenes with sparse and dense contents.</p>"},{"location":"publications/realistic_defocus_cgh/#results","title":"Results","text":"<p>In this work, we demonstrate a new rendering pipeline for multiplane Computer-Generated Holography that can provide near-accurate defocus blur.</p> <p></p> <p>Our results suggest that our work can help alliviate unintended artifacts found on existing rendering pipelines for Computer-Generated Holography.</p> <p></p> <p>We capture these results using our in-house baked holographic display prototype.</p> <p></p> <p>Our technique is suitable for Augmented Reality applications (e.g., near-eye displays, heads-up displays). Here we provide photographs of virtual images generated by our computer-generated holography pipeline overlayed on an actual scene. Note that each image is focused at a different depth level.</p> <p></p> <p>Here we show a photograph of our holographic display prototype with Augmented Reality support.</p> <p></p>"},{"location":"publications/realistic_defocus_cgh/#relevant-works-from-our-group","title":"Relevant works from our group","text":"<p>Here are relevant research works from our group:</p> <ul> <li>Odak</li> <li>Metameric Varifocal Holograms</li> <li>Learned Holographic Light Transport</li> <li>HoloBeam: Paper-Thin Near-Eye Displays</li> </ul>"},{"location":"publications/realistic_defocus_cgh/#contact","title":"Contact","text":"<p>Have any queries, questions, suggestions or comments, contact us via kaanaksit@kaanaksit.com.</p>"},{"location":"publications/realistic_defocus_cgh/#acknowledgements","title":"Acknowledgements","text":"<p>We also thank  Erdem Ulusoy and G\u00fcne\u015f Ayd\u0131ndo\u011fan for discussions in the early phases of the project;  Tim Weyrich and Makoto Yamada for dedicating GPU resources in various experimentation phases; David Walton for his feedback on the manuscript;</p> <p>Yuta Itoh is supported by the JST FOREST Program Grant Number JPMJPR17J2 and JSPS KAKENHI Grant Number JP20H05958 and JP21K19788.  </p> <p>Hakan Urey is supported by the European Innovation Council's HORIZON-EIC-2021-TRANSITION-CHALLENGES program Grant Number 101057672.  </p> <p>Kaan Ak\u015fit is supported by the Royal Society's RGS\\R2\\212229 - Research Grants 2021 Round 2 in building the hardware prototype.  </p>"},{"location":"publications/spec_track/","title":"SpecTrack: Learned Multi-Rotation Tracaking via Speckle Imaging","text":""},{"location":"publications/spec_track/#spectrack-learned-multi-rotation-tracking-via-speckle-imaging","title":"SpecTrack: Learned Multi-Rotation Tracking via Speckle Imaging","text":""},{"location":"publications/spec_track/#people","title":"People","text":"<p>Ziyang Chen<sup>1</sup></p> <p>Do\u011fa Do\u011fan<sup>2</sup></p> <p>Josef Spjut<sup>3</sup></p> <p>Kaan Ak\u015fit<sup>1</sup></p> <p> <sup>1</sup>University College London, <sup>2</sup>Adobe Research, <sup>3</sup>NVIDIA </p> <p>SIGGRAPH Asia 2024 Poster</p> <p> </p><p>Honorable Mention Award</p><p></p>"},{"location":"publications/spec_track/#resources","title":"Resources","text":"<p> Manuscript Poster Supplementary Code</p>  Bibtex <pre><code>@inproceedings{chen2024spectrack,\n  author = {Ziyang Chen and Mustafa Dogan and Josef Spjut and Kaan Ak{\\c{s}}it},\n  title = {SpecTrack: Learned Multi-Rotation Tracking via Speckle Imaging},\n  booktitle = {SIGGRAPH Asia 2024 Posters (SA Posters '24)},\n  year = {2024},\n  location = {Tokyo, Japan},\n  publisher = {ACM},\n  address = {New York, NY, USA},\n  pages = {2},\n  doi = {10.1145/3681756.3697875},\n  url = {https://doi.org/10.1145/3681756.3697875},\n  month = {December 03--06}\n}\n</code></pre>"},{"location":"publications/spec_track/#video","title":"Video","text":""},{"location":"publications/spec_track/#abstract","title":"Abstract","text":"<p>Precision pose detection is increasingly demanded in fields such as personal fabrication, Virtual Reality (VR), and robotics due to its critical role in ensuring accurate positioning information. However, conventional vision-based systems used in these systems often struggle with achieving high precision and accuracy, particularly when dealing with complex environments or fast-moving objects. To address these limitations, we investigate Laser Speckle Imaging (LSI), an emerging optical tracking method that offers promising potential for improving pose estimation accuracy. Specifically, our proposed LSI-Based Tracking leverages the captures from a lensless camera and a retro-reflector marker with a coded aperture to achieve multi-axis rotational pose estimation with high precision. Our extensive trials using our in-house built testbed have shown that SpecTrack achieves an accuracy of \\(0.31^\\circ\\) (std=\\(0.43^\\circ\\)) , significantly outperforming state-of-the-art approaches and improving accuracy up to \\(200\\%\\).</p>"},{"location":"publications/spec_track/#proposed-method","title":"Proposed Method","text":"<p>We aim to remotely obtain multiple absolute rotation angles from a coded retroreflective marker by utilizing the overlapping patterns generated by the multi-wavelength laser. the laser beam from the source (\\(S\\)) hits an arbitrary point (\\(P\\)) and diffracts at slightly different angles due to the different wavelengths (\\(\\lambda_0\\) and \\(\\lambda_1\\)). This phenomenon shows a correlation between the surface rotation angle and the captured speckle image.</p> <p>The first image below shows the structure of the proposed sensor, which contains a bare sensor, laser source and beam splitter (\\(10~mm \\times 10~mm\\)). The beam splitter is placed in front of the bare imaging sensor to ensure that most of the light reflected from the marker covers a large area of the sensor. Additionally, this co-axial optical layout eliminates the light source's lateral offsets, simplifying the speckle behavior in the rotations.</p> <p></p> <p>We can tell from the image below that the captured image formed overlappings in when the surface rotates \\(10^\\circ\\) in the y-axis.</p> <p></p> <p>Using the Fast Fourier Transform (FFT) to get the magnitudes of speckle image from various poses (y-axis roation, z-axis rotations, and z-axis displacements) or coded surface reveals interpretable patterns:</p> <p></p> <p>We employ a shallow neural network to handle the non-linearities of physical aspects and estimate the absolute rotation angles from speckle patterns.</p> <p></p> <p>Firstly, we preprocess the captured monochrome speckle frames \\(I_{speckle}\\) (\\(640\\times360\\)~px) by transforming them into the frequency domain \\(\\mathcal{F}(I_{speckle})\\) using the FFT. Then the frames are central cropped and concatenated into a tensor \\([\\mathcal{F}(I_{\\text{speckle}, i})]_{i=1}^5\\) with a shape of \\((5,320,180)\\). From our practical experiences, this concatenated frame tensor provides more robust results when the marker is in motion because it incorporates temporal information. After that, we feed the samples into three convolutional blocks, each comprising a 2D convolution layer, batch normalization, ReLU activation function, and max pooling. After the convolution, the sample is flattened and inputted into a Multi Layer Perceptron (MLP) containing six linear layers each layer is followed by a batch normalization and ReLu activation function.  The final layer of MLP outputs the rotation angles \\(\\theta_y\\), \\(\\theta_z\\) and the arbitrary depth \\(d_z\\).</p> <p>Since capturing samples in all six Degrees Of Freedom simultaneously is physically difficult, we focus on capturing the speckle imaging as the marker rotates in the z-axis and y-axis. We add controlled close-loop motors to a rotary stage to automatically capture the speckle images when the marker is rotated in various axes, as shown below. During the data collection, we control the motors to rotate the marker from \\(0^\\circ\\) to \\(40^\\circ\\) on the y-axis and \\(0^\\circ\\) to \\(90^\\circ\\) the z-axis. Besides the rotations, we repeat the experiment in different depths from \\(16~cm\\) to \\(28~cm\\).</p> <p></p>"},{"location":"publications/spec_track/#conclusions","title":"Conclusions","text":""},{"location":"publications/spec_track/#baseline","title":"Baseline","text":"<p>We compare our work with the state-of-the-art from Gibson et al. However, we lack direct access to accurate measurements, such as the wavelengths emitted by the off-the-shelf laser diode. We subsequently employed a gradient descent-based optimization with a captured training set to get the unknown variables: dominant wavelength \\(\\lambda_0\\), wavelength differences \\(\\Delta \\lambda\\), where \\(\\Delta \\lambda = \\lambda_0 - \\lambda_1 \\ll \\lambda_0\\), and light source position \\(S\\) in the 3D space. Following this, we tested the analytical model proposed by the authors with the test set that contains the speckle images captured when the marker rotates from \\(0^\\circ\\) to \\(40^\\circ\\) in the y-axis. The baseline result indicates the Mean Absolute Error (MAE) of \\(0.60^\\circ\\) (\\(std=0.35^\\circ\\)) on our testbed.</p> <p>SpecTrack achieved a lower MAE and std: \\(\\mathbf{0.31^\\circ}\\), \\(\\mathbf{0.44^\\circ}\\), respectively. At the same time, the model can estimate the z-axis rotations rotation with a MAE \\(\\mathbf{0.52^\\circ}\\) (\\(std=\\mathbf{0.36^\\circ}\\)). Furthermore, the model adapts to varying depths, showing an accuracy of \\(0.15~cm\\). </p>"},{"location":"publications/spec_track/#future-work","title":"Future work","text":"<p>Testing and optimizing the system in real-world environments, considering varying lighting, distances, and object motions, is crucial for successful operation in various applications including VR, AR, and robotics.</p>"},{"location":"publications/spec_track/#photo-gallery","title":"Photo gallery","text":"<p>Here, we release photographs from our visit to the conference, highlighting parts of our SIGGRAPH ASIA 2024 experience.</p> <p> </p>"},{"location":"publications/spec_track/#relevant-research-works","title":"Relevant research works","text":"<p>Here are relevant research works from the authors:</p> <ul> <li>Optimizing Vision and Visuals: Lectures on Cameras, Displays, and Perception</li> <li>Unrolled Primal-Dual Networks for Lensless Camera</li> <li>Odak</li> </ul>"},{"location":"publications/spec_track/#outreach","title":"Outreach","text":"<p>We host a Slack group with more than 250 members. This Slack group focuses on the topics of rendering, perception, displays and cameras. The group is open to public and you can become a member by following this link.</p>"},{"location":"publications/spec_track/#contact-us","title":"Contact Us","text":"<p>Warning</p> <p>Please reach us through email to provide your feedback and comments.</p>"},{"location":"teaching/","title":"Courses","text":"<p>The computational light laboratory offers lectures on various topics, including computational optics, computational displays, perceptual graphics and computational fabrication. For the entire list of lectures offered at the time being, please follow the menu at the right-hand side of this text. The lectures that we have offered so far are as follows:</p> Term Instructor(s) Course Content - Kaan Ak\u015fit - Computational Light Winter 2023 Kaan Ak\u015fit COMP0088 Introduction to Machine Learning Summer 2022 Kaan Ak\u015fit SIGGRAPH 2022 Optimizing Vision and Visuals: Lectures on Cameras, Displays and Perception Spring 2022 Kaan Ak\u015fit COMP0160 Lecture 2: Visual perception in perceptual graphics and computational displays Spring 2022 Kaan Ak\u015fit COMP0160 Lecture 7: Integrating Sensory Information in Computational Displays"},{"location":"teaching/comp0160_coursework_1/","title":"Coursework 1","text":""},{"location":"teaching/comp0160_coursework_1/#course-work-i-comp0160-perception-and-interfaces","title":"Course work I: COMP0160 Perception and Interfaces","text":"<p>Release date: 24th January 2022</p> <p>Due date:  7th February 2022</p> <p>Instructor: Kaan Ak\u015fit</p>"},{"location":"teaching/comp0160_coursework_1/#background","title":"Background","text":"<p>COMP0160: Perception and Interface Course offers students a gateway to get familiar with various aspects of perception and interfaces. This document is designed to explain to you the task that has to be conducted by students for delivering a successful work as an outcome of your assignment. Specifically, the documentation is for the first-course work in the perception and interfaces course. The topic of the first-course work is related to the human visual system. As the students compile their course work, they will have a strong understanding of how the human visual system works under different eye prescriptions (e.g., myopia, astigmatism). Having a detailed understanding of the given topic can help students to have insights towards solving problems in domains such as computational displays, perceptual graphics and computational imaging. The software tools used in this course are publicly available.  They are shared across the forefront of various industries and academia (e.g., data science to computational approaches in physics, biology or chemistry).</p>"},{"location":"teaching/comp0160_coursework_1/#requirements","title":"Requirements","text":"<p>This assignment assumes that you have an understanding of programming with Python language and you are familiar with Torch library that provides access to linear algebra calculations.  Within this assignment, you will be asked to deliver your solution in a Jupyter Notebook format.  The students are expected to be familiar with working Jupyter Notebooks and know the details regarding saving notebooks so that they can deliver their work for evaluation in the required form. We typically use Matplotlib library for plotting purposes while using Jupyter Notebooks.</p> <p>In our production machines, we use the Python distribution <code>3.9.7</code>, Torch distribution <code>1.9.0</code>, Matplotlib distribution <code>3.3.4</code> and Jupyter Notebook distribution <code>6.2.0</code>. For successfully compiling the assignment, make sure to have these libraries installed on your computer properly. Given that you are going to compile your work with <code>Torch</code>, you can either choose to run your code on <code>CPU</code> or <code>GPU</code> by selecting the proper device in your code. However, at the time you deliver your code, please make sure that your code runs on <code>CPU</code>. As a practical observation, you can also get a sense of speed differences in between those two devices and report within your Jupyter notebook (optional). We typically run these on a Linux operating system.  However, it is not a requirement for students to use the same operating system as these components also run on your favourite operating system (e.g., Windows, Android, Mac OS or alike). In your course work, make sure to add <code>docstring</code> type documentation for every function in your code and make sure to comment in between lines to explain your steps within a function.</p> <p>Before starting with the tasks, we encourage students to attend the second lecture of the perception and interfaces course, namely <code>Lecture 2: Visual Perception in Perceptual Graphics and Computational Displays</code> (recording available on Moodle).</p>"},{"location":"teaching/comp0160_coursework_1/#some-useful-links-for-beginners","title":"Some useful links for beginners:","text":"<ul> <li>Absolute beginners guide for Python and Jupyter Notebook</li> <li>Simple algebric operations in Torch,</li> </ul> <p>Special note from your instructor: We design this homework to make you be better in your next in your life. If you do not have the right background to use the tools proposed in this coursework or if you are a confused absolute beginner, please do not hesitate to reach out to us through Moodle. We are here to support you. Please carefully frame your questions as you approach us for support (e.g., what you want to ask and what you expect) so that we can support you at our best.</p>"},{"location":"teaching/comp0160_coursework_1/#problem-description","title":"Problem description","text":"<p>Each and every one of us has a unique visual system. At the heart of our visual system lies our eyes. Our eyes can be simplified as an optical instrument that images a three-dimensional scene to our retinas, sensor alike cellular structure. In this assignment, your task is to develop a user interface on a Jupyter Notebook that simulates how our visions are being affected by various kinds of eye prescriptions. To simply put, how would you perceive a scene if you had a certain prescription. We expect you to have this simulator in live view, enabling the user to choose different eye prescriptions based on Zernike Polynomials. Before conducting any work, we suggest you go through the listed references below:</p> <ul> <li>Zernike Polynomials,</li> <li>Watson, Andrew B. \"Computing human optical point spread functions.\" Journal of Vision 15.2 (2015): 26-26,</li> <li>Chakravarthula, Praneeth, et al. \"Gaze-contingent retinal speckle suppression for perceptually-matched foveated holographic displays.\" IEEE Transactions on Visualization and Computer Graphics 27.11 (2021): 4194-4203, -- observe Figure 4 here for sample point-spread functions.</li> <li>Animation examples with Matplotlib and Widgets with Matplotlib,</li> </ul> <p>In addition, you can get a sense of the importance of prescription in next-generation display technologies by going through the survey paper below (actual industrial applications from learning of this course work):</p> <ul> <li>Koulieris, George Alex, et al. \"Near\u2010eye display and tracking technologies for virtual and augmented reality.\" Computer Graphics Forum. Vol. 38. No. 2. 2019.</li> </ul> <p>These references can help you to find the required technical details for your subtasks.</p>"},{"location":"teaching/comp0160_coursework_1/#zernike-polynomial-generator-10-points","title":"Zernike Polynomial generator (10 points)","text":"<p>The first task is to derive a Pythonic class that can generate Zernike Polynomials on demand. These polynomials can help you to represent the point-spread functions of people with a prescription. A point-spread function can be best described as the system response of your eye to a given scene. In the way you will use point-spread functions, they can be described as kernels that can help you describe your eye as a linear transform or a system that is represented with a single convolution. Once you have fully compiled the <code>zernike_polynomial_generator</code> class in your Notebook, please proceed with the next task:</p> <pre><code>class zernike_polynomial_generator():\n\n    def __init__(self):\n          ...\n</code></pre> <p>Our expectation from you, in this case, is to have multiple functions in your <code>zernike_polynomial_generator</code> class that spits out various point-spread functions, read more from here (7.5 points). A person may be having an eye prescription composed of various point-spread functions. To support such a case, make sure to add a function in your class that outputs a weighted sum of chosen point-spread functions (2.5 points).</p>"},{"location":"teaching/comp0160_coursework_1/#forward-model-10-points","title":"Forward model (10 points)","text":"<p>We will work with an assumption that our eyes are responding to every point on a given scene in the same way (e.g., stationary kernels, not spatially varying kernels). You have to have a function that is able to load images from a given path.</p> <pre><code>def load_image(filename):\n     \"\"\"\n    Function to load an image.\n\n    Parameters\n    ------------\n    filename            : str\n                                Filename of the image.\n\n    Returns\n    --------\n    image               : torch.tensor\n                               Loaded image.\n     \"\"\"\n     ....\n    return image\n</code></pre> <p>Please do not hesitate to use images from Creative Commons for your experiments, and please make sure that these images are clean, meaning ethically good to work with. Please also make sure to work with images that has <code>1920x1080x3</code> resolution and please reduce the image into a single color image by taking the average across second axis (<code>1920x1080</code> - Black and white). Make sure to provide the image that you use together with your Jupyter Notebook in a compressed file format (ZIP).</p> <p>You will be using this specific image load definition to load images, and you will process these images with your forward model function. Here forward model corresponds to convolving the loaded image with a combination of Zernike polynomials to simulate various kinds of eye prescriptions. In the simplest form, your forward model should look like below:</p> <pre><code>def forward(image, psf):\n     \"\"\"\n    Forward model, convolving the given image with a given point-spread function.\n\n    Parameters\n    ------------\n    image              : torch.tensor\n                               Image as a torch tensor (MxN).\n    psf                   : torch.tensor\n                              Point-spread function as a torch tensor (MxN).\n\n    Returns\n    --------\n    result               : torch.tensor\n                               Abberated image.\n     \"\"\"\n     ....\n     return result\n</code></pre> <p>You will receive your 3 points for loading images properly (3 points). The remaining 7 points will be dedicated to the forward model definition (7 points).</p> <p>Hint for the forward model: torch.nn.Conv2d (Do not necessarilly have to use it but can help).</p>"},{"location":"teaching/comp0160_coursework_1/#visualizer-10-points","title":"Visualizer (10 points)","text":"<p>The last bit we want you to add to the Jupyter Notebook is related to the way you will be visualizing the outcome of your forward model. We want your code to be interactive as much as you can make it to be on  your given computer hardware. Make sure to visualize images of your forward model using <code>Matplotlib</code>. Make sure to provide buttons and controls for your users to choose different combinations of Zernike polynomial to formulate a point-spread function, and make sure to visualize the point-spread functions that you have generated. Note that we will heavily be relying on your visualizer to assess the outcome of your code; please pay attention to make sure that you have provided all the controls (either as variables to manipulate or buttons or sliders), and they are easy for a user to work with. Note that you are allowed to use other libraries beyond <code>Matplotlib</code> such as <code>Pyplot</code> or if you want to develop a user interface outside of the boundaries of a Jupyter Notebook, it is also ok. But if you do that, please make sure that you have communicated the change in a clear fashion and we are able to run your code.</p> <p>If you can plot the outcome of the forward model, this plotting can guarantee you half of the points you can receive (5 points). The remaining points can be received as you introduce more sophistication to your visualizer, as explained above (5 points).</p>"},{"location":"teaching/comp0160_coursework_1/#problem-and-potential-solutions-15-points","title":"Problem and potential solutions (15 points)","text":"<p>We want you to add a text section to your notebook, where you will find an unsolved/partially solved scientific problem related to eye prescription and visuals (displays, graphics or any other form). The source of this problem can be from the existing literature, and please make sure to survey using your favourite search engines, academic ones and non-academic ones (e.g., Brave, Google Scholar, etc.). The problem can also rely on your practical observations as well as long as you describe it clearly. You also provide potential solutions to the problem that you have found from the literature and your own predictions towards new solutions in the future. The text can not be more than 500 words, no less than 250 words. Note that the length of your text is not an indicator of success, and most powerful writing happens in shorter forms.</p> <p>You will receive half of the points from your problem description (7.5 points). The remaining half will be from your proposed solution (7.5 points).</p>"},{"location":"teaching/comp0160_coursework_1/#contacting-us","title":"Contacting Us","text":"<p>The prefered way of communication is through University College London's online lecture system, Moodle.  Please use the Moodle forum for your questions related to the course work.</p>"},{"location":"teaching/comp0160_perception_and_interfaces/","title":"Description","text":""},{"location":"teaching/comp0160_perception_and_interfaces/#comp0160-perception-and-interfaces","title":"COMP0160: Perception and Interfaces","text":""},{"location":"teaching/comp0160_perception_and_interfaces/#summary","title":"Summary","text":"<p>COMP0160: Perception and Interfaces course offers students a gateway to get familiar with various aspects of perception and interfaces. Greater details of the course and its broad description can be found in course website.</p> <p>Computational light laboratory contributes to <code>COMP0160: Perception and Interfaces</code> by providing two lectures introducing the human visual system, its relation with graphics and displays, and sensing modalities in emerging devices (e.g., near-eye displays for virtual reality and augmented reality). Each of these lectures is two hours long. In addition, we support these lectures with laboratory assignments for the students, which are vital for completing the course.</p>"},{"location":"teaching/comp0160_perception_and_interfaces/#timetable","title":"Timetable","text":"<p>The timetable provided below show parts of COMP0160 that are provided by computational light laboratory.</p> Date Instructor(s) Content 14 January 2022 - 27th March 2022 Kaan Ak\u015fit Practical 17th January 2022 - 23rd January 2022 Kaan Ak\u015fit Visual Perception in graphics and displays 28th February 2022 - 6th March 2022 Kaan Ak\u015fit Integrating Sensory Information in Computational Displays"},{"location":"teaching/comp0160_perception_and_interfaces/#parts","title":"Parts","text":""},{"location":"teaching/comp0160_perception_and_interfaces/#practical","title":"Practical","text":"<p> 12:00 noon to 1:00 pm, Fridays, 14th January 2022 - 27th March 2022</p> <p> Chandler House G15</p> <p> Description (Public)</p>"},{"location":"teaching/comp0160_perception_and_interfaces/#coursework","title":"Coursework","text":"<p> First coursework</p>"},{"location":"teaching/comp0160_perception_and_interfaces/#lecture-2-visual-perception-in-perceptual-graphics-and-computational-displays","title":"Lecture 2: Visual perception in perceptual graphics and computational displays","text":"<p> Winter 2022</p> <p> Online</p> <p> Recording (Password protected)</p> <p> Slides (Invitation required)</p> <p>This lecture focuses on human visual perception and its applications in computer graphics and computational display domains.</p> Details <p>Summary: The students will learn about human visual perception in this course. They will primarily learn about the eye and its structure. The information about the eye explained throughout the lecture will be linked to designing computational displays and perceptual graphics with real cases from the recent literature. Towards the end of this lecture, students will have enough information to build a simplified optical model of a human eye. They will be encouraged to build an eye model using this simplified optical simulation of the human eye.</p> <p>References:</p> <ul> <li> <p>Panero, Julius, and Martin Zelnik. Human dimension &amp; interior space: a source book of design reference standards. Watson-Guptill, 1979.</p> </li> <li> <p>Bekerman, Inessa, Paul Gottlieb, and Michael Vaiman. \"Variations in eyeball diameters of the healthy adults.\" Journal of ophthalmology 2014 (2014).</p> </li> <li> <p>Roberts, Bethany R., and Juliet L. Osborne. \"Testing the efficacy of a thermal camera as a search tool for locating wild bumble bee nests.\" Journal of Apicultural Research 58.4 (2019): 494-500.</p> </li> <li> <p>Park, George E., and Russell Smith Park. \"Further evidence of change in position of the eyeball during fixation.\" Archives of Ophthalmology 23.6 (1940): 1216-1230.</p> </li> <li> <p>Koulieris, George Alex, et al. \"Near\u2010eye display and tracking technologies for virtual and augmented reality.\" Computer Graphics Forum. Vol. 38. No. 2. 2019.</p> </li> <li> <p>Cakmakci, Ozan, and Jannick Rolland. \"Head-worn displays: a review.\" Journal of display technology 2.3 (2006): 199-216.</p> </li> <li> <p>De Groot, S. G., and J. W. Gebhard. \"Pupil size as determined by adapting luminance.\" JOSA 42.7 (1952): 492-495.</p> </li> <li> <p>Hunt, Robert William Gainer. \"Light and dark adaptation and the perception of color.\" JOSA 42.3 (1952): 190-199.</p> </li> <li> <p>Han, S. H., et al. \"The Change of Pupil Cycle Time after Occlusion Therapy in Amblyopia.\" Journal of the Korean Ophthalmological Society 38.2 (1997): 290-295.</p> </li> <li> <p>Fine, I., et al. \"Optical properties of the sclera.\" Physics in Medicine &amp; Biology 30.6 (1985): 565.</p> </li> <li> <p>Zoulinakis, Georgios, et al. \"Accommodation in human eye models: a comparison between the optical designs of Navarro, Arizona and Liou-Brennan.\" International journal of ophthalmology 10.1 (2017): 43.</p> </li> <li> <p>Herndon, Leon W., Jennifer S. Weizer, and Sandra S. Stinnett. \"Central corneal thickness as a risk factor for advanced glaucoma damage.\" Archives of ophthalmology 122.1 (2004): 17-21.</p> </li> <li> <p>Glasser, Adrian, and Melanie CW Campbell. \"Presbyopia and the optical changes in the human crystalline lens with age.\" Vision research 38.2 (1998): 209-229.</p> </li> <li> <p>Bharadwaj, Shrikant R., and Clifton M. Schor. \"Acceleration characteristics of human ocular accommodation.\" Vision Research 45.1 (2005): 17-28.</p> </li> <li> <p>Campbell, F. W., and G. Westheimer. \"Dynamics of accommodation responses of the human eye.\" The Journal of physiology 151.2 (1960): 285-295.</p> </li> <li> <p>Heron, Gordon, W. N. Charman, and C. Schor. \"Dynamics of the accommodation response to abrupt changes in target vergence as a function of age.\" Vision research 41.4 (2001): 507-519.</p> </li> <li> <p>Phillips, Stephen, Douglas Shirachi, and Lawrence Stark. \"Analysis of accommodative response times using histogram information.\" Optometry and Vision Science 49.5 (1972): 389-401.</p> </li> <li> <p>Deering, Michael F. \"A photon accurate model of the human eye.\" ACM Transactions on Graphics (TOG) 24.3 (2005): 649-658.</p> </li> <li> <p>Ratnam, Kavitha, et al. \"Relationship between foveal cone structure and clinical measures of visual function in patients with inherited retinal degenerations.\" Investigative ophthalmology &amp; visual science 54.8 (2013): 5836-5847.</p> </li> <li> <p>Kim, Jonghyun, et al. \"Foveated AR: dynamically-foveated augmented reality display.\" ACM Transactions on Graphics (TOG) 38.4 (2019): 1-15.</p> </li> </ul>"},{"location":"teaching/comp0160_perception_and_interfaces/#lecture-7-integrating-sensory-information-in-computational-displays","title":"Lecture 7: Integrating Sensory Information in Computational Displays","text":"<p> Winter 2022</p> <p> Online</p> <p> Recording (Password protected)</p> <p> Slides (Invitation required)</p> <p>This lecture focuses on integrating various kinds of sensory information to the next generation displays.</p> Details <p>Summary: In this course, students will learn about sensors and their integration into modern display systems such as Virtual and Augmented Reality near-eye displays and three-dimensional displays. In the first half, a review of various kinds of sensors that could capture vital signs from a user, such as heart rate and gaze orientation, will be provided. The second half will cover applications that use captured sensory information. These applications will be sampled from actual products on the market and research prototypes at the forefront of science.</p> <ul> <li> <p>Cennini, G., Arguel, J., Ak\u015fit, K., &amp; van Leest, A. (2010). Heart rate monitoring via remote photoplethysmography with motion artifacts reduction. Optics express, 18(5), 4867-4875.</p> </li> <li> <p>Li, Richard, Eric Whitmire, Michael Stengel, Ben Boudaoud, Jan Kautz, David Luebke, Shwetak Patel, and Kaan Ak\u015fit. \"Optical gaze tracking with spatially-sparse single-pixel detectors.\" In 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), pp. 117-126. IEEE, 2020.</p> </li> <li> <p>Angelopoulos, Anastasios N., Julien NP Martel, Amit P. Kohli, Jorg Conradt, and Gordon Wetzstein. \"Event-Based Near-Eye Gaze Tracking Beyond 10,000 Hz.\" IEEE transactions on visualization and computer graphics 27, no. 5 (2021): 2577-2586.</p> </li> <li> <p>Wei, Shih-En, Jason Saragih, Tomas Simon, Adam W. Harley, Stephen Lombardi, Michal Perdoch, Alexander Hypes, Dawei Wang, Hernan Badino, and Yaser Sheikh. \"Vr facial animation via multiview image translation.\" ACM Transactions on Graphics (TOG) 38, no. 4 (2019): 1-16.</p> </li> <li> <p>Yaldiz, Mustafa B., Andreas Meuleman, Hyeonjoong Jang, Hyunho Ha, and Min H. Kim. \"DeepFormableTag: end-to-end generation and recognition of deformable fiducial markers.\" ACM Transactions on Graphics (TOG) 40, no. 4 (2021): 1-14.</p> </li> <li> <p>Glauser, O., Wu, S., Panozzo, D., Hilliges, O., &amp; Sorkine-Hornung, O. (2019). Interactive hand pose estimation using a stretch-sensing soft glove. ACM Transactions on Graphics (TOG), 38(4), 1-15.</p> </li> <li> <p>Glauser, O., Panozzo, D., Hilliges, O., &amp; Sorkine-Hornung, O. (2019). Deformation capture via soft and stretchable sensor arrays. ACM Transactions on Graphics (TOG), 38(2), 1-16.</p> </li> <li> <p>HP Reverb G2 VR Headset</p> </li> <li> <p>MediaPipe Iris: Real-time Iris Tracking and Depth Estimation</p> </li> <li> <p>Brelyon: a window to a whole new world</p> </li> <li> <p>Tobii's eye and head tracking for professional esports</p> </li> </ul>"},{"location":"teaching/comp0160_perception_and_interfaces/#team","title":"Team","text":"<p>Kaan Ak\u015fit</p> <p>Instructor</p> <p> E-mail </p>"},{"location":"teaching/comp0160_perception_and_interfaces/#contact-us","title":"Contact Us","text":"<p>Warning</p> <p>The prefered way of communication is through University College London's online lecture system, Moodle. Please do not reach us through email unless the thing you want to achieve or establish or ask is not possible through the online lecture system.</p>"},{"location":"teaching/comp0160_practical/","title":"Practical","text":""},{"location":"teaching/comp0160_practical/#welcome","title":"Welcome","text":"<p>Start date: 12:00 pm, 14th January 2022,</p> <p>Duration: This description is valid for first four weeks of practical sessions.</p> <p>Welcome to the practical session for COMP0160: Perception and Interfaces. This course is designed to offer you, the students, a gateway to get familiar with various aspects of perception and interfaces.</p> <p>In your lectures, you will learn about the human visual system and how humans perceive light from their surroundings. Specifically, the course \"Visual Perception in graphics and displays\", held between 17th January and 23rd January 2022, will teach you how eyes gaze and perceive their surroundings. </p> <p>In these practical sessions, we provide the opportunity for you to get familiarized with standard tools used in computer graphics.  These tools will help you translate what you learn from the courses to an actual outcome. The rest of this document will describe your task in these practical sessions. Remember that the task description provided here contains the information on the session's aims, objectives, and goals. But the description does not necessarily contain all the steps required to conduct your work. This case is because everybody learns things in a slightly different way. We create that environment to elaborate by not strictly telling you which steps to take.</p> <p>Please make sure to bring your personal laptop with you if you are physically attending.</p> <p>Once again welcome to COMP0160 practical sessions!</p> <p>Kaan Ak\u015fit</p>"},{"location":"teaching/comp0160_practical/#background-15-minutes","title":"Background (15 minutes)","text":"<p>Imagine you woke up to ten years from now on. You have long graduated from your program, and you found yourself a job in the industry. Guess what your job is about! It is about humans and their perceptions.</p> <p>The company that you are working at builds digital humans and avatars to represent people or their artificial intelligence-powered bots. These bots offer innovative assistance services such as representing you when you are unavailable (multi conversation) or offering you services that are carried out by human workers today (e.g., call centres). These efforts are for their upcoming revolution in virtual environments, or more popularly known as Metaverse nowadays.</p> <p>In this hypothetical future, your manager calls you from your virtual reality device while having your first coffee of the day. She assigns you as the lead for upgrading the eye model that they use in their avatars and bots. They want to make sure that the eyes rendered at these avatars look realistic and physically plausible.</p> <p>You now have these flashbacks, remembering that you learned in some class in the past called \"Visual Perception in Graphics and Displays\" how eyes rotate, shift and view.</p> <p>In this exercise, please put that information into work, and we ask you to build an eye model that rotates by following the constraints taught in that class. In short, your task is to build a 3D model of the eye that follows physical constraints while gazing (e.g., eyeballs can not rotate 180 degrees, today's cutting-edge PC games make such easy mistakes).</p>"},{"location":"teaching/comp0160_practical/#tools-45-minutes","title":"Tools (45 minutes)","text":"<p>In this practical, you will be using Blender 3D, an open-source tool for modelling and rendering with state of the art in computer graphics.</p> <p>Please make sure to download Blender 3D to your production machine and familiarise yourself with the software controls. You can find many tutorials online by typing Blender tutorial in your favourite search engine (e.g., Google) or cloud video platform (e.g., YouTube).</p>"},{"location":"teaching/comp0160_practical/#create-a-mesh-of-your-face-1-hour","title":"Create a mesh of your face (1 hour)","text":"<p>Your first target in this practice is to figure out how to create a mesh of your face from a photograph. This way, we will use the face mesh you created to place the two eyeballs that we aim to add. Once again, there are multiple tutorials online (e.g., sample tutorial), which you can easily search for. You are free to choose your way of creating a mesh of your face. You can also rely on the methods that offer one click add-on base solutions.</p> <p>Once the face mesh is created, please remove the triangles where your eye should be and make sure to texture your face mesh using your photograph.</p>"},{"location":"teaching/comp0160_practical/#adding-the-eyeballs-45-minutes","title":"Adding the eyeballs (45 minutes)","text":"<p>Our next step is to add the eyeballs to your face mesh. Following the geometry that you have learned from the lecture, you will be creating an eyeball that is faithful to the physiology of the eye. These eyeballs will later be placed on the face mesh that you generated. There are again multiple tutorials online that you can rely on to generate a realistic-looking eyeball (e.g., sample tutorial). Once you are complete with this task, make sure to place these eyeballs on your face mesh.</p>"},{"location":"teaching/comp0160_practical/#realistic-gaze-15-minutes","title":"Realistic gaze (15 minutes)","text":"<p>Now that you have generated a face mesh and two eyeballs for your face mesh, please explore the option of constraints in the Blender 3D. Our aim here is to add a target (e.g., a box) at some distance in front of the face mesh, and we want our newly generated digital avatar to gaze at the target. You have to identify means to rotate the eyeballs such that they always gaze at the box. But remember, you also learned how much and how far an eyeball can rotate in the lecture. We want to be able to add that as a constrain as well. So once the target moves, eyeballs should rotate, but they shouldn't turn beyond their rotation ability.</p>"},{"location":"teaching/comp0160_practical/#changing-the-iris-size-with-light-levels-1-hour","title":"Changing the iris size with light levels (1 hour)","text":"<p>This last task is entirely optional. You will seek means to change iris diameter with the light levels in the environment of your digital avatar. As a first step, make sure to set the target you have created to be a light source. Since our digital avatar is always gazing at it, this target's light level will determine the size of the iris. You learned how large or small an iris could be in your lecture. We want you to manipulate the eyeball mesh procedurally with the light level of your target.</p>"},{"location":"teaching/comp0160_practical/#conclusion","title":"Conclusion","text":"<p>You may have multiple questions during practical. Do not hesitate to reach out to us at the practical time or via Moodle. Once you are complete with practical and trust that you have generated an excellent digital avatar, please let us know as this can return as an investment towards improving your final mark from this course. If you provide your consent (since this is containing your personal data -- your face), you can also share your blender file with us by uploading it to a cloud service (Microsoft One Drive provided by the university), share the link of the file with us through an email.  In your email, please make sure to state under what licence you are providing us the Blender file (e.g, MIT License, Creative Commons), and please make sure to state if you allow us to use this data in scientific research in the future. Note that this exercise is also beneficial for your final assignment, and it can potentially help you achieve your task in your final project.</p>"},{"location":"teaching/siggraph2022_optimizing_vision_and_visuals/","title":"Optimizing Vision and Visuals: Lectures on Cameras, Displays and Perception","text":""},{"location":"teaching/siggraph2022_optimizing_vision_and_visuals/#optimizing-vision-and-visuals-lectures-on-cameras-displays-and-perception","title":"Optimizing Vision and Visuals: Lectures on Cameras, Displays and Perception","text":""},{"location":"teaching/siggraph2022_optimizing_vision_and_visuals/#people","title":"People","text":"<p>Koray Kavakl\u0131<sup>1</sup></p> <p>David Walton<sup>2</sup></p> <p>Nick Antipa<sup>3</sup></p> <p>Rafa\u0142 Mantiuk<sup>4</sup></p> <p>Douglas Lanman<sup>5,6</sup></p> <p>Kaan Ak\u015fit<sup>2</sup></p> <p><sup>1</sup>Ko\u00e7 University, <sup>2</sup>University College London, <sup>3</sup>University of California San Diego, <sup>4</sup>University of Cambridge, <sup>5</sup>Meta Reality Labs, <sup>6</sup>University of Washington  </p> <p>SIGGRAPH 2022</p>"},{"location":"teaching/siggraph2022_optimizing_vision_and_visuals/#resources","title":"Resources","text":"<p> Lecture recording Code Foreword</p>  Bibtex <pre><code>@inproceedings{10.1145/3532720.3535650,\n author = {Kavakli, Koray and Walton, David Robert and Antipa, Nick and Mantiuk, Rafa\\l{} and Lanman, Douglas and Ak\\c{s}it, Kaan},\n title = {Optimizing Vision and Visuals: Lectures on Cameras, Displays and Perception},\n year = {2022},\n isbn = {9781450393621},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/3532720.3535650},\n doi = {10.1145/3532720.3535650},\n booktitle = {ACM SIGGRAPH 2022 Courses},\n articleno = {17},\n numpages = {66},\n location = {Vancouver, British Columbia, Canada},\n series = {SIGGRAPH '22}\n}\n</code></pre>"},{"location":"teaching/siggraph2022_optimizing_vision_and_visuals/#presentation","title":"Presentation","text":""},{"location":"teaching/siggraph2022_optimizing_vision_and_visuals/#abstract","title":"Abstract","text":"<p>The evolution of the internet is underway, where immersive virtual 3D environments (commonly known as metaverse or telelife) will replace flat 2D interfaces. Crucial ingredients in this transformation are next-generation displays and cameras representing genuinely 3D visuals while meeting the human visual system's perceptual requirements.</p> <p>This course will provide a fast-paced introduction to optimization methods for next-generation interfaces geared towards immersive virtual 3D environments. Firstly, we will introduce lensless cameras for high dimensional compressive sensing (e.g., single exposure capture to a video or one-shot 3D). Our audience will learn to process images from a lensless camera at the end. Secondly, we introduce holographic displays as a potential candidate for next-generation displays. By the end of this course, you will learn to create your 3D images that can be viewed using a standard holographic display. Lastly, we will introduce perceptual guidance that could be an integral part of the optimization routines of displays and cameras. Our audience will gather experience in integrating perception to display and camera optimizations.</p> <p>This course targets a wide range of audiences, from domain experts to newcomers. To do so, examples from this course will be based on our in-house toolkit to be replicable for future use. The course material will provide example codes and a broad survey with crucial information on cameras, displays and perception.</p>"},{"location":"teaching/siggraph2022_optimizing_vision_and_visuals/#relevant-research-works","title":"Relevant research works","text":"<p>Here are relevant research works from the authors:</p> <ul> <li>Odak</li> <li>Metameric Varifocal Holograms</li> <li>Learned Holographic Light Transport</li> <li>Unrolled Primal-Dual Networks for Lensless Cameras</li> </ul>"},{"location":"teaching/siggraph2022_optimizing_vision_and_visuals/#outreach","title":"Outreach","text":"<p>We host a Slack group with more than 250 members. This Slack group focuses on the topics of rendering, perception, displays and cameras. The group is open to public and you can become a member by following this link.</p>"},{"location":"teaching/siggraph2022_optimizing_vision_and_visuals/#contact-us","title":"Contact Us","text":"<p>Warning</p> <p>Please reach us through email or through GitHub issues to ask your questions or to provide your feedback and comments.</p>"},{"location":"teaching/siggraph2022_optimizing_vision_and_visuals/#acknowledgements","title":"Acknowledgements","text":"<p>The authors would like to thank reviewers for their valuable feedback.</p> <p>Kaan Ak\u015fit is supported by the Royal Society's RGS\\R2\\212229 - Research Grants 2021 Round 2 in building the hardware prototype used in generating the course material. Kaan Ak\u015fit is also supported by Meta Reality Labs inclusive rendering initiative 2022.  </p>"},{"location":"timeline/","title":"Recent news","text":""},{"location":"timeline/#timeline","title":"Timeline","text":""},{"location":"timeline/#2025","title":"2025","text":""},{"location":"timeline/#15-december-2025","title":"15 December 2025","text":"<p>\u26a1 Computational Light Laboratory bridges student potential into scientific success with global academic and industrial partners in 2025. Find out more by visiting our article.  </p> <p>Our work, \"Learned Display Radiance Fields with Lensless Cameras\", is presented under the technical communications program at ACM SIGGRAPH Asia 2025 in Hong Kong, China. This work is a collaboration with a series of academic partners including Ziyang Chen (\u9648\u5b50\u626c), Yuta Itoh, and Kaan Ak\u015fit.</p> <p>Our work, \"Foveation Improves Payload Capacity in Steganography\", is presented under the poster program at ACM SIGGRAPH Asia 2025 in Hong Kong, China. This work is a collaboration with a series of academic partners including Lifeng Qiu Lin, Henry Kam (\u7518\u7693\u5b87), Qi Sun, and Kaan Ak\u015fit.</p>"},{"location":"timeline/#25-november-2025","title":"25 November 2025","text":"<p>Kaan Ak\u015fit is pleased to announce that he is elected as a fellow member by the board of directors of Optica (formerly known as The Optical Society (OSA)). Their decision came in the light of Kaan's track record on advanced 3D display and perceptually guided computer graphics techniques.</p>"},{"location":"timeline/#27-october-2025","title":"27 October 2025","text":"<p>Our work, \"Learned Single-Pass Multitasking Perceptual Graphics for Immersive Displays\", is presented under the oral presentation program at ACM Multimedia 2025 in Dublin, Ireland. This work is a collaboration with a series of academic and industry partners including Do\u011fa Y\u0131lmaz, He Wang, Towaki Takikawa, Duygu Ceylan, and Kaan Ak\u015fit.</p>"},{"location":"timeline/#11-august-2025","title":"11 August 2025","text":"<p>Our work, \"Assessing Learned Models for Phase-only Hologram Compression\", is presented under the posters program at ACM SIGGRAPH 2025 in Vancouver, Canada. This work is a collaboration with a series of academic and industry partners including Zicong Peng, Yicheng Zhan (\u6218\u5f08\u8bda), Josef Spjut, and Kaan Ak\u015fit.</p> <p>Our work, \"Efficient Proxy Raytracer for Optical Systems using Implicit Neural Representations\", is presented under the posters program at ACM SIGGRAPH 2025 in Vancouver, Canada. This work is a collaboration with a series of academic partners including Shiva Sinaei, Chuanjun Zheng, Kaan Ak\u015fit, and Daisuke Iwai.</p>"},{"location":"timeline/#6-february-2025","title":"6 February 2025","text":"<p>Our work, \"SpecTrack: Learned Multi-Rotation Tracking via Speckle Imaging\", won the honorable mention award under the posters program at ACM SIGGRAPH Asia 2024 in Tokyo, Japan. These works are collaborations with a series of academic and industry partners including Ziyang Chen (\u9648\u5b50\u626c), Mustafa Do\u011fa Do\u011fan, Josef Spjut, and Kaan Ak\u015fit.</p>"},{"location":"timeline/#2024","title":"2024","text":""},{"location":"timeline/#3-6-december-2024","title":"3-6 December 2024","text":"<p>Our innovative research on computational displays and lensless cameras are presented at ACM SIGGRAPH Asia 2024 in Tokyo, Japan. These works are collaborations with a series of academic and industry partners including Chuanjun Zheng, Liang Shi, Ozan Cakmakci,  Yicheng Zhan (\u6218\u5f08\u8bda), Ziyang Chen (\u9648\u5b50\u626c), Mustafa Do\u011fa Do\u011fan, Josef Spjut, and Kaan Ak\u015fit.</p>"},{"location":"timeline/#22-april-2024","title":"22 April 2024","text":"<p>Our paper, <code>All-optical image denoising using a diffractive visual processor</code>, is recognized as the most downloaded paper in March 2024 by Nature's Light and Science Applications. This work is a collaboration between \u00c7a\u011fatay I\u015f\u0131l, Tianyi Gan, Fazil Onuralp, Koray Mentesoglu, Jagrit Digani, Huseyin Karaca, Hanlong Chen, Jingxi Li, Deniz Mengu, Mona Jarrahi, Kaan Ak\u015fit, and Ozcan Aydogan. </p> <p>Our paper, <code>Multi-color Holograms Improve Brightness in Holographic Displays</code>, is awarded with Graphics Replicability Stamp Initiative's replicability stamp. This work is a collaboration between Koray Kavakl\u0131, Liang Shi, Hakan Urey, Wojciech Matusik, and Kaan Ak\u015fit.  </p>"},{"location":"timeline/#29-january-2024","title":"29 January 2024","text":"<p>Our paper, <code>AutoColor: Learned Light Power Control for Multi-Color Holograms</code>, is presented at SPIE AR|VR|MR 2024. This work is a collaboration between Yicheng Zhan(\u6218\u5f08\u8bda), Koray Kavakl\u0131, Hakan Urey, Qi Sun, and Kaan Ak\u015fit.  </p>"},{"location":"timeline/#2023","title":"2023","text":""},{"location":"timeline/#13-december-2023","title":"13 December 2023","text":"<p>Our paper,  <code>Multi-color Holograms Improve Brightness in Holographic Displays</code>,  is presented at  SIGGRAPH Asia 2023. This work is a collaboration between Koray Kavakl\u0131, Liang Shi, Hakan Urey, Wojciech Matusik, and Kaan Ak\u015fit.  </p>"},{"location":"timeline/#30-october-2023","title":"30 October 2023","text":"<p>We are pleased to announce an achievement at the UKRI AI CDT Conference 2023 in Bristol, United Kingdom. Ahmet G\u00fczel showcased our research project, ChromaCorrect, among a diverse array of over 50 posters at the event. We are honored to have been awarded First Prize for Best Poster, and we thank Foundational Artificial Intelligence Center at University College London.   </p>"},{"location":"timeline/#12-13-october-2023","title":"12-13 October 2023","text":"<p>Kaan helped organizing Optical Waveguides: A key to Socially Acceptable Augmented Reality Glasses? as an Optica Incubator. Kaan also gave an invited talk titled <code>Role of Differentiable Models in Computational Display Research</code> at the same incubator event.  </p>"},{"location":"timeline/#11-october-2023","title":"11 October 2023","text":"<p>Kaan attended and presented at Meta's Academic forum 2023 upon Meta Reality Labs invitation. Kaan's talk is titled <code>Unlocking Next-Generation Display Technologies with Holography.</code> </p>"},{"location":"timeline/#9-10-october-2023","title":"9-10 October 2023","text":"<p>Kaan helped organizing Virtual Reality and Augmented Vision theme at Optica's Frontiers Optics. Kaan also gave an invited talk on his group's work, <code>Headsetless Holographic Virtual Reality Displays</code> in the same theme.  </p>"},{"location":"timeline/#16-august-2023","title":"16 August 2023","text":"<p>We are grateful to Optica for inviting our Ahmet Hamdi G\u00fczel to present his work at the Vision and Color summer data blast webinar.  Have you missed it? The recording is now online.  </p>"},{"location":"timeline/#28-june-2023","title":"28 June 2023","text":"<p>Kaan presented an invited talk titled <code>Could holographic displays be the key to achieving realism?</code> at Stanford University.  </p>"},{"location":"timeline/#1-june-2023","title":"1 June 2023","text":"<p>In her latest article, \"The Promise of Holographic Displays,\" Sandrine Ceurstemont gathered perspectives on the promise of holographic displays and provided some space for ours.   </p>"},{"location":"timeline/#21-april-2023","title":"21 April 2023","text":"<p>Our paper,  <code>ChromaCorrect: Prescription Correction in Virtual Reality Headsets through Perceptual Guidance</code>,  is published at Optica's Biomedical Optics Express. This work is a result of a collaboration with  Ahmet H. G\u00fczel,  Jeanne Beyazian, Praneeth Chakravarthula, and Kaan Ak\u015fit.  </p>"},{"location":"timeline/#28-march-2023","title":"28 March 2023","text":"<p>Our paper,  <code>HoloBeam: Paper-Thin Near-Eye Displays</code>,  is presented at  IEEE VR 2023. This work is a collaboration between Yuta Itoh, and Kaan Ak\u015fit.  </p> <p>Our paper,  <code>Realistic Defocus Blur for Multiplane Computer-Generated Holography</code>,  is presented at  IEEE VR 2023. This work is a collaboration between Koray Kavakl\u0131, Yuta Itoh, Hakan \u00dcrey, and Kaan Ak\u015fit.  </p>"},{"location":"timeline/#15-march-2023","title":"15 March 2023","text":"<p>Kaan presented an invited talk titled <code>Could holographic displays be the key to achieving realism in displays?</code>. We are thankful to Huawei, United Kingdom for their kind hospitality.  </p>"},{"location":"timeline/#6-february-2023","title":"6 February 2023","text":"<p>Kaan presented an invited talk titled <code>Could holographic displays be the key to achieving realism?</code>. We are thankful to University of Rochester's institute of optics for their kind hospitality.  </p>"},{"location":"timeline/#4-january-2023","title":"4 January 2023","text":"<p>We are thankful to T\u00dcB\u0130TAK's 2224-A support for our valuable member and a PhD student, Koray Kavakl\u0131 in presenting his work at SPIE's Photonics West 2023. This fund covers a significant portion of his attendance at SPIE's Photonics West.  </p>"},{"location":"timeline/#3-january-2023","title":"3 January 2023","text":"<p>We are thankful to Oracle for offering us to rely on their cloud infrastructure support for our computational needs. We had to decline their award as we have purchased new computational resources most recently.  </p>"},{"location":"timeline/#2022","title":"2022","text":""},{"location":"timeline/#18-november-2022","title":"18 November 2022","text":"<p>Our paper, <code>Unrolled Primal-Dual Networks for Lensless Cameras</code>,  is published at  Optica's Optics Express. This work is a result of a collaboration between  Oliver Kingshott,  Nick Antipa,  Emrah Bostan, and Kaan Ak\u015fit.  </p>"},{"location":"timeline/#25-october-2022","title":"25 October 2022","text":"<p>Our paper,  <code>Metameric Inpainting for Image Warping</code>,  is published at IEEE's Transaction on Visualization and Computer Graphics. This work is a collaboration between  Rafael Kuffner dos Anjos,  David Robert Walton,  Sebastian Friston,  David Swapp,  Anthony Steed,  Tobias Ritschel, and Kaan Ak\u015fit.  </p>"},{"location":"timeline/#19-october-2022","title":"19 October 2022","text":"<p>In collaboration with Meta Reality Laboratory's Douglas Lanman, we helped organise a successful augmented reality and virtual reality theme at Optica's Frontiers in Optics 2022. Kaan Ak\u015fit presented a talk titled <code>Realistic Image Reconstruction with Multiplane Computer-Generated Holography</code>, while Koray Kavakl\u0131 presented a talk titled <code>Introduction to Odak: a Differentiable Toolkit for Optical Sciences, Vision Sciences and Computer Graphics</code>.  </p>"},{"location":"timeline/#3-august-2022","title":"3 August 2022","text":"<p>Our course, Optimizing Vision and Visuals: Lectures on Cameras, Displays and Perception is available and online in SIGGRAPH 2022. This work is a collaboration between Koray Kavakl\u0131, David Walton, Nick Antipa, Rafa\u0142 Mantiuk, Douglas Lanman and Kaan Ak\u015fit.  </p>"},{"location":"timeline/#23-june-2022","title":"23 June 2022","text":"<p>We are grateful to the Meta Reality Labs for supporting our research through the inclusive rendering initiative 2022. Their award will enable us to investigate inclusive graphics pipelines in terms of human visual perception. Their award is worth <code>75000 USD</code>.  </p>"},{"location":"timeline/#5-may-2022","title":"5 May 2022","text":"<p>Our panel, <code>Telelife: A Vision of Remote Living in 2035</code>, is presented at CHI 2022. This work is a collaboration between  Kenan Bekta\u015f,  Jeeeun Kim,  Kiyoshi Kiyokawa,  Anthony Steed,  Tobias H\u00f6llerer,  Nataliya Kosmyna,  Misha Sra,  Jason Orlosky,  and Kaan Ak\u015fit.  </p>"},{"location":"timeline/#14-march-2022","title":"14 March 2022","text":"<p>We introduce our work, Metameric Varifocal Holograms, at IEEE VR 2022. This work is a collaboration between David R. Walton, Koray Kavakli, Rafael Kuffner dos Anjos, David Swapp, Tim Weyrich, Hakan Urey, Anthony Steed, Tobias Ritschel and Kaan Ak\u015fit. David Walton presented the work at the conference.   </p> <p>Kaan Ak\u015fit served as program committee for journal papers, and also as technical achievement and lifetime achievement awards committee member.  </p>"},{"location":"timeline/#11-march-2022","title":"11 March 2022","text":"<p>Kaan Ak\u015fit together with Jannick Rolland and Babak Amirsolaimani is acting as a guest editor for Journal of Optical Microsystems from SPIE at a special issue targetting optics research in augmented, virtual and mixed reality. Here is a link for call flyer and to submit your work, please follow this link.  </p>"},{"location":"timeline/#23-february-2022","title":"23 February 2022","text":"<p>Kaan Ak\u015fit serves as a program committee for EGSR 2022, which will take place as a hybrid conference, virtual and physically located in Czech Republic's Prag.  </p>"},{"location":"timeline/#24-january-2022","title":"24 January 2022","text":"<p>We presented two invited talks at SPIE's Photonics West. Our first talk is on Perceptually guided computer-generated holography, and our second talk is on Beaming Displays: Towards Displayless Augmented Reality Near-eye Displays.  </p>"},{"location":"timeline/#17-january-2022","title":"17 January 2022","text":"<p>We thank the next byte podcast for covering our collaboration on SensiCut project with MIT. They did a great job in explaining our in their podcast. You can reach the podcast using this link.  </p>"},{"location":"timeline/#2021","title":"2021","text":""},{"location":"timeline/#29-november-2021","title":"29 November 2021","text":"<p>Our vision, \"Telelife: The Future of Remote Living\", is published at Frontiers in Virtual Reality. We share our vision for the future, specifically in the year 2035.  </p>"},{"location":"timeline/#12-november-2021","title":"12 November 2021","text":"<p>Our invited work, \"Learned Holographic Light Transport\", is published at Optica's Applied Optics. We show that light transport can be made more accurate by learning hardware dependent kernels.  </p>"},{"location":"timeline/#8-november-2021","title":"8 November 2021","text":"<p>We are grateful to the Royal Society for awarding us with their research grants 2021 round two scheme. Their award will enable us to invent new holographic light transport models in the future. This is award is worth <code>14994.65 GBP</code>. The title of our submission is <code>Learned models for Computer-Generated Holography</code>.  </p>"},{"location":"timeline/#1-2-november-2021","title":"1-2 November 2021","text":"<p>In collaboration with Meta Reality Laboratory's Douglas Lanman, we helped organise a successful augmented reality and virtual reality theme at Optica's Frontiers in Optics 2021. Kaan Ak\u015fit presented his work on holographic beaming displays proposal at the same event.  </p>"},{"location":"timeline/#18-february-2021","title":"18 February 2021","text":"<p>We appear on UCL news for receiving UCL-Osaka university strategic partnership fund.  </p>"},{"location":"timeline/#4-january-2021","title":"4 January 2021","text":"<p>Kaan Ak\u015fit joined University College London's computer science department as an Associate Professor. He is now part of the Virtual Reality and Computer Graphics group, and he leads the Computational light laboratory.  </p>"},{"location":"timeline/#2020","title":"2020","text":""},{"location":"timeline/#17-november-2020","title":"17 November 2020","text":"<p>Kaan Ak\u015fit and Jason Orlosky They have been granted UCL-Osaka University Strategic Parner Funds funds. This award is worth <code>10000 GBP</code>. The title of our submission is <code>Development of a joint Telelife technology seminar using virtual reality</code>.  </p>"},{"location":"timeline/#1-august-2020","title":"1 August 2020","text":"<p>Kaan Ak\u015fit has left his Senior Research scientist position at NVIDIA in the US, and accepted to join University College London's computer science department as an Associate Professor.  </p>"}]}